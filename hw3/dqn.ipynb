{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path as osp\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "from dqn_utils import *\n",
    "from gym import wrappers\n",
    "from atari_wrappers import *\n",
    "\n",
    "\n",
    "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\", \"lr_schedule\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_session():\n",
    "    tf.reset_default_graph()\n",
    "    tf_config = tf.ConfigProto(\n",
    "        inter_op_parallelism_threads=1,\n",
    "        intra_op_parallelism_threads=1)\n",
    "    session = tf.Session(config=tf_config)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def atari_model(img_in, num_actions, scope, reuse=False):\n",
    "    # as described in https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = img_in\n",
    "        with tf.variable_scope(\"convnet\"):\n",
    "            # original architecture\n",
    "            out = layers.convolution2d(out, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)\n",
    "        out = layers.flatten(out)\n",
    "        with tf.variable_scope(\"action_value\"):\n",
    "            out = layers.fully_connected(out, num_outputs=512,         activation_fn=tf.nn.relu)\n",
    "            out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_timesteps = 1000000\n",
    "num_iterations = float(num_timesteps) / 4.0\n",
    "\n",
    "lr_multiplier = 1.0\n",
    "lr_schedule = PiecewiseSchedule([\n",
    "                                     (0,                   1e-4 * lr_multiplier),\n",
    "                                     (num_iterations / 10, 1e-4 * lr_multiplier),\n",
    "                                     (num_iterations / 2,  5e-5 * lr_multiplier),\n",
    "                                ],\n",
    "                                outside_value=5e-5 * lr_multiplier)\n",
    "optimizer = OptimizerSpec(\n",
    "    constructor=tf.train.AdamOptimizer,\n",
    "    kwargs=dict(epsilon=1e-4),\n",
    "    lr_schedule=lr_schedule\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 01:29:16,983] Making new env: PongNoFrameskip-v4\n",
      "[2018-06-21 01:29:17,118] Clearing 4 monitor files from previous run (because force=True was provided)\n"
     ]
    }
   ],
   "source": [
    "benchmark = gym.benchmark_spec('Atari40M')\n",
    "\n",
    "# Change the index to select a different game.\n",
    "task = benchmark.tasks[3]\n",
    "\n",
    "# Run training\n",
    "seed = 0 # Use a seed of zero (you may want to randomize the seed!)\n",
    "env = gym.make(task.env_id)\n",
    "# set_global_seeds(seed)\n",
    "# env.seed(seed)\n",
    "\n",
    "expt_dir = '/tmp/hw3_vid_dir2/'\n",
    "env = wrappers.Monitor(env, osp.join(expt_dir, \"gym\"), force=True)\n",
    "env = wrap_deepmind(env)\n",
    "\n",
    "\n",
    "\n",
    "session = get_session()\n",
    "# tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = env\n",
    "q_func = atari_model\n",
    "optimizer_spec = optimizer\n",
    "session =  session\n",
    "exploration=LinearSchedule(1000000, 0.1)\n",
    "stopping_criterion=None\n",
    "replay_buffer_size=1000000\n",
    "batch_size=32\n",
    "gamma=0.99\n",
    "learning_starts=50000\n",
    "learning_freq=4\n",
    "frame_history_len=4\n",
    "target_update_freq=10000\n",
    "grad_norm_clipping=10\n",
    "\"\"\"Run Deep Q-learning algorithm.\n",
    "\n",
    "You can specify your own convnet using q_func.\n",
    "\n",
    "All schedules are w.r.t. total number of steps taken in the environment.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "env: gym.Env\n",
    "    gym environment to train on.\n",
    "q_func: function\n",
    "    Model to use for computing the q function. It should accept the\n",
    "    following named arguments:\n",
    "        img_in: tf.Tensor\n",
    "            tensorflow tensor representing the input image\n",
    "        num_actions: int\n",
    "            number of actions\n",
    "        scope: str\n",
    "            scope in which all the model related variables\n",
    "            should be created\n",
    "        reuse: bool\n",
    "            whether previously created variables should be reused.\n",
    "optimizer_spec: OptimizerSpec\n",
    "    Specifying the constructor and kwargs, as well as learning rate schedule\n",
    "    for the optimizer\n",
    "session: tf.Session\n",
    "    tensorflow session to use.\n",
    "exploration: rl_algs.deepq.utils.schedules.Schedule\n",
    "    schedule for probability of chosing random action.\n",
    "stopping_criterion: (env, t) -> bool\n",
    "    should return true when it's ok for the RL algorithm to stop.\n",
    "    takes in env and the number of steps executed so far.\n",
    "replay_buffer_size: int\n",
    "    How many memories to store in the replay buffer.\n",
    "batch_size: int\n",
    "    How many transitions to sample each time experience is replayed.\n",
    "gamma: float\n",
    "    Discount Factor\n",
    "learning_starts: int\n",
    "    After how many environment steps to start replaying experiences\n",
    "learning_freq: int\n",
    "    How many steps of environment to take between every experience replay\n",
    "frame_history_len: int\n",
    "    How many past frames to include as input to the model.\n",
    "target_update_freq: int\n",
    "    How many experience replay rounds (not steps!) to perform between\n",
    "    each update to the target Q network\n",
    "grad_norm_clipping: float or None\n",
    "    If not None gradients' norms are clipped to this value.\n",
    "\"\"\"\n",
    "assert type(env.observation_space) == gym.spaces.Box\n",
    "assert type(env.action_space)      == gym.spaces.Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# BUILD MODEL #\n",
    "###############\n",
    "\n",
    "if len(env.observation_space.shape) == 1:\n",
    "    # This means we are running on low-dimensional observations (e.g. RAM)\n",
    "    input_shape = env.observation_space.shape\n",
    "else:\n",
    "    img_h, img_w, img_c = env.observation_space.shape\n",
    "    input_shape = (img_h, img_w, frame_history_len * img_c)\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up placeholders\n",
    "# placeholder for current observation (or state)\n",
    "obs_t_ph              = tf.placeholder(tf.uint8, [None] + list(input_shape))\n",
    "# placeholder for current action\n",
    "act_t_ph              = tf.placeholder(tf.int32,   [None])\n",
    "# placeholder for current reward\n",
    "rew_t_ph              = tf.placeholder(tf.float32, [None])\n",
    "# placeholder for next observation (or state)\n",
    "obs_tp1_ph            = tf.placeholder(tf.uint8, [None] + list(input_shape))\n",
    "# placeholder for end of episode mask\n",
    "# this value is 1 if the next state corresponds to the end of an episode,\n",
    "# in which case there is no Q-value at the next state; at the end of an\n",
    "# episode, only the current state reward contributes to the target, not the\n",
    "# next state Q-value (i.e. target is just rew_t_ph, not rew_t_ph + gamma * q_tp1)\n",
    "done_mask_ph          = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "# casting to float on GPU ensures lower data transfer times.\n",
    "obs_t_float   = tf.cast(obs_t_ph,   tf.float32) / 255.0\n",
    "obs_tp1_float = tf.cast(obs_tp1_ph, tf.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, you should fill in your own code to compute the Bellman error. This requires\n",
    "# evaluating the current and next Q-values and constructing the corresponding error.\n",
    "# TensorFlow will differentiate this error for you, you just need to pass it to the\n",
    "# optimizer. See assignment text for details.\n",
    "# Your code should produce one scalar-valued tensor: total_error\n",
    "# This will be passed to the optimizer in the provided code below.\n",
    "# Your code should also produce two collections of variables:\n",
    "# q_func_vars\n",
    "# target_q_func_vars\n",
    "# These should hold all of the variables of the Q-function network and target network,\n",
    "# respectively. A convenient way to get these is to make use of TF's \"scope\" feature.\n",
    "# For example, you can create your Q-function network with the scope \"q_func\" like this:\n",
    "# <something> = q_func(obs_t_float, num_actions, scope=\"q_func\", reuse=False)\n",
    "# And then you can obtain the variables like this:\n",
    "# q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_func')\n",
    "# Older versions of TensorFlow may require using \"VARIABLES\" instead of \"GLOBAL_VARIABLES\"\n",
    "######\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#get q value for all actions for the current step and for the next step\n",
    "q_t_all = q_func(img_in=obs_t_float, num_actions=num_actions, scope=\"q_func\", reuse=False)\n",
    "greedy_action = tf.argmax(q_t_all, axis=1)\n",
    "\n",
    "q_tp1_all = q_func(img_in=obs_tp1_float, num_actions=num_actions, scope=\"target_q_func\", reuse=False)\n",
    "\n",
    "q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_func')\n",
    "target_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_q_func')\n",
    "\n",
    "q_t = tf.reduce_sum(tf.one_hot(act_t_ph, depth=num_actions, dtype=tf.float32) * q_t_all, axis=1)\n",
    "v_tp1 = tf.reduce_max(q_tp1_all, axis=1)\n",
    "q_target = rew_t_ph + (1.0 - done_mask_ph) * gamma * v_tp1\n",
    "\n",
    "total_error = tf.losses.mean_squared_error(q_target, q_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 01:31:55,616] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video000000.mp4\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "\n",
    "# construct optimization op (with gradient clipping)\n",
    "learning_rate = tf.placeholder(tf.float32, (), name=\"learning_rate\")\n",
    "optimizer = optimizer_spec.constructor(learning_rate=learning_rate, **optimizer_spec.kwargs)\n",
    "train_fn = minimize_and_clip(optimizer, total_error,\n",
    "             var_list=q_func_vars, clip_val=grad_norm_clipping)\n",
    "\n",
    "# update_target_fn will be called periodically to copy Q network to target Q network\n",
    "update_target_fn = []\n",
    "for var, var_target in zip(sorted(q_func_vars,        key=lambda v: v.name),\n",
    "                           sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "    update_target_fn.append(var_target.assign(var))\n",
    "update_target_fn = tf.group(*update_target_fn)\n",
    "\n",
    "# construct the replay buffer\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n",
    "\n",
    "###############\n",
    "# RUN ENV     #\n",
    "###############\n",
    "model_initialized = False\n",
    "num_param_updates = 0\n",
    "mean_episode_reward      = -float('nan')\n",
    "best_mean_episode_reward = -float('inf')\n",
    "last_obs = env.reset()\n",
    "LOG_EVERY_N_STEPS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 0\n",
      "mean reward (100 episodes) -19.760000\n",
      "best mean reward -19.720000\n",
      "episodes 380\n",
      "exploration 1.000000\n",
      "learning_rate 0.000100\n",
      "Timestep 10000\n",
      "mean reward (100 episodes) -19.780000\n",
      "best mean reward -19.690000\n",
      "episodes 390\n",
      "exploration 0.991000\n",
      "learning_rate 0.000100\n",
      "Timestep 20000\n",
      "mean reward (100 episodes) -19.770000\n",
      "best mean reward -19.690000\n",
      "episodes 400\n",
      "exploration 0.982000\n",
      "learning_rate 0.000100\n",
      "Timestep 30000\n",
      "mean reward (100 episodes) -19.840000\n",
      "best mean reward -19.690000\n",
      "episodes 411\n",
      "exploration 0.973000\n",
      "learning_rate 0.000097\n",
      "Timestep 40000\n",
      "mean reward (100 episodes) -19.880000\n",
      "best mean reward -19.690000\n",
      "episodes 422\n",
      "exploration 0.964000\n",
      "learning_rate 0.000092\n",
      "Timestep 50000\n",
      "mean reward (100 episodes) -19.910000\n",
      "best mean reward -19.690000\n",
      "episodes 433\n",
      "exploration 0.955000\n",
      "learning_rate 0.000087\n",
      "Timestep 60000\n",
      "mean reward (100 episodes) -19.960000\n",
      "best mean reward -19.690000\n",
      "episodes 443\n",
      "exploration 0.946000\n",
      "learning_rate 0.000082\n",
      "Timestep 70000\n",
      "mean reward (100 episodes) -20.020000\n",
      "best mean reward -19.690000\n",
      "episodes 453\n",
      "exploration 0.937000\n",
      "learning_rate 0.000077\n",
      "Timestep 80000\n",
      "mean reward (100 episodes) -20.110000\n",
      "best mean reward -19.690000\n",
      "episodes 464\n",
      "exploration 0.928000\n",
      "learning_rate 0.000073\n",
      "Timestep 90000\n",
      "mean reward (100 episodes) -20.030000\n",
      "best mean reward -19.690000\n",
      "episodes 473\n",
      "exploration 0.919000\n",
      "learning_rate 0.000068\n",
      "Timestep 100000\n",
      "mean reward (100 episodes) -20.000000\n",
      "best mean reward -19.690000\n",
      "episodes 483\n",
      "exploration 0.910000\n",
      "learning_rate 0.000063\n",
      "Timestep 110000\n",
      "mean reward (100 episodes) -19.980000\n",
      "best mean reward -19.690000\n",
      "episodes 493\n",
      "exploration 0.901000\n",
      "learning_rate 0.000058\n",
      "Timestep 120000\n",
      "mean reward (100 episodes) -19.970000\n",
      "best mean reward -19.690000\n",
      "episodes 503\n",
      "exploration 0.892000\n",
      "learning_rate 0.000053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 02:33:59,576] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 130000\n",
      "mean reward (100 episodes) -19.880000\n",
      "best mean reward -19.690000\n",
      "episodes 512\n",
      "exploration 0.883000\n",
      "learning_rate 0.000050\n",
      "Timestep 140000\n",
      "mean reward (100 episodes) -19.820000\n",
      "best mean reward -19.690000\n",
      "episodes 521\n",
      "exploration 0.874000\n",
      "learning_rate 0.000050\n",
      "Timestep 150000\n",
      "mean reward (100 episodes) -19.780000\n",
      "best mean reward -19.690000\n",
      "episodes 531\n",
      "exploration 0.865000\n",
      "learning_rate 0.000050\n",
      "Timestep 160000\n",
      "mean reward (100 episodes) -19.760000\n",
      "best mean reward -19.690000\n",
      "episodes 541\n",
      "exploration 0.856000\n",
      "learning_rate 0.000050\n",
      "Timestep 170000\n",
      "mean reward (100 episodes) -19.690000\n",
      "best mean reward -19.680000\n",
      "episodes 550\n",
      "exploration 0.847000\n",
      "learning_rate 0.000050\n",
      "Timestep 180000\n",
      "mean reward (100 episodes) -19.670000\n",
      "best mean reward -19.620000\n",
      "episodes 560\n",
      "exploration 0.838000\n",
      "learning_rate 0.000050\n",
      "Timestep 190000\n",
      "mean reward (100 episodes) -19.610000\n",
      "best mean reward -19.580000\n",
      "episodes 568\n",
      "exploration 0.829000\n",
      "learning_rate 0.000050\n",
      "Timestep 200000\n",
      "mean reward (100 episodes) -19.620000\n",
      "best mean reward -19.580000\n",
      "episodes 577\n",
      "exploration 0.820000\n",
      "learning_rate 0.000050\n",
      "Timestep 210000\n",
      "mean reward (100 episodes) -19.540000\n",
      "best mean reward -19.540000\n",
      "episodes 586\n",
      "exploration 0.811000\n",
      "learning_rate 0.000050\n",
      "Timestep 220000\n",
      "mean reward (100 episodes) -19.510000\n",
      "best mean reward -19.510000\n",
      "episodes 595\n",
      "exploration 0.802000\n",
      "learning_rate 0.000050\n",
      "Timestep 230000\n",
      "mean reward (100 episodes) -19.380000\n",
      "best mean reward -19.370000\n",
      "episodes 603\n",
      "exploration 0.793000\n",
      "learning_rate 0.000050\n",
      "Timestep 240000\n",
      "mean reward (100 episodes) -19.330000\n",
      "best mean reward -19.310000\n",
      "episodes 612\n",
      "exploration 0.784000\n",
      "learning_rate 0.000050\n",
      "Timestep 250000\n",
      "mean reward (100 episodes) -19.250000\n",
      "best mean reward -19.250000\n",
      "episodes 620\n",
      "exploration 0.775000\n",
      "learning_rate 0.000050\n",
      "Timestep 260000\n",
      "mean reward (100 episodes) -19.190000\n",
      "best mean reward -19.190000\n",
      "episodes 629\n",
      "exploration 0.766000\n",
      "learning_rate 0.000050\n",
      "Timestep 270000\n",
      "mean reward (100 episodes) -19.130000\n",
      "best mean reward -19.120000\n",
      "episodes 637\n",
      "exploration 0.757000\n",
      "learning_rate 0.000050\n",
      "Timestep 280000\n",
      "mean reward (100 episodes) -19.070000\n",
      "best mean reward -19.070000\n",
      "episodes 645\n",
      "exploration 0.748000\n",
      "learning_rate 0.000050\n",
      "Timestep 290000\n",
      "mean reward (100 episodes) -19.050000\n",
      "best mean reward -19.030000\n",
      "episodes 653\n",
      "exploration 0.739000\n",
      "learning_rate 0.000050\n",
      "Timestep 300000\n",
      "mean reward (100 episodes) -18.950000\n",
      "best mean reward -18.940000\n",
      "episodes 661\n",
      "exploration 0.730000\n",
      "learning_rate 0.000050\n",
      "Timestep 310000\n",
      "mean reward (100 episodes) -18.910000\n",
      "best mean reward -18.910000\n",
      "episodes 670\n",
      "exploration 0.721000\n",
      "learning_rate 0.000050\n",
      "Timestep 320000\n",
      "mean reward (100 episodes) -18.910000\n",
      "best mean reward -18.910000\n",
      "episodes 678\n",
      "exploration 0.712000\n",
      "learning_rate 0.000050\n",
      "Timestep 330000\n",
      "mean reward (100 episodes) -18.890000\n",
      "best mean reward -18.860000\n",
      "episodes 686\n",
      "exploration 0.703000\n",
      "learning_rate 0.000050\n",
      "Timestep 340000\n",
      "mean reward (100 episodes) -18.850000\n",
      "best mean reward -18.850000\n",
      "episodes 693\n",
      "exploration 0.694000\n",
      "learning_rate 0.000050\n",
      "Timestep 350000\n",
      "mean reward (100 episodes) -18.920000\n",
      "best mean reward -18.830000\n",
      "episodes 701\n",
      "exploration 0.685000\n",
      "learning_rate 0.000050\n",
      "Timestep 360000\n",
      "mean reward (100 episodes) -18.850000\n",
      "best mean reward -18.830000\n",
      "episodes 708\n",
      "exploration 0.676000\n",
      "learning_rate 0.000050\n",
      "Timestep 370000\n",
      "mean reward (100 episodes) -18.750000\n",
      "best mean reward -18.730000\n",
      "episodes 716\n",
      "exploration 0.667000\n",
      "learning_rate 0.000050\n",
      "Timestep 380000\n",
      "mean reward (100 episodes) -18.690000\n",
      "best mean reward -18.690000\n",
      "episodes 723\n",
      "exploration 0.658000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 02:46:26,232] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 390000\n",
      "mean reward (100 episodes) -18.660000\n",
      "best mean reward -18.640000\n",
      "episodes 731\n",
      "exploration 0.649000\n",
      "learning_rate 0.000050\n",
      "Timestep 400000\n",
      "mean reward (100 episodes) -18.540000\n",
      "best mean reward -18.540000\n",
      "episodes 738\n",
      "exploration 0.640000\n",
      "learning_rate 0.000050\n",
      "Timestep 410000\n",
      "mean reward (100 episodes) -18.510000\n",
      "best mean reward -18.490000\n",
      "episodes 745\n",
      "exploration 0.631000\n",
      "learning_rate 0.000050\n",
      "Timestep 420000\n",
      "mean reward (100 episodes) -18.390000\n",
      "best mean reward -18.390000\n",
      "episodes 751\n",
      "exploration 0.622000\n",
      "learning_rate 0.000050\n",
      "Timestep 430000\n",
      "mean reward (100 episodes) -18.360000\n",
      "best mean reward -18.360000\n",
      "episodes 758\n",
      "exploration 0.613000\n",
      "learning_rate 0.000050\n",
      "Timestep 440000\n",
      "mean reward (100 episodes) -18.270000\n",
      "best mean reward -18.250000\n",
      "episodes 765\n",
      "exploration 0.604000\n",
      "learning_rate 0.000050\n",
      "Timestep 450000\n",
      "mean reward (100 episodes) -18.160000\n",
      "best mean reward -18.160000\n",
      "episodes 772\n",
      "exploration 0.595000\n",
      "learning_rate 0.000050\n",
      "Timestep 460000\n",
      "mean reward (100 episodes) -18.060000\n",
      "best mean reward -18.060000\n",
      "episodes 779\n",
      "exploration 0.586000\n",
      "learning_rate 0.000050\n",
      "Timestep 470000\n",
      "mean reward (100 episodes) -18.010000\n",
      "best mean reward -18.010000\n",
      "episodes 786\n",
      "exploration 0.577000\n",
      "learning_rate 0.000050\n",
      "Timestep 480000\n",
      "mean reward (100 episodes) -17.860000\n",
      "best mean reward -17.860000\n",
      "episodes 793\n",
      "exploration 0.568000\n",
      "learning_rate 0.000050\n",
      "Timestep 490000\n",
      "mean reward (100 episodes) -17.790000\n",
      "best mean reward -17.780000\n",
      "episodes 800\n",
      "exploration 0.559000\n",
      "learning_rate 0.000050\n",
      "Timestep 500000\n",
      "mean reward (100 episodes) -17.710000\n",
      "best mean reward -17.710000\n",
      "episodes 806\n",
      "exploration 0.550000\n",
      "learning_rate 0.000050\n",
      "Timestep 510000\n",
      "mean reward (100 episodes) -17.730000\n",
      "best mean reward -17.680000\n",
      "episodes 813\n",
      "exploration 0.541000\n",
      "learning_rate 0.000050\n",
      "Timestep 520000\n",
      "mean reward (100 episodes) -17.670000\n",
      "best mean reward -17.670000\n",
      "episodes 819\n",
      "exploration 0.532000\n",
      "learning_rate 0.000050\n",
      "Timestep 530000\n",
      "mean reward (100 episodes) -17.670000\n",
      "best mean reward -17.610000\n",
      "episodes 826\n",
      "exploration 0.523000\n",
      "learning_rate 0.000050\n",
      "Timestep 540000\n",
      "mean reward (100 episodes) -17.630000\n",
      "best mean reward -17.610000\n",
      "episodes 832\n",
      "exploration 0.514000\n",
      "learning_rate 0.000050\n",
      "Timestep 550000\n",
      "mean reward (100 episodes) -17.610000\n",
      "best mean reward -17.610000\n",
      "episodes 839\n",
      "exploration 0.505000\n",
      "learning_rate 0.000050\n",
      "Timestep 560000\n",
      "mean reward (100 episodes) -17.470000\n",
      "best mean reward -17.470000\n",
      "episodes 843\n",
      "exploration 0.496000\n",
      "learning_rate 0.000050\n",
      "Timestep 570000\n",
      "mean reward (100 episodes) -17.390000\n",
      "best mean reward -17.360000\n",
      "episodes 849\n",
      "exploration 0.487000\n",
      "learning_rate 0.000050\n",
      "Timestep 580000\n",
      "mean reward (100 episodes) -17.320000\n",
      "best mean reward -17.310000\n",
      "episodes 855\n",
      "exploration 0.478000\n",
      "learning_rate 0.000050\n",
      "Timestep 590000\n",
      "mean reward (100 episodes) -17.240000\n",
      "best mean reward -17.230000\n",
      "episodes 861\n",
      "exploration 0.469000\n",
      "learning_rate 0.000050\n",
      "Timestep 600000\n",
      "mean reward (100 episodes) -17.090000\n",
      "best mean reward -17.090000\n",
      "episodes 866\n",
      "exploration 0.460000\n",
      "learning_rate 0.000050\n",
      "Timestep 610000\n",
      "mean reward (100 episodes) -16.970000\n",
      "best mean reward -16.970000\n",
      "episodes 872\n",
      "exploration 0.451000\n",
      "learning_rate 0.000050\n",
      "Timestep 620000\n",
      "mean reward (100 episodes) -17.010000\n",
      "best mean reward -16.940000\n",
      "episodes 878\n",
      "exploration 0.442000\n",
      "learning_rate 0.000050\n",
      "Timestep 630000\n",
      "mean reward (100 episodes) -16.870000\n",
      "best mean reward -16.860000\n",
      "episodes 883\n",
      "exploration 0.433000\n",
      "learning_rate 0.000050\n",
      "Timestep 640000\n",
      "mean reward (100 episodes) -16.820000\n",
      "best mean reward -16.790000\n",
      "episodes 889\n",
      "exploration 0.424000\n",
      "learning_rate 0.000050\n",
      "Timestep 650000\n",
      "mean reward (100 episodes) -16.750000\n",
      "best mean reward -16.750000\n",
      "episodes 893\n",
      "exploration 0.415000\n",
      "learning_rate 0.000050\n",
      "Timestep 660000\n",
      "mean reward (100 episodes) -16.520000\n",
      "best mean reward -16.520000\n",
      "episodes 898\n",
      "exploration 0.406000\n",
      "learning_rate 0.000050\n",
      "Timestep 670000\n",
      "mean reward (100 episodes) -16.400000\n",
      "best mean reward -16.400000\n",
      "episodes 903\n",
      "exploration 0.397000\n",
      "learning_rate 0.000050\n",
      "Timestep 680000\n",
      "mean reward (100 episodes) -16.220000\n",
      "best mean reward -16.220000\n",
      "episodes 908\n",
      "exploration 0.388000\n",
      "learning_rate 0.000050\n",
      "Timestep 690000\n",
      "mean reward (100 episodes) -16.120000\n",
      "best mean reward -16.120000\n",
      "episodes 913\n",
      "exploration 0.379000\n",
      "learning_rate 0.000050\n",
      "Timestep 700000\n",
      "mean reward (100 episodes) -16.050000\n",
      "best mean reward -16.050000\n",
      "episodes 918\n",
      "exploration 0.370000\n",
      "learning_rate 0.000050\n",
      "Timestep 710000\n",
      "mean reward (100 episodes) -16.100000\n",
      "best mean reward -16.050000\n",
      "episodes 922\n",
      "exploration 0.361000\n",
      "learning_rate 0.000050\n",
      "Timestep 720000\n",
      "mean reward (100 episodes) -15.820000\n",
      "best mean reward -15.820000\n",
      "episodes 927\n",
      "exploration 0.352000\n",
      "learning_rate 0.000050\n",
      "Timestep 730000\n",
      "mean reward (100 episodes) -15.630000\n",
      "best mean reward -15.630000\n",
      "episodes 931\n",
      "exploration 0.343000\n",
      "learning_rate 0.000050\n",
      "Timestep 740000\n",
      "mean reward (100 episodes) -15.420000\n",
      "best mean reward -15.420000\n",
      "episodes 935\n",
      "exploration 0.334000\n",
      "learning_rate 0.000050\n",
      "Timestep 750000\n",
      "mean reward (100 episodes) -15.300000\n",
      "best mean reward -15.300000\n",
      "episodes 939\n",
      "exploration 0.325000\n",
      "learning_rate 0.000050\n",
      "Timestep 760000\n",
      "mean reward (100 episodes) -15.180000\n",
      "best mean reward -15.130000\n",
      "episodes 944\n",
      "exploration 0.316000\n",
      "learning_rate 0.000050\n",
      "Timestep 770000\n",
      "mean reward (100 episodes) -14.950000\n",
      "best mean reward -14.950000\n",
      "episodes 948\n",
      "exploration 0.307000\n",
      "learning_rate 0.000050\n",
      "Timestep 780000\n",
      "mean reward (100 episodes) -14.760000\n",
      "best mean reward -14.760000\n",
      "episodes 952\n",
      "exploration 0.298000\n",
      "learning_rate 0.000050\n",
      "Timestep 790000\n",
      "mean reward (100 episodes) -14.580000\n",
      "best mean reward -14.570000\n",
      "episodes 956\n",
      "exploration 0.289000\n",
      "learning_rate 0.000050\n",
      "Timestep 800000\n",
      "mean reward (100 episodes) -14.360000\n",
      "best mean reward -14.360000\n",
      "episodes 960\n",
      "exploration 0.280000\n",
      "learning_rate 0.000050\n",
      "Timestep 810000\n",
      "mean reward (100 episodes) -14.310000\n",
      "best mean reward -14.290000\n",
      "episodes 964\n",
      "exploration 0.271000\n",
      "learning_rate 0.000050\n",
      "Timestep 820000\n",
      "mean reward (100 episodes) -14.020000\n",
      "best mean reward -14.020000\n",
      "episodes 968\n",
      "exploration 0.262000\n",
      "learning_rate 0.000050\n",
      "Timestep 830000\n",
      "mean reward (100 episodes) -13.900000\n",
      "best mean reward -13.900000\n",
      "episodes 971\n",
      "exploration 0.253000\n",
      "learning_rate 0.000050\n",
      "Timestep 840000\n",
      "mean reward (100 episodes) -13.620000\n",
      "best mean reward -13.620000\n",
      "episodes 976\n",
      "exploration 0.244000\n",
      "learning_rate 0.000050\n",
      "Timestep 850000\n",
      "mean reward (100 episodes) -13.540000\n",
      "best mean reward -13.530000\n",
      "episodes 980\n",
      "exploration 0.235000\n",
      "learning_rate 0.000050\n",
      "Timestep 860000\n",
      "mean reward (100 episodes) -13.450000\n",
      "best mean reward -13.440000\n",
      "episodes 984\n",
      "exploration 0.226000\n",
      "learning_rate 0.000050\n",
      "Timestep 870000\n",
      "mean reward (100 episodes) -13.210000\n",
      "best mean reward -13.210000\n",
      "episodes 987\n",
      "exploration 0.217000\n",
      "learning_rate 0.000050\n",
      "Timestep 880000\n",
      "mean reward (100 episodes) -13.030000\n",
      "best mean reward -13.030000\n",
      "episodes 991\n",
      "exploration 0.208000\n",
      "learning_rate 0.000050\n",
      "Timestep 890000\n",
      "mean reward (100 episodes) -13.000000\n",
      "best mean reward -13.000000\n",
      "episodes 995\n",
      "exploration 0.199000\n",
      "learning_rate 0.000050\n",
      "Timestep 900000\n",
      "mean reward (100 episodes) -12.790000\n",
      "best mean reward -12.790000\n",
      "episodes 998\n",
      "exploration 0.190000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 03:13:40,160] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 910000\n",
      "mean reward (100 episodes) -12.470000\n",
      "best mean reward -12.470000\n",
      "episodes 1001\n",
      "exploration 0.181000\n",
      "learning_rate 0.000050\n",
      "Timestep 920000\n",
      "mean reward (100 episodes) -12.410000\n",
      "best mean reward -12.410000\n",
      "episodes 1004\n",
      "exploration 0.172000\n",
      "learning_rate 0.000050\n",
      "Timestep 930000\n",
      "mean reward (100 episodes) -12.150000\n",
      "best mean reward -12.150000\n",
      "episodes 1007\n",
      "exploration 0.163000\n",
      "learning_rate 0.000050\n",
      "Timestep 940000\n",
      "mean reward (100 episodes) -11.920000\n",
      "best mean reward -11.920000\n",
      "episodes 1010\n",
      "exploration 0.154000\n",
      "learning_rate 0.000050\n",
      "Timestep 950000\n",
      "mean reward (100 episodes) -11.640000\n",
      "best mean reward -11.640000\n",
      "episodes 1013\n",
      "exploration 0.145000\n",
      "learning_rate 0.000050\n",
      "Timestep 960000\n",
      "mean reward (100 episodes) -11.180000\n",
      "best mean reward -11.180000\n",
      "episodes 1016\n",
      "exploration 0.136000\n",
      "learning_rate 0.000050\n",
      "Timestep 970000\n",
      "mean reward (100 episodes) -10.860000\n",
      "best mean reward -10.860000\n",
      "episodes 1018\n",
      "exploration 0.127000\n",
      "learning_rate 0.000050\n",
      "Timestep 980000\n",
      "mean reward (100 episodes) -10.340000\n",
      "best mean reward -10.340000\n",
      "episodes 1021\n",
      "exploration 0.118000\n",
      "learning_rate 0.000050\n",
      "Timestep 990000\n",
      "mean reward (100 episodes) -9.930000\n",
      "best mean reward -9.930000\n",
      "episodes 1024\n",
      "exploration 0.109000\n",
      "learning_rate 0.000050\n",
      "Timestep 1000000\n",
      "mean reward (100 episodes) -9.770000\n",
      "best mean reward -9.770000\n",
      "episodes 1027\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1010000\n",
      "mean reward (100 episodes) -9.610000\n",
      "best mean reward -9.610000\n",
      "episodes 1029\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1020000\n",
      "mean reward (100 episodes) -9.300000\n",
      "best mean reward -9.300000\n",
      "episodes 1032\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1030000\n",
      "mean reward (100 episodes) -8.990000\n",
      "best mean reward -8.990000\n",
      "episodes 1035\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1040000\n",
      "mean reward (100 episodes) -8.670000\n",
      "best mean reward -8.670000\n",
      "episodes 1038\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1050000\n",
      "mean reward (100 episodes) -8.490000\n",
      "best mean reward -8.490000\n",
      "episodes 1041\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1060000\n",
      "mean reward (100 episodes) -8.220000\n",
      "best mean reward -8.220000\n",
      "episodes 1043\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1070000\n",
      "mean reward (100 episodes) -8.020000\n",
      "best mean reward -8.020000\n",
      "episodes 1046\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1080000\n",
      "mean reward (100 episodes) -7.810000\n",
      "best mean reward -7.810000\n",
      "episodes 1049\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1090000\n",
      "mean reward (100 episodes) -7.620000\n",
      "best mean reward -7.620000\n",
      "episodes 1051\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1100000\n",
      "mean reward (100 episodes) -7.520000\n",
      "best mean reward -7.500000\n",
      "episodes 1054\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1110000\n",
      "mean reward (100 episodes) -7.280000\n",
      "best mean reward -7.280000\n",
      "episodes 1056\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1120000\n",
      "mean reward (100 episodes) -7.100000\n",
      "best mean reward -7.100000\n",
      "episodes 1059\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1130000\n",
      "mean reward (100 episodes) -6.730000\n",
      "best mean reward -6.730000\n",
      "episodes 1062\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1140000\n",
      "mean reward (100 episodes) -6.400000\n",
      "best mean reward -6.400000\n",
      "episodes 1064\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1150000\n",
      "mean reward (100 episodes) -6.320000\n",
      "best mean reward -6.320000\n",
      "episodes 1067\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1160000\n",
      "mean reward (100 episodes) -6.130000\n",
      "best mean reward -6.130000\n",
      "episodes 1070\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1170000\n",
      "mean reward (100 episodes) -6.010000\n",
      "best mean reward -6.010000\n",
      "episodes 1073\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1180000\n",
      "mean reward (100 episodes) -5.770000\n",
      "best mean reward -5.770000\n",
      "episodes 1075\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1190000\n",
      "mean reward (100 episodes) -5.460000\n",
      "best mean reward -5.460000\n",
      "episodes 1078\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1200000\n",
      "mean reward (100 episodes) -5.190000\n",
      "best mean reward -5.190000\n",
      "episodes 1081\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1210000\n",
      "mean reward (100 episodes) -4.820000\n",
      "best mean reward -4.820000\n",
      "episodes 1084\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1220000\n",
      "mean reward (100 episodes) -4.700000\n",
      "best mean reward -4.700000\n",
      "episodes 1086\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1230000\n",
      "mean reward (100 episodes) -4.250000\n",
      "best mean reward -4.250000\n",
      "episodes 1089\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1240000\n",
      "mean reward (100 episodes) -3.970000\n",
      "best mean reward -3.970000\n",
      "episodes 1091\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1250000\n",
      "mean reward (100 episodes) -3.440000\n",
      "best mean reward -3.440000\n",
      "episodes 1094\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1260000\n",
      "mean reward (100 episodes) -3.060000\n",
      "best mean reward -3.060000\n",
      "episodes 1097\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1270000\n",
      "mean reward (100 episodes) -2.920000\n",
      "best mean reward -2.920000\n",
      "episodes 1099\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1280000\n",
      "mean reward (100 episodes) -2.630000\n",
      "best mean reward -2.630000\n",
      "episodes 1102\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1290000\n",
      "mean reward (100 episodes) -2.400000\n",
      "best mean reward -2.400000\n",
      "episodes 1105\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1300000\n",
      "mean reward (100 episodes) -2.270000\n",
      "best mean reward -2.270000\n",
      "episodes 1107\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1310000\n",
      "mean reward (100 episodes) -2.170000\n",
      "best mean reward -2.170000\n",
      "episodes 1110\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1320000\n",
      "mean reward (100 episodes) -2.020000\n",
      "best mean reward -2.020000\n",
      "episodes 1112\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1330000\n",
      "mean reward (100 episodes) -1.930000\n",
      "best mean reward -1.850000\n",
      "episodes 1115\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1340000\n",
      "mean reward (100 episodes) -1.920000\n",
      "best mean reward -1.820000\n",
      "episodes 1118\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1350000\n",
      "mean reward (100 episodes) -1.720000\n",
      "best mean reward -1.720000\n",
      "episodes 1121\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1360000\n",
      "mean reward (100 episodes) -1.710000\n",
      "best mean reward -1.710000\n",
      "episodes 1123\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1370000\n",
      "mean reward (100 episodes) -1.360000\n",
      "best mean reward -1.360000\n",
      "episodes 1126\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1380000\n",
      "mean reward (100 episodes) -1.320000\n",
      "best mean reward -1.320000\n",
      "episodes 1128\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1390000\n",
      "mean reward (100 episodes) -0.990000\n",
      "best mean reward -0.990000\n",
      "episodes 1131\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1400000\n",
      "mean reward (100 episodes) -0.650000\n",
      "best mean reward -0.650000\n",
      "episodes 1134\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1410000\n",
      "mean reward (100 episodes) -0.610000\n",
      "best mean reward -0.610000\n",
      "episodes 1136\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1420000\n",
      "mean reward (100 episodes) -0.490000\n",
      "best mean reward -0.490000\n",
      "episodes 1139\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1430000\n",
      "mean reward (100 episodes) -0.180000\n",
      "best mean reward -0.180000\n",
      "episodes 1142\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1440000\n",
      "mean reward (100 episodes) 0.160000\n",
      "best mean reward 0.160000\n",
      "episodes 1145\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1450000\n",
      "mean reward (100 episodes) 0.540000\n",
      "best mean reward 0.540000\n",
      "episodes 1148\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1460000\n",
      "mean reward (100 episodes) 0.740000\n",
      "best mean reward 0.740000\n",
      "episodes 1150\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1470000\n",
      "mean reward (100 episodes) 1.100000\n",
      "best mean reward 1.100000\n",
      "episodes 1153\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1480000\n",
      "mean reward (100 episodes) 1.240000\n",
      "best mean reward 1.240000\n",
      "episodes 1155\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1490000\n",
      "mean reward (100 episodes) 1.490000\n",
      "best mean reward 1.490000\n",
      "episodes 1158\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 1500000\n",
      "mean reward (100 episodes) 1.720000\n",
      "best mean reward 1.730000\n",
      "episodes 1161\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1510000\n",
      "mean reward (100 episodes) 1.860000\n",
      "best mean reward 1.860000\n",
      "episodes 1164\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1520000\n",
      "mean reward (100 episodes) 2.130000\n",
      "best mean reward 2.130000\n",
      "episodes 1166\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1530000\n",
      "mean reward (100 episodes) 2.450000\n",
      "best mean reward 2.450000\n",
      "episodes 1169\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1540000\n",
      "mean reward (100 episodes) 2.870000\n",
      "best mean reward 2.870000\n",
      "episodes 1172\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1550000\n",
      "mean reward (100 episodes) 3.080000\n",
      "best mean reward 3.080000\n",
      "episodes 1175\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1560000\n",
      "mean reward (100 episodes) 3.310000\n",
      "best mean reward 3.310000\n",
      "episodes 1177\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1570000\n",
      "mean reward (100 episodes) 3.940000\n",
      "best mean reward 3.940000\n",
      "episodes 1181\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1580000\n",
      "mean reward (100 episodes) 4.270000\n",
      "best mean reward 4.270000\n",
      "episodes 1184\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1590000\n",
      "mean reward (100 episodes) 4.560000\n",
      "best mean reward 4.560000\n",
      "episodes 1187\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1600000\n",
      "mean reward (100 episodes) 4.800000\n",
      "best mean reward 4.800000\n",
      "episodes 1190\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1610000\n",
      "mean reward (100 episodes) 5.080000\n",
      "best mean reward 5.080000\n",
      "episodes 1194\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1620000\n",
      "mean reward (100 episodes) 5.360000\n",
      "best mean reward 5.360000\n",
      "episodes 1197\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1630000\n",
      "mean reward (100 episodes) 5.860000\n",
      "best mean reward 5.860000\n",
      "episodes 1201\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1640000\n",
      "mean reward (100 episodes) 6.300000\n",
      "best mean reward 6.300000\n",
      "episodes 1205\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1650000\n",
      "mean reward (100 episodes) 6.970000\n",
      "best mean reward 6.970000\n",
      "episodes 1209\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1660000\n",
      "mean reward (100 episodes) 7.450000\n",
      "best mean reward 7.450000\n",
      "episodes 1213\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1670000\n",
      "mean reward (100 episodes) 8.080000\n",
      "best mean reward 8.080000\n",
      "episodes 1217\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1680000\n",
      "mean reward (100 episodes) 8.500000\n",
      "best mean reward 8.500000\n",
      "episodes 1221\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1690000\n",
      "mean reward (100 episodes) 9.000000\n",
      "best mean reward 9.000000\n",
      "episodes 1225\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1700000\n",
      "mean reward (100 episodes) 9.400000\n",
      "best mean reward 9.400000\n",
      "episodes 1229\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1710000\n",
      "mean reward (100 episodes) 9.520000\n",
      "best mean reward 9.570000\n",
      "episodes 1233\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1720000\n",
      "mean reward (100 episodes) 9.780000\n",
      "best mean reward 9.780000\n",
      "episodes 1236\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1730000\n",
      "mean reward (100 episodes) 10.080000\n",
      "best mean reward 10.080000\n",
      "episodes 1240\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1740000\n",
      "mean reward (100 episodes) 10.200000\n",
      "best mean reward 10.210000\n",
      "episodes 1243\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1750000\n",
      "mean reward (100 episodes) 10.080000\n",
      "best mean reward 10.210000\n",
      "episodes 1247\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1760000\n",
      "mean reward (100 episodes) 10.070000\n",
      "best mean reward 10.210000\n",
      "episodes 1250\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1770000\n",
      "mean reward (100 episodes) 10.330000\n",
      "best mean reward 10.330000\n",
      "episodes 1254\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1780000\n",
      "mean reward (100 episodes) 10.500000\n",
      "best mean reward 10.500000\n",
      "episodes 1257\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1790000\n",
      "mean reward (100 episodes) 10.650000\n",
      "best mean reward 10.650000\n",
      "episodes 1261\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1800000\n",
      "mean reward (100 episodes) 10.640000\n",
      "best mean reward 10.670000\n",
      "episodes 1264\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1810000\n",
      "mean reward (100 episodes) 10.800000\n",
      "best mean reward 10.800000\n",
      "episodes 1268\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1820000\n",
      "mean reward (100 episodes) 10.850000\n",
      "best mean reward 10.880000\n",
      "episodes 1271\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1830000\n",
      "mean reward (100 episodes) 10.750000\n",
      "best mean reward 10.880000\n",
      "episodes 1274\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1840000\n",
      "mean reward (100 episodes) 10.700000\n",
      "best mean reward 10.880000\n",
      "episodes 1278\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1850000\n",
      "mean reward (100 episodes) 10.870000\n",
      "best mean reward 10.880000\n",
      "episodes 1282\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1860000\n",
      "mean reward (100 episodes) 11.070000\n",
      "best mean reward 11.070000\n",
      "episodes 1286\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1870000\n",
      "mean reward (100 episodes) 11.250000\n",
      "best mean reward 11.250000\n",
      "episodes 1290\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1880000\n",
      "mean reward (100 episodes) 11.370000\n",
      "best mean reward 11.380000\n",
      "episodes 1295\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1890000\n",
      "mean reward (100 episodes) 11.420000\n",
      "best mean reward 11.490000\n",
      "episodes 1299\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1900000\n",
      "mean reward (100 episodes) 11.580000\n",
      "best mean reward 11.580000\n",
      "episodes 1303\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1910000\n",
      "mean reward (100 episodes) 11.550000\n",
      "best mean reward 11.580000\n",
      "episodes 1307\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1920000\n",
      "mean reward (100 episodes) 11.230000\n",
      "best mean reward 11.580000\n",
      "episodes 1311\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1930000\n",
      "mean reward (100 episodes) 11.200000\n",
      "best mean reward 11.580000\n",
      "episodes 1315\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1940000\n",
      "mean reward (100 episodes) 11.160000\n",
      "best mean reward 11.580000\n",
      "episodes 1319\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1950000\n",
      "mean reward (100 episodes) 11.160000\n",
      "best mean reward 11.580000\n",
      "episodes 1323\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1960000\n",
      "mean reward (100 episodes) 11.190000\n",
      "best mean reward 11.580000\n",
      "episodes 1327\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1970000\n",
      "mean reward (100 episodes) 11.100000\n",
      "best mean reward 11.580000\n",
      "episodes 1331\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1980000\n",
      "mean reward (100 episodes) 11.370000\n",
      "best mean reward 11.580000\n",
      "episodes 1336\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 1990000\n",
      "mean reward (100 episodes) 11.460000\n",
      "best mean reward 11.580000\n",
      "episodes 1339\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2000000\n",
      "mean reward (100 episodes) 11.580000\n",
      "best mean reward 11.580000\n",
      "episodes 1343\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2010000\n",
      "mean reward (100 episodes) 11.920000\n",
      "best mean reward 11.920000\n",
      "episodes 1348\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2020000\n",
      "mean reward (100 episodes) 12.210000\n",
      "best mean reward 12.210000\n",
      "episodes 1352\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2030000\n",
      "mean reward (100 episodes) 12.390000\n",
      "best mean reward 12.390000\n",
      "episodes 1357\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2040000\n",
      "mean reward (100 episodes) 12.590000\n",
      "best mean reward 12.590000\n",
      "episodes 1361\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2050000\n",
      "mean reward (100 episodes) 12.920000\n",
      "best mean reward 12.920000\n",
      "episodes 1365\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2060000\n",
      "mean reward (100 episodes) 13.210000\n",
      "best mean reward 13.210000\n",
      "episodes 1370\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2070000\n",
      "mean reward (100 episodes) 13.770000\n",
      "best mean reward 13.770000\n",
      "episodes 1374\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2080000\n",
      "mean reward (100 episodes) 13.990000\n",
      "best mean reward 13.990000\n",
      "episodes 1378\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2090000\n",
      "mean reward (100 episodes) 14.070000\n",
      "best mean reward 14.070000\n",
      "episodes 1383\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 2100000\n",
      "mean reward (100 episodes) 14.000000\n",
      "best mean reward 14.070000\n",
      "episodes 1387\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2110000\n",
      "mean reward (100 episodes) 14.110000\n",
      "best mean reward 14.130000\n",
      "episodes 1392\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2120000\n",
      "mean reward (100 episodes) 14.030000\n",
      "best mean reward 14.150000\n",
      "episodes 1396\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2130000\n",
      "mean reward (100 episodes) 14.070000\n",
      "best mean reward 14.150000\n",
      "episodes 1400\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2140000\n",
      "mean reward (100 episodes) 14.190000\n",
      "best mean reward 14.190000\n",
      "episodes 1404\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2150000\n",
      "mean reward (100 episodes) 14.310000\n",
      "best mean reward 14.310000\n",
      "episodes 1408\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2160000\n",
      "mean reward (100 episodes) 14.550000\n",
      "best mean reward 14.550000\n",
      "episodes 1412\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2170000\n",
      "mean reward (100 episodes) 14.450000\n",
      "best mean reward 14.550000\n",
      "episodes 1416\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2180000\n",
      "mean reward (100 episodes) 14.480000\n",
      "best mean reward 14.550000\n",
      "episodes 1420\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2190000\n",
      "mean reward (100 episodes) 14.450000\n",
      "best mean reward 14.550000\n",
      "episodes 1424\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2200000\n",
      "mean reward (100 episodes) 14.300000\n",
      "best mean reward 14.550000\n",
      "episodes 1427\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2210000\n",
      "mean reward (100 episodes) 14.070000\n",
      "best mean reward 14.550000\n",
      "episodes 1431\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2220000\n",
      "mean reward (100 episodes) 13.900000\n",
      "best mean reward 14.550000\n",
      "episodes 1435\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2230000\n",
      "mean reward (100 episodes) 13.660000\n",
      "best mean reward 14.550000\n",
      "episodes 1438\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2240000\n",
      "mean reward (100 episodes) 13.510000\n",
      "best mean reward 14.550000\n",
      "episodes 1441\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2250000\n",
      "mean reward (100 episodes) 13.530000\n",
      "best mean reward 14.550000\n",
      "episodes 1445\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2260000\n",
      "mean reward (100 episodes) 13.410000\n",
      "best mean reward 14.550000\n",
      "episodes 1449\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2270000\n",
      "mean reward (100 episodes) 13.370000\n",
      "best mean reward 14.550000\n",
      "episodes 1453\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2280000\n",
      "mean reward (100 episodes) 13.230000\n",
      "best mean reward 14.550000\n",
      "episodes 1457\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2290000\n",
      "mean reward (100 episodes) 13.270000\n",
      "best mean reward 14.550000\n",
      "episodes 1461\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2300000\n",
      "mean reward (100 episodes) 13.230000\n",
      "best mean reward 14.550000\n",
      "episodes 1465\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2310000\n",
      "mean reward (100 episodes) 13.130000\n",
      "best mean reward 14.550000\n",
      "episodes 1470\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2320000\n",
      "mean reward (100 episodes) 12.980000\n",
      "best mean reward 14.550000\n",
      "episodes 1473\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2330000\n",
      "mean reward (100 episodes) 12.980000\n",
      "best mean reward 14.550000\n",
      "episodes 1477\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2340000\n",
      "mean reward (100 episodes) 12.850000\n",
      "best mean reward 14.550000\n",
      "episodes 1481\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2350000\n",
      "mean reward (100 episodes) 12.820000\n",
      "best mean reward 14.550000\n",
      "episodes 1486\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2360000\n",
      "mean reward (100 episodes) 12.830000\n",
      "best mean reward 14.550000\n",
      "episodes 1490\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2370000\n",
      "mean reward (100 episodes) 12.870000\n",
      "best mean reward 14.550000\n",
      "episodes 1494\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2380000\n",
      "mean reward (100 episodes) 12.820000\n",
      "best mean reward 14.550000\n",
      "episodes 1498\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2390000\n",
      "mean reward (100 episodes) 12.850000\n",
      "best mean reward 14.550000\n",
      "episodes 1503\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2400000\n",
      "mean reward (100 episodes) 12.930000\n",
      "best mean reward 14.550000\n",
      "episodes 1507\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2410000\n",
      "mean reward (100 episodes) 12.950000\n",
      "best mean reward 14.550000\n",
      "episodes 1512\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2420000\n",
      "mean reward (100 episodes) 13.130000\n",
      "best mean reward 14.550000\n",
      "episodes 1516\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2430000\n",
      "mean reward (100 episodes) 13.240000\n",
      "best mean reward 14.550000\n",
      "episodes 1521\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2440000\n",
      "mean reward (100 episodes) 13.310000\n",
      "best mean reward 14.550000\n",
      "episodes 1525\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2450000\n",
      "mean reward (100 episodes) 13.720000\n",
      "best mean reward 14.550000\n",
      "episodes 1530\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2460000\n",
      "mean reward (100 episodes) 14.200000\n",
      "best mean reward 14.550000\n",
      "episodes 1534\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2470000\n",
      "mean reward (100 episodes) 14.420000\n",
      "best mean reward 14.550000\n",
      "episodes 1538\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2480000\n",
      "mean reward (100 episodes) 14.660000\n",
      "best mean reward 14.690000\n",
      "episodes 1543\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2490000\n",
      "mean reward (100 episodes) 14.880000\n",
      "best mean reward 14.880000\n",
      "episodes 1547\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2500000\n",
      "mean reward (100 episodes) 14.900000\n",
      "best mean reward 14.950000\n",
      "episodes 1551\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2510000\n",
      "mean reward (100 episodes) 14.890000\n",
      "best mean reward 14.950000\n",
      "episodes 1555\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2520000\n",
      "mean reward (100 episodes) 15.020000\n",
      "best mean reward 15.050000\n",
      "episodes 1560\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2530000\n",
      "mean reward (100 episodes) 15.110000\n",
      "best mean reward 15.150000\n",
      "episodes 1564\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2540000\n",
      "mean reward (100 episodes) 15.210000\n",
      "best mean reward 15.220000\n",
      "episodes 1568\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2550000\n",
      "mean reward (100 episodes) 15.410000\n",
      "best mean reward 15.410000\n",
      "episodes 1573\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2560000\n",
      "mean reward (100 episodes) 15.500000\n",
      "best mean reward 15.500000\n",
      "episodes 1577\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2570000\n",
      "mean reward (100 episodes) 15.640000\n",
      "best mean reward 15.660000\n",
      "episodes 1581\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2580000\n",
      "mean reward (100 episodes) 15.580000\n",
      "best mean reward 15.660000\n",
      "episodes 1585\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2590000\n",
      "mean reward (100 episodes) 15.640000\n",
      "best mean reward 15.660000\n",
      "episodes 1590\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2600000\n",
      "mean reward (100 episodes) 15.640000\n",
      "best mean reward 15.660000\n",
      "episodes 1594\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2610000\n",
      "mean reward (100 episodes) 15.840000\n",
      "best mean reward 15.840000\n",
      "episodes 1599\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2620000\n",
      "mean reward (100 episodes) 15.840000\n",
      "best mean reward 15.870000\n",
      "episodes 1603\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2630000\n",
      "mean reward (100 episodes) 15.830000\n",
      "best mean reward 15.890000\n",
      "episodes 1608\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2640000\n",
      "mean reward (100 episodes) 15.740000\n",
      "best mean reward 15.890000\n",
      "episodes 1612\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2650000\n",
      "mean reward (100 episodes) 15.730000\n",
      "best mean reward 15.890000\n",
      "episodes 1616\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2660000\n",
      "mean reward (100 episodes) 15.750000\n",
      "best mean reward 15.890000\n",
      "episodes 1620\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2670000\n",
      "mean reward (100 episodes) 15.800000\n",
      "best mean reward 15.890000\n",
      "episodes 1624\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2680000\n",
      "mean reward (100 episodes) 15.720000\n",
      "best mean reward 15.890000\n",
      "episodes 1628\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 2690000\n",
      "mean reward (100 episodes) 15.570000\n",
      "best mean reward 15.890000\n",
      "episodes 1632\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2700000\n",
      "mean reward (100 episodes) 15.540000\n",
      "best mean reward 15.890000\n",
      "episodes 1637\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2710000\n",
      "mean reward (100 episodes) 15.540000\n",
      "best mean reward 15.890000\n",
      "episodes 1641\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2720000\n",
      "mean reward (100 episodes) 15.490000\n",
      "best mean reward 15.890000\n",
      "episodes 1646\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2730000\n",
      "mean reward (100 episodes) 15.560000\n",
      "best mean reward 15.890000\n",
      "episodes 1650\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2740000\n",
      "mean reward (100 episodes) 15.690000\n",
      "best mean reward 15.890000\n",
      "episodes 1654\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2750000\n",
      "mean reward (100 episodes) 15.760000\n",
      "best mean reward 15.890000\n",
      "episodes 1658\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2760000\n",
      "mean reward (100 episodes) 15.780000\n",
      "best mean reward 15.890000\n",
      "episodes 1663\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2770000\n",
      "mean reward (100 episodes) 15.770000\n",
      "best mean reward 15.890000\n",
      "episodes 1667\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2780000\n",
      "mean reward (100 episodes) 15.750000\n",
      "best mean reward 15.890000\n",
      "episodes 1672\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2790000\n",
      "mean reward (100 episodes) 15.760000\n",
      "best mean reward 15.890000\n",
      "episodes 1676\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2800000\n",
      "mean reward (100 episodes) 15.750000\n",
      "best mean reward 15.890000\n",
      "episodes 1681\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2810000\n",
      "mean reward (100 episodes) 15.800000\n",
      "best mean reward 15.890000\n",
      "episodes 1685\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2820000\n",
      "mean reward (100 episodes) 15.820000\n",
      "best mean reward 15.890000\n",
      "episodes 1689\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2830000\n",
      "mean reward (100 episodes) 15.810000\n",
      "best mean reward 15.890000\n",
      "episodes 1694\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2840000\n",
      "mean reward (100 episodes) 15.760000\n",
      "best mean reward 15.890000\n",
      "episodes 1698\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2850000\n",
      "mean reward (100 episodes) 15.790000\n",
      "best mean reward 15.890000\n",
      "episodes 1703\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2860000\n",
      "mean reward (100 episodes) 15.850000\n",
      "best mean reward 15.890000\n",
      "episodes 1708\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2870000\n",
      "mean reward (100 episodes) 15.880000\n",
      "best mean reward 15.890000\n",
      "episodes 1712\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2880000\n",
      "mean reward (100 episodes) 15.920000\n",
      "best mean reward 15.970000\n",
      "episodes 1716\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2890000\n",
      "mean reward (100 episodes) 15.880000\n",
      "best mean reward 15.970000\n",
      "episodes 1720\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2900000\n",
      "mean reward (100 episodes) 15.830000\n",
      "best mean reward 15.970000\n",
      "episodes 1724\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2910000\n",
      "mean reward (100 episodes) 15.900000\n",
      "best mean reward 15.970000\n",
      "episodes 1729\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2920000\n",
      "mean reward (100 episodes) 15.720000\n",
      "best mean reward 15.970000\n",
      "episodes 1733\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2930000\n",
      "mean reward (100 episodes) 15.680000\n",
      "best mean reward 15.970000\n",
      "episodes 1737\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2940000\n",
      "mean reward (100 episodes) 15.730000\n",
      "best mean reward 15.970000\n",
      "episodes 1741\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2950000\n",
      "mean reward (100 episodes) 15.720000\n",
      "best mean reward 15.970000\n",
      "episodes 1746\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2960000\n",
      "mean reward (100 episodes) 15.790000\n",
      "best mean reward 15.970000\n",
      "episodes 1750\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2970000\n",
      "mean reward (100 episodes) 15.800000\n",
      "best mean reward 15.970000\n",
      "episodes 1754\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2980000\n",
      "mean reward (100 episodes) 15.740000\n",
      "best mean reward 15.970000\n",
      "episodes 1759\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 2990000\n",
      "mean reward (100 episodes) 15.640000\n",
      "best mean reward 15.970000\n",
      "episodes 1763\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3000000\n",
      "mean reward (100 episodes) 15.600000\n",
      "best mean reward 15.970000\n",
      "episodes 1767\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3010000\n",
      "mean reward (100 episodes) 15.520000\n",
      "best mean reward 15.970000\n",
      "episodes 1772\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3020000\n",
      "mean reward (100 episodes) 15.560000\n",
      "best mean reward 15.970000\n",
      "episodes 1777\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3030000\n",
      "mean reward (100 episodes) 15.670000\n",
      "best mean reward 15.970000\n",
      "episodes 1782\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3040000\n",
      "mean reward (100 episodes) 15.650000\n",
      "best mean reward 15.970000\n",
      "episodes 1786\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3050000\n",
      "mean reward (100 episodes) 15.760000\n",
      "best mean reward 15.970000\n",
      "episodes 1791\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3060000\n",
      "mean reward (100 episodes) 15.810000\n",
      "best mean reward 15.970000\n",
      "episodes 1796\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3070000\n",
      "mean reward (100 episodes) 15.790000\n",
      "best mean reward 15.970000\n",
      "episodes 1800\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3080000\n",
      "mean reward (100 episodes) 15.780000\n",
      "best mean reward 15.970000\n",
      "episodes 1804\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3090000\n",
      "mean reward (100 episodes) 15.790000\n",
      "best mean reward 15.970000\n",
      "episodes 1809\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3100000\n",
      "mean reward (100 episodes) 15.840000\n",
      "best mean reward 15.970000\n",
      "episodes 1814\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3110000\n",
      "mean reward (100 episodes) 15.910000\n",
      "best mean reward 15.970000\n",
      "episodes 1818\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3120000\n",
      "mean reward (100 episodes) 15.930000\n",
      "best mean reward 15.970000\n",
      "episodes 1822\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3130000\n",
      "mean reward (100 episodes) 15.880000\n",
      "best mean reward 15.970000\n",
      "episodes 1826\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3140000\n",
      "mean reward (100 episodes) 15.950000\n",
      "best mean reward 15.970000\n",
      "episodes 1831\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3150000\n",
      "mean reward (100 episodes) 16.210000\n",
      "best mean reward 16.210000\n",
      "episodes 1836\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3160000\n",
      "mean reward (100 episodes) 16.180000\n",
      "best mean reward 16.210000\n",
      "episodes 1840\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3170000\n",
      "mean reward (100 episodes) 16.230000\n",
      "best mean reward 16.230000\n",
      "episodes 1844\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3180000\n",
      "mean reward (100 episodes) 16.170000\n",
      "best mean reward 16.230000\n",
      "episodes 1849\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3190000\n",
      "mean reward (100 episodes) 16.140000\n",
      "best mean reward 16.230000\n",
      "episodes 1853\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3200000\n",
      "mean reward (100 episodes) 16.230000\n",
      "best mean reward 16.230000\n",
      "episodes 1857\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3210000\n",
      "mean reward (100 episodes) 16.240000\n",
      "best mean reward 16.240000\n",
      "episodes 1862\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3220000\n",
      "mean reward (100 episodes) 16.370000\n",
      "best mean reward 16.380000\n",
      "episodes 1867\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3230000\n",
      "mean reward (100 episodes) 16.300000\n",
      "best mean reward 16.380000\n",
      "episodes 1871\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3240000\n",
      "mean reward (100 episodes) 16.260000\n",
      "best mean reward 16.380000\n",
      "episodes 1875\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3250000\n",
      "mean reward (100 episodes) 16.220000\n",
      "best mean reward 16.380000\n",
      "episodes 1880\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3260000\n",
      "mean reward (100 episodes) 16.160000\n",
      "best mean reward 16.380000\n",
      "episodes 1884\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3270000\n",
      "mean reward (100 episodes) 16.030000\n",
      "best mean reward 16.380000\n",
      "episodes 1888\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 3280000\n",
      "mean reward (100 episodes) 16.000000\n",
      "best mean reward 16.380000\n",
      "episodes 1893\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3290000\n",
      "mean reward (100 episodes) 15.940000\n",
      "best mean reward 16.380000\n",
      "episodes 1897\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3300000\n",
      "mean reward (100 episodes) 15.950000\n",
      "best mean reward 16.380000\n",
      "episodes 1902\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3310000\n",
      "mean reward (100 episodes) 16.010000\n",
      "best mean reward 16.380000\n",
      "episodes 1906\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3320000\n",
      "mean reward (100 episodes) 16.070000\n",
      "best mean reward 16.380000\n",
      "episodes 1911\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3330000\n",
      "mean reward (100 episodes) 16.000000\n",
      "best mean reward 16.380000\n",
      "episodes 1916\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3340000\n",
      "mean reward (100 episodes) 15.940000\n",
      "best mean reward 16.380000\n",
      "episodes 1920\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3350000\n",
      "mean reward (100 episodes) 16.110000\n",
      "best mean reward 16.380000\n",
      "episodes 1925\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3360000\n",
      "mean reward (100 episodes) 16.150000\n",
      "best mean reward 16.380000\n",
      "episodes 1930\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3370000\n",
      "mean reward (100 episodes) 16.120000\n",
      "best mean reward 16.380000\n",
      "episodes 1934\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3380000\n",
      "mean reward (100 episodes) 16.150000\n",
      "best mean reward 16.380000\n",
      "episodes 1938\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3390000\n",
      "mean reward (100 episodes) 16.250000\n",
      "best mean reward 16.380000\n",
      "episodes 1943\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3400000\n",
      "mean reward (100 episodes) 16.320000\n",
      "best mean reward 16.380000\n",
      "episodes 1948\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3410000\n",
      "mean reward (100 episodes) 16.390000\n",
      "best mean reward 16.390000\n",
      "episodes 1953\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3420000\n",
      "mean reward (100 episodes) 16.440000\n",
      "best mean reward 16.470000\n",
      "episodes 1958\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3430000\n",
      "mean reward (100 episodes) 16.460000\n",
      "best mean reward 16.470000\n",
      "episodes 1962\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3440000\n",
      "mean reward (100 episodes) 16.460000\n",
      "best mean reward 16.470000\n",
      "episodes 1967\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3450000\n",
      "mean reward (100 episodes) 16.430000\n",
      "best mean reward 16.470000\n",
      "episodes 1971\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3460000\n",
      "mean reward (100 episodes) 16.470000\n",
      "best mean reward 16.500000\n",
      "episodes 1976\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3470000\n",
      "mean reward (100 episodes) 16.600000\n",
      "best mean reward 16.600000\n",
      "episodes 1981\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3480000\n",
      "mean reward (100 episodes) 16.750000\n",
      "best mean reward 16.750000\n",
      "episodes 1986\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3490000\n",
      "mean reward (100 episodes) 16.800000\n",
      "best mean reward 16.830000\n",
      "episodes 1991\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3500000\n",
      "mean reward (100 episodes) 16.920000\n",
      "best mean reward 16.920000\n",
      "episodes 1995\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 05:40:50,647] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video002000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 3510000\n",
      "mean reward (100 episodes) 17.020000\n",
      "best mean reward 17.020000\n",
      "episodes 2000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3520000\n",
      "mean reward (100 episodes) 17.090000\n",
      "best mean reward 17.090000\n",
      "episodes 2005\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3530000\n",
      "mean reward (100 episodes) 17.110000\n",
      "best mean reward 17.150000\n",
      "episodes 2010\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3540000\n",
      "mean reward (100 episodes) 17.140000\n",
      "best mean reward 17.170000\n",
      "episodes 2015\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3550000\n",
      "mean reward (100 episodes) 17.340000\n",
      "best mean reward 17.340000\n",
      "episodes 2020\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3560000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 17.410000\n",
      "episodes 2025\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3570000\n",
      "mean reward (100 episodes) 17.350000\n",
      "best mean reward 17.410000\n",
      "episodes 2030\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3580000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 17.440000\n",
      "episodes 2035\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3590000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 17.530000\n",
      "episodes 2040\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3600000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 17.530000\n",
      "episodes 2045\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3610000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 17.530000\n",
      "episodes 2049\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3620000\n",
      "mean reward (100 episodes) 17.350000\n",
      "best mean reward 17.530000\n",
      "episodes 2054\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3630000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 17.530000\n",
      "episodes 2059\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3640000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 17.530000\n",
      "episodes 2064\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3650000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 17.530000\n",
      "episodes 2068\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3660000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 17.560000\n",
      "episodes 2073\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3670000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 17.610000\n",
      "episodes 2078\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3680000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 17.610000\n",
      "episodes 2082\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3690000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 17.610000\n",
      "episodes 2087\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3700000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 17.610000\n",
      "episodes 2091\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3710000\n",
      "mean reward (100 episodes) 17.380000\n",
      "best mean reward 17.610000\n",
      "episodes 2096\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3720000\n",
      "mean reward (100 episodes) 17.340000\n",
      "best mean reward 17.610000\n",
      "episodes 2101\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3730000\n",
      "mean reward (100 episodes) 17.240000\n",
      "best mean reward 17.610000\n",
      "episodes 2106\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3740000\n",
      "mean reward (100 episodes) 17.210000\n",
      "best mean reward 17.610000\n",
      "episodes 2111\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3750000\n",
      "mean reward (100 episodes) 17.300000\n",
      "best mean reward 17.610000\n",
      "episodes 2116\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3760000\n",
      "mean reward (100 episodes) 17.250000\n",
      "best mean reward 17.610000\n",
      "episodes 2121\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3770000\n",
      "mean reward (100 episodes) 17.240000\n",
      "best mean reward 17.610000\n",
      "episodes 2126\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3780000\n",
      "mean reward (100 episodes) 17.220000\n",
      "best mean reward 17.610000\n",
      "episodes 2130\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3790000\n",
      "mean reward (100 episodes) 17.280000\n",
      "best mean reward 17.610000\n",
      "episodes 2135\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3800000\n",
      "mean reward (100 episodes) 17.240000\n",
      "best mean reward 17.610000\n",
      "episodes 2140\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3810000\n",
      "mean reward (100 episodes) 17.200000\n",
      "best mean reward 17.610000\n",
      "episodes 2145\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3820000\n",
      "mean reward (100 episodes) 17.220000\n",
      "best mean reward 17.610000\n",
      "episodes 2149\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3830000\n",
      "mean reward (100 episodes) 17.350000\n",
      "best mean reward 17.610000\n",
      "episodes 2155\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3840000\n",
      "mean reward (100 episodes) 17.290000\n",
      "best mean reward 17.610000\n",
      "episodes 2159\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3850000\n",
      "mean reward (100 episodes) 17.290000\n",
      "best mean reward 17.610000\n",
      "episodes 2164\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3860000\n",
      "mean reward (100 episodes) 17.200000\n",
      "best mean reward 17.610000\n",
      "episodes 2168\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3870000\n",
      "mean reward (100 episodes) 17.140000\n",
      "best mean reward 17.610000\n",
      "episodes 2173\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3880000\n",
      "mean reward (100 episodes) 17.010000\n",
      "best mean reward 17.610000\n",
      "episodes 2177\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3890000\n",
      "mean reward (100 episodes) 17.110000\n",
      "best mean reward 17.610000\n",
      "episodes 2182\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3900000\n",
      "mean reward (100 episodes) 17.160000\n",
      "best mean reward 17.610000\n",
      "episodes 2187\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3910000\n",
      "mean reward (100 episodes) 17.110000\n",
      "best mean reward 17.610000\n",
      "episodes 2191\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3920000\n",
      "mean reward (100 episodes) 17.170000\n",
      "best mean reward 17.610000\n",
      "episodes 2197\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3930000\n",
      "mean reward (100 episodes) 17.160000\n",
      "best mean reward 17.610000\n",
      "episodes 2201\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3940000\n",
      "mean reward (100 episodes) 17.140000\n",
      "best mean reward 17.610000\n",
      "episodes 2206\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3950000\n",
      "mean reward (100 episodes) 17.110000\n",
      "best mean reward 17.610000\n",
      "episodes 2211\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3960000\n",
      "mean reward (100 episodes) 17.050000\n",
      "best mean reward 17.610000\n",
      "episodes 2216\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3970000\n",
      "mean reward (100 episodes) 17.080000\n",
      "best mean reward 17.610000\n",
      "episodes 2221\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3980000\n",
      "mean reward (100 episodes) 17.120000\n",
      "best mean reward 17.610000\n",
      "episodes 2226\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 3990000\n",
      "mean reward (100 episodes) 17.100000\n",
      "best mean reward 17.610000\n",
      "episodes 2230\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4000000\n",
      "mean reward (100 episodes) 17.040000\n",
      "best mean reward 17.610000\n",
      "episodes 2235\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4010000\n",
      "mean reward (100 episodes) 17.100000\n",
      "best mean reward 17.610000\n",
      "episodes 2239\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4020000\n",
      "mean reward (100 episodes) 17.130000\n",
      "best mean reward 17.610000\n",
      "episodes 2244\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4030000\n",
      "mean reward (100 episodes) 17.090000\n",
      "best mean reward 17.610000\n",
      "episodes 2249\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4040000\n",
      "mean reward (100 episodes) 17.060000\n",
      "best mean reward 17.610000\n",
      "episodes 2254\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4050000\n",
      "mean reward (100 episodes) 17.030000\n",
      "best mean reward 17.610000\n",
      "episodes 2258\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4060000\n",
      "mean reward (100 episodes) 16.950000\n",
      "best mean reward 17.610000\n",
      "episodes 2263\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4070000\n",
      "mean reward (100 episodes) 17.100000\n",
      "best mean reward 17.610000\n",
      "episodes 2268\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4080000\n",
      "mean reward (100 episodes) 17.250000\n",
      "best mean reward 17.610000\n",
      "episodes 2273\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4090000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 17.610000\n",
      "episodes 2278\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 4100000\n",
      "mean reward (100 episodes) 17.230000\n",
      "best mean reward 17.610000\n",
      "episodes 2283\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4110000\n",
      "mean reward (100 episodes) 17.170000\n",
      "best mean reward 17.610000\n",
      "episodes 2287\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4120000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 17.610000\n",
      "episodes 2292\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4130000\n",
      "mean reward (100 episodes) 17.110000\n",
      "best mean reward 17.610000\n",
      "episodes 2297\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4140000\n",
      "mean reward (100 episodes) 17.160000\n",
      "best mean reward 17.610000\n",
      "episodes 2302\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4150000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 17.610000\n",
      "episodes 2307\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4160000\n",
      "mean reward (100 episodes) 17.360000\n",
      "best mean reward 17.610000\n",
      "episodes 2312\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4170000\n",
      "mean reward (100 episodes) 17.300000\n",
      "best mean reward 17.610000\n",
      "episodes 2316\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4180000\n",
      "mean reward (100 episodes) 17.280000\n",
      "best mean reward 17.610000\n",
      "episodes 2322\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4190000\n",
      "mean reward (100 episodes) 17.310000\n",
      "best mean reward 17.610000\n",
      "episodes 2327\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4200000\n",
      "mean reward (100 episodes) 17.280000\n",
      "best mean reward 17.610000\n",
      "episodes 2331\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4210000\n",
      "mean reward (100 episodes) 17.240000\n",
      "best mean reward 17.610000\n",
      "episodes 2336\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4220000\n",
      "mean reward (100 episodes) 17.280000\n",
      "best mean reward 17.610000\n",
      "episodes 2341\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4230000\n",
      "mean reward (100 episodes) 17.330000\n",
      "best mean reward 17.610000\n",
      "episodes 2346\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4240000\n",
      "mean reward (100 episodes) 17.300000\n",
      "best mean reward 17.610000\n",
      "episodes 2350\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4250000\n",
      "mean reward (100 episodes) 17.310000\n",
      "best mean reward 17.610000\n",
      "episodes 2355\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4260000\n",
      "mean reward (100 episodes) 17.400000\n",
      "best mean reward 17.610000\n",
      "episodes 2360\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4270000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 17.610000\n",
      "episodes 2365\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4280000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 17.610000\n",
      "episodes 2370\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4290000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 17.610000\n",
      "episodes 2375\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4300000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 17.630000\n",
      "episodes 2380\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4310000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 17.690000\n",
      "episodes 2385\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4320000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 17.710000\n",
      "episodes 2390\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4330000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 17.730000\n",
      "episodes 2395\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4340000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 17.730000\n",
      "episodes 2399\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4350000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 17.730000\n",
      "episodes 2404\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4360000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 17.730000\n",
      "episodes 2409\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4370000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 17.730000\n",
      "episodes 2414\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4380000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 17.730000\n",
      "episodes 2419\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4390000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 17.730000\n",
      "episodes 2424\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4400000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 17.730000\n",
      "episodes 2429\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4410000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 17.730000\n",
      "episodes 2434\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4420000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 17.740000\n",
      "episodes 2438\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4430000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 17.740000\n",
      "episodes 2443\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4440000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 17.740000\n",
      "episodes 2448\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4450000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 17.740000\n",
      "episodes 2452\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4460000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 17.740000\n",
      "episodes 2457\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4470000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 17.740000\n",
      "episodes 2462\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4480000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 17.740000\n",
      "episodes 2466\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4490000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 17.740000\n",
      "episodes 2471\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4500000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 17.740000\n",
      "episodes 2476\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4510000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 17.740000\n",
      "episodes 2481\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4520000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 17.740000\n",
      "episodes 2486\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4530000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 17.740000\n",
      "episodes 2491\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4540000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 17.740000\n",
      "episodes 2496\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4550000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 17.740000\n",
      "episodes 2501\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4560000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 17.740000\n",
      "episodes 2506\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4570000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 17.740000\n",
      "episodes 2511\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4580000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 17.740000\n",
      "episodes 2516\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4590000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 17.740000\n",
      "episodes 2520\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4600000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 17.740000\n",
      "episodes 2525\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4610000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 17.740000\n",
      "episodes 2531\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4620000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 17.740000\n",
      "episodes 2535\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4630000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 17.740000\n",
      "episodes 2540\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4640000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 17.740000\n",
      "episodes 2545\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4650000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 17.740000\n",
      "episodes 2549\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4660000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 17.740000\n",
      "episodes 2554\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4670000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 17.740000\n",
      "episodes 2558\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4680000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 17.740000\n",
      "episodes 2563\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 4690000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 17.740000\n",
      "episodes 2568\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4700000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 17.740000\n",
      "episodes 2573\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4710000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 17.740000\n",
      "episodes 2578\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4720000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 17.740000\n",
      "episodes 2583\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4730000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 17.740000\n",
      "episodes 2587\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4740000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 17.740000\n",
      "episodes 2593\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4750000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 17.740000\n",
      "episodes 2597\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4760000\n",
      "mean reward (100 episodes) 17.360000\n",
      "best mean reward 17.740000\n",
      "episodes 2602\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4770000\n",
      "mean reward (100 episodes) 17.270000\n",
      "best mean reward 17.740000\n",
      "episodes 2607\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4780000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 17.740000\n",
      "episodes 2612\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4790000\n",
      "mean reward (100 episodes) 17.240000\n",
      "best mean reward 17.740000\n",
      "episodes 2617\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4800000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 17.740000\n",
      "episodes 2622\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4810000\n",
      "mean reward (100 episodes) 17.410000\n",
      "best mean reward 17.740000\n",
      "episodes 2627\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4820000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 17.740000\n",
      "episodes 2632\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4830000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 17.740000\n",
      "episodes 2637\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4840000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 17.740000\n",
      "episodes 2642\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4850000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 17.740000\n",
      "episodes 2647\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4860000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 17.740000\n",
      "episodes 2652\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4870000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 17.790000\n",
      "episodes 2657\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4880000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 17.790000\n",
      "episodes 2662\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4890000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 17.810000\n",
      "episodes 2667\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4900000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 17.810000\n",
      "episodes 2672\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4910000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 17.870000\n",
      "episodes 2678\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4920000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 17.890000\n",
      "episodes 2683\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4930000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 17.910000\n",
      "episodes 2688\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4940000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 17.930000\n",
      "episodes 2693\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4950000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 17.940000\n",
      "episodes 2697\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4960000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 17.940000\n",
      "episodes 2702\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4970000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 17.970000\n",
      "episodes 2707\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4980000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 17.990000\n",
      "episodes 2712\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 4990000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.060000\n",
      "episodes 2717\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5000000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.060000\n",
      "episodes 2721\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5010000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.060000\n",
      "episodes 2726\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5020000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.060000\n",
      "episodes 2731\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5030000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.060000\n",
      "episodes 2736\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5040000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.060000\n",
      "episodes 2741\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5050000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 18.060000\n",
      "episodes 2746\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5060000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.060000\n",
      "episodes 2751\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5070000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.060000\n",
      "episodes 2756\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5080000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.060000\n",
      "episodes 2761\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5090000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.060000\n",
      "episodes 2765\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5100000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.060000\n",
      "episodes 2771\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5110000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.060000\n",
      "episodes 2776\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5120000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.060000\n",
      "episodes 2781\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5130000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.060000\n",
      "episodes 2786\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5140000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.060000\n",
      "episodes 2790\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5150000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.060000\n",
      "episodes 2795\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5160000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.060000\n",
      "episodes 2800\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5170000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.060000\n",
      "episodes 2805\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5180000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 18.060000\n",
      "episodes 2810\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5190000\n",
      "mean reward (100 episodes) 17.320000\n",
      "best mean reward 18.060000\n",
      "episodes 2814\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5200000\n",
      "mean reward (100 episodes) 17.360000\n",
      "best mean reward 18.060000\n",
      "episodes 2819\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5210000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 18.060000\n",
      "episodes 2824\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5220000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.060000\n",
      "episodes 2830\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5230000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.060000\n",
      "episodes 2835\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5240000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.060000\n",
      "episodes 2840\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5250000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.060000\n",
      "episodes 2845\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5260000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.060000\n",
      "episodes 2850\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5270000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.060000\n",
      "episodes 2855\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 5280000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.060000\n",
      "episodes 2860\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5290000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.060000\n",
      "episodes 2865\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5300000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.060000\n",
      "episodes 2870\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5310000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.060000\n",
      "episodes 2874\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5320000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.060000\n",
      "episodes 2879\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5330000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 18.060000\n",
      "episodes 2884\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5340000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.060000\n",
      "episodes 2890\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5350000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.060000\n",
      "episodes 2895\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5360000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.060000\n",
      "episodes 2900\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5370000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.060000\n",
      "episodes 2906\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5380000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.120000\n",
      "episodes 2911\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5390000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.150000\n",
      "episodes 2915\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5400000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.220000\n",
      "episodes 2921\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5410000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.280000\n",
      "episodes 2926\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5420000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.410000\n",
      "episodes 2932\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5430000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.410000\n",
      "episodes 2936\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5440000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.410000\n",
      "episodes 2941\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5450000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.410000\n",
      "episodes 2946\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5460000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.410000\n",
      "episodes 2952\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5470000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.410000\n",
      "episodes 2957\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5480000\n",
      "mean reward (100 episodes) 18.450000\n",
      "best mean reward 18.450000\n",
      "episodes 2962\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5490000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.450000\n",
      "episodes 2967\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5500000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.450000\n",
      "episodes 2971\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5510000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.450000\n",
      "episodes 2977\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5520000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.450000\n",
      "episodes 2982\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5530000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.450000\n",
      "episodes 2987\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5540000\n",
      "mean reward (100 episodes) 18.450000\n",
      "best mean reward 18.450000\n",
      "episodes 2993\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5550000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.450000\n",
      "episodes 2997\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 07:36:16,374] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video003000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 5560000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.450000\n",
      "episodes 3002\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5570000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.450000\n",
      "episodes 3007\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5580000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.450000\n",
      "episodes 3012\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5590000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.450000\n",
      "episodes 3018\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5600000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.450000\n",
      "episodes 3023\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5610000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.450000\n",
      "episodes 3028\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5620000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.450000\n",
      "episodes 3033\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5630000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.450000\n",
      "episodes 3038\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5640000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.450000\n",
      "episodes 3043\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5650000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.450000\n",
      "episodes 3048\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5660000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.450000\n",
      "episodes 3053\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5670000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.450000\n",
      "episodes 3058\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5680000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.450000\n",
      "episodes 3063\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5690000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.450000\n",
      "episodes 3068\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5700000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.450000\n",
      "episodes 3073\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5710000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.450000\n",
      "episodes 3078\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5720000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.450000\n",
      "episodes 3083\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5730000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.450000\n",
      "episodes 3089\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5740000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.450000\n",
      "episodes 3093\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5750000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.450000\n",
      "episodes 3098\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5760000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.450000\n",
      "episodes 3104\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5770000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.450000\n",
      "episodes 3109\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5780000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.450000\n",
      "episodes 3114\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5790000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.450000\n",
      "episodes 3119\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5800000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.450000\n",
      "episodes 3124\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5810000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.450000\n",
      "episodes 3128\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5820000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.450000\n",
      "episodes 3134\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5830000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.450000\n",
      "episodes 3139\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5840000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.450000\n",
      "episodes 3144\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5850000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.450000\n",
      "episodes 3149\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5860000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.450000\n",
      "episodes 3153\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5870000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.450000\n",
      "episodes 3158\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5880000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.450000\n",
      "episodes 3163\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5890000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.450000\n",
      "episodes 3168\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5900000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.450000\n",
      "episodes 3173\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5910000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.450000\n",
      "episodes 3178\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5920000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.450000\n",
      "episodes 3183\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5930000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.450000\n",
      "episodes 3187\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5940000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.450000\n",
      "episodes 3192\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5950000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.450000\n",
      "episodes 3197\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5960000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.450000\n",
      "episodes 3202\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5970000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.450000\n",
      "episodes 3207\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5980000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.450000\n",
      "episodes 3212\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 5990000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.450000\n",
      "episodes 3216\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6000000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.450000\n",
      "episodes 3221\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6010000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.450000\n",
      "episodes 3226\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6020000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.450000\n",
      "episodes 3231\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6030000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.450000\n",
      "episodes 3236\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6040000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.450000\n",
      "episodes 3241\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6050000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 18.450000\n",
      "episodes 3246\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6060000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.450000\n",
      "episodes 3252\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6070000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.450000\n",
      "episodes 3257\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6080000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.450000\n",
      "episodes 3262\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6090000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.450000\n",
      "episodes 3267\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6100000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.450000\n",
      "episodes 3272\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6110000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.450000\n",
      "episodes 3277\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6120000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.450000\n",
      "episodes 3282\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6130000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.450000\n",
      "episodes 3287\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6140000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.450000\n",
      "episodes 3292\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 6150000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.450000\n",
      "episodes 3296\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6160000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.450000\n",
      "episodes 3301\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6170000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.450000\n",
      "episodes 3307\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6180000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.450000\n",
      "episodes 3312\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6190000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.450000\n",
      "episodes 3317\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6200000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.450000\n",
      "episodes 3322\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6210000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.450000\n",
      "episodes 3327\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6220000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.450000\n",
      "episodes 3331\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6230000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.450000\n",
      "episodes 3337\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6240000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.450000\n",
      "episodes 3342\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6250000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 18.450000\n",
      "episodes 3347\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6260000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.450000\n",
      "episodes 3352\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6270000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.450000\n",
      "episodes 3357\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6280000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.450000\n",
      "episodes 3362\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6290000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.450000\n",
      "episodes 3367\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6300000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 18.450000\n",
      "episodes 3372\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6310000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.450000\n",
      "episodes 3376\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6320000\n",
      "mean reward (100 episodes) 17.410000\n",
      "best mean reward 18.450000\n",
      "episodes 3382\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6330000\n",
      "mean reward (100 episodes) 17.380000\n",
      "best mean reward 18.450000\n",
      "episodes 3387\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6340000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.450000\n",
      "episodes 3392\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6350000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.450000\n",
      "episodes 3397\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6360000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.450000\n",
      "episodes 3402\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6370000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.450000\n",
      "episodes 3407\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6380000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.450000\n",
      "episodes 3412\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6390000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.450000\n",
      "episodes 3418\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6400000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.450000\n",
      "episodes 3423\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6410000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.450000\n",
      "episodes 3428\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6420000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.450000\n",
      "episodes 3433\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6430000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.450000\n",
      "episodes 3438\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6440000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.450000\n",
      "episodes 3443\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6450000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.450000\n",
      "episodes 3448\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6460000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.450000\n",
      "episodes 3453\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6470000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.450000\n",
      "episodes 3459\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6480000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.450000\n",
      "episodes 3464\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6490000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.450000\n",
      "episodes 3469\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6500000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.450000\n",
      "episodes 3474\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6510000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.450000\n",
      "episodes 3479\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6520000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.450000\n",
      "episodes 3484\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6530000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.450000\n",
      "episodes 3489\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6540000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.450000\n",
      "episodes 3494\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6550000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.450000\n",
      "episodes 3499\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6560000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.450000\n",
      "episodes 3504\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6570000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.450000\n",
      "episodes 3509\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6580000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.450000\n",
      "episodes 3514\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6590000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.450000\n",
      "episodes 3519\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6600000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.450000\n",
      "episodes 3524\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6610000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.450000\n",
      "episodes 3529\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6620000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.450000\n",
      "episodes 3534\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6630000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.450000\n",
      "episodes 3539\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6640000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.450000\n",
      "episodes 3544\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6650000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.450000\n",
      "episodes 3549\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6660000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.450000\n",
      "episodes 3554\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6670000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.450000\n",
      "episodes 3559\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6680000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.450000\n",
      "episodes 3564\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6690000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.450000\n",
      "episodes 3568\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6700000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.450000\n",
      "episodes 3574\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6710000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.450000\n",
      "episodes 3579\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6720000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.450000\n",
      "episodes 3584\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6730000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.450000\n",
      "episodes 3589\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 6740000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.450000\n",
      "episodes 3595\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6750000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.450000\n",
      "episodes 3600\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6760000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.450000\n",
      "episodes 3605\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6770000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.450000\n",
      "episodes 3610\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6780000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.450000\n",
      "episodes 3615\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6790000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.450000\n",
      "episodes 3620\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6800000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.450000\n",
      "episodes 3625\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6810000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.450000\n",
      "episodes 3630\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6820000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.450000\n",
      "episodes 3635\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6830000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.450000\n",
      "episodes 3640\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6840000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.450000\n",
      "episodes 3646\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6850000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.450000\n",
      "episodes 3650\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6860000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.450000\n",
      "episodes 3656\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6870000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.450000\n",
      "episodes 3661\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6880000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.450000\n",
      "episodes 3666\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6890000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.450000\n",
      "episodes 3671\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6900000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.450000\n",
      "episodes 3677\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6910000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.450000\n",
      "episodes 3682\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6920000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.450000\n",
      "episodes 3687\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6930000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.450000\n",
      "episodes 3692\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6940000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.450000\n",
      "episodes 3698\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6950000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.450000\n",
      "episodes 3703\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6960000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.450000\n",
      "episodes 3707\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6970000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.450000\n",
      "episodes 3713\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6980000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.450000\n",
      "episodes 3718\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 6990000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.450000\n",
      "episodes 3723\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7000000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.450000\n",
      "episodes 3728\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7010000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.450000\n",
      "episodes 3733\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7020000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.450000\n",
      "episodes 3738\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7030000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.450000\n",
      "episodes 3743\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7040000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.450000\n",
      "episodes 3748\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7050000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.450000\n",
      "episodes 3753\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7060000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.450000\n",
      "episodes 3758\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7070000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.450000\n",
      "episodes 3763\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7080000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.450000\n",
      "episodes 3768\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7090000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.450000\n",
      "episodes 3774\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7100000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.450000\n",
      "episodes 3779\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7110000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.450000\n",
      "episodes 3784\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7120000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.450000\n",
      "episodes 3789\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7130000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.450000\n",
      "episodes 3794\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7140000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.450000\n",
      "episodes 3799\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7150000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.450000\n",
      "episodes 3804\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7160000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.450000\n",
      "episodes 3809\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7170000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.450000\n",
      "episodes 3814\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7180000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.450000\n",
      "episodes 3819\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7190000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.450000\n",
      "episodes 3825\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7200000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.450000\n",
      "episodes 3829\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7210000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.450000\n",
      "episodes 3835\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7220000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.450000\n",
      "episodes 3840\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7230000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.450000\n",
      "episodes 3844\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7240000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.450000\n",
      "episodes 3849\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7250000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.450000\n",
      "episodes 3854\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7260000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.450000\n",
      "episodes 3859\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7270000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.450000\n",
      "episodes 3864\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7280000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.450000\n",
      "episodes 3869\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7290000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.450000\n",
      "episodes 3874\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7300000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.450000\n",
      "episodes 3879\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7310000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.450000\n",
      "episodes 3884\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7320000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.450000\n",
      "episodes 3889\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 7330000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.450000\n",
      "episodes 3894\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7340000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.450000\n",
      "episodes 3899\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7350000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.450000\n",
      "episodes 3904\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7360000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.450000\n",
      "episodes 3909\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7370000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.450000\n",
      "episodes 3914\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7380000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.450000\n",
      "episodes 3919\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7390000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.450000\n",
      "episodes 3925\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7400000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.450000\n",
      "episodes 3930\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7410000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.450000\n",
      "episodes 3935\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7420000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.450000\n",
      "episodes 3940\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7430000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.450000\n",
      "episodes 3945\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7440000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.450000\n",
      "episodes 3950\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7450000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.450000\n",
      "episodes 3955\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7460000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.450000\n",
      "episodes 3961\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7470000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.450000\n",
      "episodes 3966\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7480000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.450000\n",
      "episodes 3972\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7490000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.450000\n",
      "episodes 3977\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7500000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.450000\n",
      "episodes 3982\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7510000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.450000\n",
      "episodes 3987\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7520000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.450000\n",
      "episodes 3992\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7530000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.450000\n",
      "episodes 3997\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 09:28:01,179] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video004000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 7540000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.450000\n",
      "episodes 4002\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7550000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.450000\n",
      "episodes 4007\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7560000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.450000\n",
      "episodes 4012\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7570000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.450000\n",
      "episodes 4017\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7580000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.450000\n",
      "episodes 4022\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7590000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.450000\n",
      "episodes 4027\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7600000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.450000\n",
      "episodes 4032\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7610000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.450000\n",
      "episodes 4037\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7620000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.450000\n",
      "episodes 4042\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7630000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.450000\n",
      "episodes 4047\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7640000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.450000\n",
      "episodes 4052\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7650000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.450000\n",
      "episodes 4057\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7660000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.450000\n",
      "episodes 4062\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7670000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.450000\n",
      "episodes 4067\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7680000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.450000\n",
      "episodes 4072\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7690000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.450000\n",
      "episodes 4077\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7700000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.450000\n",
      "episodes 4082\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7710000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.450000\n",
      "episodes 4087\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7720000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.450000\n",
      "episodes 4092\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7730000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.450000\n",
      "episodes 4097\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7740000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.450000\n",
      "episodes 4102\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7750000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.450000\n",
      "episodes 4107\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7760000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.450000\n",
      "episodes 4112\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7770000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.450000\n",
      "episodes 4117\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7780000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.450000\n",
      "episodes 4122\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7790000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.450000\n",
      "episodes 4127\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7800000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.450000\n",
      "episodes 4132\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7810000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.450000\n",
      "episodes 4137\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7820000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.450000\n",
      "episodes 4142\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7830000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.450000\n",
      "episodes 4147\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7840000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.450000\n",
      "episodes 4152\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7850000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.450000\n",
      "episodes 4157\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7860000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.450000\n",
      "episodes 4163\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7870000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.450000\n",
      "episodes 4167\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7880000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.450000\n",
      "episodes 4172\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7890000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.450000\n",
      "episodes 4177\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7900000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.450000\n",
      "episodes 4182\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7910000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.450000\n",
      "episodes 4187\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7920000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.450000\n",
      "episodes 4192\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7930000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.450000\n",
      "episodes 4197\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7940000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.450000\n",
      "episodes 4202\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7950000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.450000\n",
      "episodes 4207\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7960000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.450000\n",
      "episodes 4212\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7970000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.450000\n",
      "episodes 4217\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7980000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.450000\n",
      "episodes 4223\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 7990000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.450000\n",
      "episodes 4228\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8000000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.450000\n",
      "episodes 4233\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8010000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.450000\n",
      "episodes 4238\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8020000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.450000\n",
      "episodes 4243\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8030000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.450000\n",
      "episodes 4248\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8040000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.450000\n",
      "episodes 4253\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8050000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.450000\n",
      "episodes 4258\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8060000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.450000\n",
      "episodes 4263\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8070000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.450000\n",
      "episodes 4268\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8080000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.450000\n",
      "episodes 4273\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8090000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.450000\n",
      "episodes 4278\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8100000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.450000\n",
      "episodes 4283\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8110000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.450000\n",
      "episodes 4287\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8120000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.450000\n",
      "episodes 4293\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 8130000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.450000\n",
      "episodes 4298\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8140000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.450000\n",
      "episodes 4302\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8150000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.450000\n",
      "episodes 4308\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8160000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.450000\n",
      "episodes 4313\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8170000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.450000\n",
      "episodes 4318\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8180000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.450000\n",
      "episodes 4323\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8190000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.450000\n",
      "episodes 4328\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8200000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.450000\n",
      "episodes 4334\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8210000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.450000\n",
      "episodes 4338\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8220000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.450000\n",
      "episodes 4344\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8230000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.450000\n",
      "episodes 4349\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8240000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.450000\n",
      "episodes 4354\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8250000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.450000\n",
      "episodes 4359\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8260000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.450000\n",
      "episodes 4364\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8270000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.450000\n",
      "episodes 4369\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8280000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.450000\n",
      "episodes 4374\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8290000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.450000\n",
      "episodes 4379\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8300000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.450000\n",
      "episodes 4384\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8310000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.450000\n",
      "episodes 4390\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8320000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.450000\n",
      "episodes 4395\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8330000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.450000\n",
      "episodes 4400\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8340000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.450000\n",
      "episodes 4405\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8350000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.450000\n",
      "episodes 4410\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8360000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.450000\n",
      "episodes 4416\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8370000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.450000\n",
      "episodes 4421\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8380000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.450000\n",
      "episodes 4426\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8390000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.450000\n",
      "episodes 4431\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8400000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.450000\n",
      "episodes 4436\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8410000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.450000\n",
      "episodes 4441\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8420000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.450000\n",
      "episodes 4446\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8430000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.450000\n",
      "episodes 4450\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8440000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.450000\n",
      "episodes 4456\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8450000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.450000\n",
      "episodes 4461\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8460000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.450000\n",
      "episodes 4467\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8470000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.450000\n",
      "episodes 4472\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8480000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.450000\n",
      "episodes 4477\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8490000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.450000\n",
      "episodes 4482\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8500000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.450000\n",
      "episodes 4487\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8510000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.450000\n",
      "episodes 4492\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8520000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.450000\n",
      "episodes 4497\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8530000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.450000\n",
      "episodes 4502\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8540000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.450000\n",
      "episodes 4507\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8550000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.450000\n",
      "episodes 4512\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8560000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.450000\n",
      "episodes 4517\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8570000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.450000\n",
      "episodes 4522\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8580000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.450000\n",
      "episodes 4527\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8590000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.450000\n",
      "episodes 4532\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8600000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.450000\n",
      "episodes 4537\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8610000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.450000\n",
      "episodes 4543\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 8620000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.450000\n",
      "episodes 4547\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9060000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.540000\n",
      "episodes 4773\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9070000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.540000\n",
      "episodes 4778\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9080000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.540000\n",
      "episodes 4783\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9090000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.540000\n",
      "episodes 4788\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9100000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.540000\n",
      "episodes 4793\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9110000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.540000\n",
      "episodes 4798\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9120000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.540000\n",
      "episodes 4803\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9130000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.540000\n",
      "episodes 4809\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9140000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.540000\n",
      "episodes 4814\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 9150000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.540000\n",
      "episodes 4819\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9160000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.540000\n",
      "episodes 4824\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9170000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.540000\n",
      "episodes 4829\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9180000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.540000\n",
      "episodes 4834\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9190000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.540000\n",
      "episodes 4840\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9200000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.540000\n",
      "episodes 4845\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9210000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.540000\n",
      "episodes 4850\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9220000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.540000\n",
      "episodes 4855\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9230000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.540000\n",
      "episodes 4860\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9240000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.540000\n",
      "episodes 4865\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9250000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.540000\n",
      "episodes 4870\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9260000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.540000\n",
      "episodes 4875\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9270000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.540000\n",
      "episodes 4880\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9280000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.540000\n",
      "episodes 4886\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9290000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.540000\n",
      "episodes 4891\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9300000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.540000\n",
      "episodes 4896\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9310000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.540000\n",
      "episodes 4901\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9320000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.540000\n",
      "episodes 4906\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9330000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.540000\n",
      "episodes 4912\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9340000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.540000\n",
      "episodes 4917\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9350000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.540000\n",
      "episodes 4922\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9360000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.540000\n",
      "episodes 4927\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9370000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.540000\n",
      "episodes 4932\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9380000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.540000\n",
      "episodes 4938\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9390000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.540000\n",
      "episodes 4943\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9400000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.540000\n",
      "episodes 4948\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9410000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.540000\n",
      "episodes 4953\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9420000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.540000\n",
      "episodes 4959\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9430000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.540000\n",
      "episodes 4964\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9440000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.540000\n",
      "episodes 4969\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9450000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.540000\n",
      "episodes 4974\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9460000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.540000\n",
      "episodes 4979\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9470000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.540000\n",
      "episodes 4984\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9480000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.540000\n",
      "episodes 4989\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9490000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.540000\n",
      "episodes 4994\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 11:18:58,334] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video005000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 9500000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.540000\n",
      "episodes 5000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9510000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.540000\n",
      "episodes 5005\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9520000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.540000\n",
      "episodes 5010\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9530000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.540000\n",
      "episodes 5015\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9540000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.540000\n",
      "episodes 5020\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9550000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.540000\n",
      "episodes 5025\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9560000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.540000\n",
      "episodes 5031\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9570000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.540000\n",
      "episodes 5036\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9580000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.540000\n",
      "episodes 5041\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9590000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.540000\n",
      "episodes 5046\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9600000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.540000\n",
      "episodes 5052\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9610000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.540000\n",
      "episodes 5057\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9620000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.540000\n",
      "episodes 5062\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9630000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.540000\n",
      "episodes 5067\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9640000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.540000\n",
      "episodes 5072\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9650000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.540000\n",
      "episodes 5078\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9660000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.540000\n",
      "episodes 5083\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9670000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.540000\n",
      "episodes 5088\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9680000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.540000\n",
      "episodes 5094\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9690000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.540000\n",
      "episodes 5099\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9700000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.540000\n",
      "episodes 5104\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9710000\n",
      "mean reward (100 episodes) 18.460000\n",
      "best mean reward 18.540000\n",
      "episodes 5109\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9720000\n",
      "mean reward (100 episodes) 18.550000\n",
      "best mean reward 18.550000\n",
      "episodes 5115\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9730000\n",
      "mean reward (100 episodes) 18.510000\n",
      "best mean reward 18.570000\n",
      "episodes 5120\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9740000\n",
      "mean reward (100 episodes) 18.590000\n",
      "best mean reward 18.600000\n",
      "episodes 5125\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9750000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.600000\n",
      "episodes 5130\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9760000\n",
      "mean reward (100 episodes) 18.530000\n",
      "best mean reward 18.600000\n",
      "episodes 5135\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9770000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.600000\n",
      "episodes 5141\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9780000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.600000\n",
      "episodes 5146\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9790000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.600000\n",
      "episodes 5151\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9800000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.600000\n",
      "episodes 5156\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9810000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.600000\n",
      "episodes 5161\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9820000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.600000\n",
      "episodes 5166\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9830000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.600000\n",
      "episodes 5171\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9840000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.600000\n",
      "episodes 5177\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9850000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.600000\n",
      "episodes 5182\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9860000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.600000\n",
      "episodes 5187\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9870000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.600000\n",
      "episodes 5192\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9880000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.600000\n",
      "episodes 5197\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9890000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.600000\n",
      "episodes 5202\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9900000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.600000\n",
      "episodes 5207\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9910000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.600000\n",
      "episodes 5212\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9920000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.600000\n",
      "episodes 5218\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9930000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.600000\n",
      "episodes 5222\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9940000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.600000\n",
      "episodes 5228\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9950000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.600000\n",
      "episodes 5233\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9960000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.600000\n",
      "episodes 5238\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9970000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.600000\n",
      "episodes 5243\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9980000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.600000\n",
      "episodes 5248\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 9990000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.600000\n",
      "episodes 5253\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10000000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.600000\n",
      "episodes 5259\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10010000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.600000\n",
      "episodes 5263\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10020000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.600000\n",
      "episodes 5269\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10030000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.600000\n",
      "episodes 5274\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10040000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.600000\n",
      "episodes 5278\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10050000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.600000\n",
      "episodes 5283\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10060000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.600000\n",
      "episodes 5289\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10070000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.600000\n",
      "episodes 5294\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10080000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.600000\n",
      "episodes 5299\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 10090000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.600000\n",
      "episodes 5304\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10100000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.600000\n",
      "episodes 5309\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10110000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.600000\n",
      "episodes 5315\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10120000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.600000\n",
      "episodes 5320\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10130000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.600000\n",
      "episodes 5325\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10140000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.600000\n",
      "episodes 5330\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10150000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.600000\n",
      "episodes 5335\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10160000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.600000\n",
      "episodes 5340\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10170000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.600000\n",
      "episodes 5345\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10180000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.600000\n",
      "episodes 5350\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10190000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.600000\n",
      "episodes 5356\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10200000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.600000\n",
      "episodes 5361\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10210000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.600000\n",
      "episodes 5366\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10220000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.600000\n",
      "episodes 5371\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10230000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.600000\n",
      "episodes 5376\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10240000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.600000\n",
      "episodes 5381\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10250000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.600000\n",
      "episodes 5387\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10260000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.600000\n",
      "episodes 5392\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10270000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.600000\n",
      "episodes 5397\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10280000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.600000\n",
      "episodes 5402\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10290000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.600000\n",
      "episodes 5407\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10300000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.600000\n",
      "episodes 5413\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10310000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.600000\n",
      "episodes 5418\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10320000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.600000\n",
      "episodes 5423\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10330000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.600000\n",
      "episodes 5428\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10340000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.600000\n",
      "episodes 5433\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10350000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.600000\n",
      "episodes 5438\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10360000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.600000\n",
      "episodes 5443\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10370000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.600000\n",
      "episodes 5448\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10380000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.600000\n",
      "episodes 5454\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10390000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.600000\n",
      "episodes 5459\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10400000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.600000\n",
      "episodes 5464\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10410000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.600000\n",
      "episodes 5469\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10420000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.600000\n",
      "episodes 5474\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10430000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.600000\n",
      "episodes 5479\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10440000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.600000\n",
      "episodes 5484\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10450000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.600000\n",
      "episodes 5489\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10460000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.600000\n",
      "episodes 5494\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10470000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.600000\n",
      "episodes 5499\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10480000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.600000\n",
      "episodes 5504\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10490000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.600000\n",
      "episodes 5509\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10500000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.600000\n",
      "episodes 5515\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10510000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.600000\n",
      "episodes 5520\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10520000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.600000\n",
      "episodes 5525\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10530000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.600000\n",
      "episodes 5530\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10540000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.600000\n",
      "episodes 5536\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10550000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.600000\n",
      "episodes 5541\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10560000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.600000\n",
      "episodes 5546\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10570000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.600000\n",
      "episodes 5551\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10580000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.600000\n",
      "episodes 5556\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10590000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.600000\n",
      "episodes 5561\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10600000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.600000\n",
      "episodes 5566\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10610000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.600000\n",
      "episodes 5572\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10620000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.600000\n",
      "episodes 5577\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10630000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.600000\n",
      "episodes 5583\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10640000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.600000\n",
      "episodes 5588\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10650000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.600000\n",
      "episodes 5593\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10660000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.600000\n",
      "episodes 5598\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10670000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.600000\n",
      "episodes 5604\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 10680000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.600000\n",
      "episodes 5609\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10690000\n",
      "mean reward (100 episodes) 18.460000\n",
      "best mean reward 18.600000\n",
      "episodes 5614\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10700000\n",
      "mean reward (100 episodes) 18.490000\n",
      "best mean reward 18.600000\n",
      "episodes 5619\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10710000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.600000\n",
      "episodes 5624\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10720000\n",
      "mean reward (100 episodes) 18.460000\n",
      "best mean reward 18.600000\n",
      "episodes 5629\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10730000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.600000\n",
      "episodes 5634\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10740000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.600000\n",
      "episodes 5640\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10750000\n",
      "mean reward (100 episodes) 18.490000\n",
      "best mean reward 18.600000\n",
      "episodes 5645\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10760000\n",
      "mean reward (100 episodes) 18.570000\n",
      "best mean reward 18.600000\n",
      "episodes 5650\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10770000\n",
      "mean reward (100 episodes) 18.510000\n",
      "best mean reward 18.600000\n",
      "episodes 5655\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10780000\n",
      "mean reward (100 episodes) 18.510000\n",
      "best mean reward 18.600000\n",
      "episodes 5660\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10790000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.600000\n",
      "episodes 5666\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10800000\n",
      "mean reward (100 episodes) 18.510000\n",
      "best mean reward 18.600000\n",
      "episodes 5671\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10810000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.600000\n",
      "episodes 5676\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10820000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.600000\n",
      "episodes 5681\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10830000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.600000\n",
      "episodes 5687\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10840000\n",
      "mean reward (100 episodes) 18.460000\n",
      "best mean reward 18.600000\n",
      "episodes 5692\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10850000\n",
      "mean reward (100 episodes) 18.490000\n",
      "best mean reward 18.600000\n",
      "episodes 5698\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10860000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.600000\n",
      "episodes 5703\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10870000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.600000\n",
      "episodes 5708\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10880000\n",
      "mean reward (100 episodes) 18.530000\n",
      "best mean reward 18.600000\n",
      "episodes 5714\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10890000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.600000\n",
      "episodes 5719\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10900000\n",
      "mean reward (100 episodes) 18.660000\n",
      "best mean reward 18.660000\n",
      "episodes 5725\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10910000\n",
      "mean reward (100 episodes) 18.620000\n",
      "best mean reward 18.660000\n",
      "episodes 5730\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10920000\n",
      "mean reward (100 episodes) 18.640000\n",
      "best mean reward 18.660000\n",
      "episodes 5735\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10930000\n",
      "mean reward (100 episodes) 18.570000\n",
      "best mean reward 18.660000\n",
      "episodes 5740\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10940000\n",
      "mean reward (100 episodes) 18.510000\n",
      "best mean reward 18.660000\n",
      "episodes 5745\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10950000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.660000\n",
      "episodes 5750\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10960000\n",
      "mean reward (100 episodes) 18.580000\n",
      "best mean reward 18.660000\n",
      "episodes 5755\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10970000\n",
      "mean reward (100 episodes) 18.660000\n",
      "best mean reward 18.660000\n",
      "episodes 5761\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10980000\n",
      "mean reward (100 episodes) 18.630000\n",
      "best mean reward 18.660000\n",
      "episodes 5766\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 10990000\n",
      "mean reward (100 episodes) 18.630000\n",
      "best mean reward 18.680000\n",
      "episodes 5771\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11000000\n",
      "mean reward (100 episodes) 18.610000\n",
      "best mean reward 18.680000\n",
      "episodes 5776\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11010000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.680000\n",
      "episodes 5781\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11020000\n",
      "mean reward (100 episodes) 18.470000\n",
      "best mean reward 18.680000\n",
      "episodes 5786\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11030000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.680000\n",
      "episodes 5791\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11040000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.680000\n",
      "episodes 5797\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11050000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.680000\n",
      "episodes 5802\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11060000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.680000\n",
      "episodes 5807\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11070000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.680000\n",
      "episodes 5812\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11080000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.680000\n",
      "episodes 5817\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11090000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 5823\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11100000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 5828\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11110000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.680000\n",
      "episodes 5833\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11120000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.680000\n",
      "episodes 5838\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11130000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 5843\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11140000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.680000\n",
      "episodes 5849\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11150000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 5854\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11160000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 5859\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11170000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 5865\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11180000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.680000\n",
      "episodes 5870\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11190000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 5875\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11200000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.680000\n",
      "episodes 5880\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11210000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 5886\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11220000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.680000\n",
      "episodes 5891\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11230000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.680000\n",
      "episodes 5896\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11240000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 5901\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11250000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.680000\n",
      "episodes 5906\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11260000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 5912\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 11270000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 5917\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11280000\n",
      "mean reward (100 episodes) 18.470000\n",
      "best mean reward 18.680000\n",
      "episodes 5923\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11290000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.680000\n",
      "episodes 5928\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11300000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.680000\n",
      "episodes 5933\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11310000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 5938\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11320000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 5943\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11330000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.680000\n",
      "episodes 5948\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11340000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.680000\n",
      "episodes 5953\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11350000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.680000\n",
      "episodes 5959\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11360000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.680000\n",
      "episodes 5964\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11370000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 5969\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11380000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.680000\n",
      "episodes 5974\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11390000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.680000\n",
      "episodes 5980\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11400000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.680000\n",
      "episodes 5985\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11410000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.680000\n",
      "episodes 5990\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11420000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.680000\n",
      "episodes 5995\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 13:08:05,457] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video006000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 11430000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.680000\n",
      "episodes 6000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11440000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.680000\n",
      "episodes 6005\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11450000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.680000\n",
      "episodes 6010\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11460000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.680000\n",
      "episodes 6016\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11470000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.680000\n",
      "episodes 6021\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11480000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.680000\n",
      "episodes 6026\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11490000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.680000\n",
      "episodes 6031\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11500000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.680000\n",
      "episodes 6037\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11510000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.680000\n",
      "episodes 6042\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11520000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.680000\n",
      "episodes 6047\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11530000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.680000\n",
      "episodes 6052\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11540000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.680000\n",
      "episodes 6058\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11550000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.680000\n",
      "episodes 6063\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11560000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.680000\n",
      "episodes 6068\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11570000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 6073\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11580000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 6079\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11590000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 6084\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11600000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 6089\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11610000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 6095\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11620000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.680000\n",
      "episodes 6100\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11630000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.680000\n",
      "episodes 6105\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11640000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 6110\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11650000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.680000\n",
      "episodes 6116\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11660000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 6121\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11670000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.680000\n",
      "episodes 6126\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11680000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 6131\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11690000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.680000\n",
      "episodes 6137\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11700000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 6142\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11710000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.680000\n",
      "episodes 6147\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11720000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.680000\n",
      "episodes 6152\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11730000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 6157\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11740000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.680000\n",
      "episodes 6162\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11750000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 6167\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11760000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.680000\n",
      "episodes 6173\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11770000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 6178\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11780000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 6184\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11790000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.680000\n",
      "episodes 6189\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11800000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.680000\n",
      "episodes 6194\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11810000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.680000\n",
      "episodes 6200\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11820000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 6205\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11830000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 6210\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11840000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 6215\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11850000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 6221\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11860000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.680000\n",
      "episodes 6226\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11870000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.680000\n",
      "episodes 6231\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11880000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.680000\n",
      "episodes 6236\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11890000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.680000\n",
      "episodes 6242\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11900000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 6247\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11910000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 6252\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11920000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 6257\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11930000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 6262\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11940000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.680000\n",
      "episodes 6268\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11950000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.680000\n",
      "episodes 6273\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11960000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.680000\n",
      "episodes 6278\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11970000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.680000\n",
      "episodes 6284\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11980000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.680000\n",
      "episodes 6289\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 11990000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.680000\n",
      "episodes 6294\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12000000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.680000\n",
      "episodes 6299\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12010000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.680000\n",
      "episodes 6305\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 12020000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.680000\n",
      "episodes 6310\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12030000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 6315\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12040000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 6321\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12050000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.680000\n",
      "episodes 6326\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12060000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.680000\n",
      "episodes 6331\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12070000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 6336\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12080000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.680000\n",
      "episodes 6341\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12090000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.680000\n",
      "episodes 6347\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12100000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 6352\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12110000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 6357\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12120000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.680000\n",
      "episodes 6363\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12130000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.680000\n",
      "episodes 6368\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12140000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.680000\n",
      "episodes 6373\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12150000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.680000\n",
      "episodes 6378\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12160000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.680000\n",
      "episodes 6383\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12170000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.680000\n",
      "episodes 6389\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12180000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.680000\n",
      "episodes 6394\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12190000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.680000\n",
      "episodes 6399\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12200000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.680000\n",
      "episodes 6404\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12210000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.680000\n",
      "episodes 6409\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12220000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.680000\n",
      "episodes 6415\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12230000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.680000\n",
      "episodes 6420\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12240000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.680000\n",
      "episodes 6425\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12250000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.680000\n",
      "episodes 6430\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12260000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.680000\n",
      "episodes 6436\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12270000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 6441\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12280000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.680000\n",
      "episodes 6446\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12290000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.680000\n",
      "episodes 6452\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12300000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.680000\n",
      "episodes 6457\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12310000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.680000\n",
      "episodes 6462\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12320000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 6467\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12330000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.680000\n",
      "episodes 6473\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12340000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.680000\n",
      "episodes 6478\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12350000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 6483\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12360000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.680000\n",
      "episodes 6488\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12370000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.680000\n",
      "episodes 6494\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12380000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.680000\n",
      "episodes 6499\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12390000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.680000\n",
      "episodes 6504\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12400000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.680000\n",
      "episodes 6509\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12410000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.680000\n",
      "episodes 6514\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12420000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.680000\n",
      "episodes 6519\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12430000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.680000\n",
      "episodes 6525\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12440000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.680000\n",
      "episodes 6530\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12450000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.680000\n",
      "episodes 6535\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12460000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.680000\n",
      "episodes 6541\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12470000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.680000\n",
      "episodes 6546\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12480000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.680000\n",
      "episodes 6551\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12490000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 6556\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12500000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.680000\n",
      "episodes 6561\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12510000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.680000\n",
      "episodes 6567\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12520000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.680000\n",
      "episodes 6572\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12530000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.680000\n",
      "episodes 6577\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12540000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.680000\n",
      "episodes 6582\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12550000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.680000\n",
      "episodes 6588\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12560000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.680000\n",
      "episodes 6593\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12570000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.680000\n",
      "episodes 6598\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12580000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.680000\n",
      "episodes 6603\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12590000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.680000\n",
      "episodes 6608\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12600000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 6613\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 12610000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 6619\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12620000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.680000\n",
      "episodes 6624\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12630000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.680000\n",
      "episodes 6630\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12640000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.680000\n",
      "episodes 6635\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12650000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.680000\n",
      "episodes 6640\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12660000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 6645\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12670000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.680000\n",
      "episodes 6651\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12680000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.680000\n",
      "episodes 6656\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12690000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.680000\n",
      "episodes 6661\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12700000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 6666\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12710000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 6672\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12720000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.680000\n",
      "episodes 6677\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12730000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 6682\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12740000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.680000\n",
      "episodes 6687\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12750000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.680000\n",
      "episodes 6693\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12760000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.680000\n",
      "episodes 6698\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12770000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 6703\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12780000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.680000\n",
      "episodes 6708\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12790000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 6714\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12800000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.680000\n",
      "episodes 6719\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12810000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.680000\n",
      "episodes 6724\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12820000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 6730\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12830000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 6735\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12840000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.680000\n",
      "episodes 6740\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12850000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.680000\n",
      "episodes 6746\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12860000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.680000\n",
      "episodes 6751\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12870000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 6756\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12880000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.680000\n",
      "episodes 6761\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12890000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 6767\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12900000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 6772\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12910000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 6777\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12920000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.680000\n",
      "episodes 6782\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12930000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 6787\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12940000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.680000\n",
      "episodes 6793\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12950000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.680000\n",
      "episodes 6798\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12960000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.680000\n",
      "episodes 6803\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12970000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.680000\n",
      "episodes 6808\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12980000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.680000\n",
      "episodes 6813\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 12990000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.680000\n",
      "episodes 6819\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13000000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.680000\n",
      "episodes 6824\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13010000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.680000\n",
      "episodes 6829\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13020000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.680000\n",
      "episodes 6834\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13030000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.680000\n",
      "episodes 6840\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13040000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.680000\n",
      "episodes 6845\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13050000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.680000\n",
      "episodes 6850\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13060000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.680000\n",
      "episodes 6856\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13070000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.680000\n",
      "episodes 6861\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13080000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.680000\n",
      "episodes 6866\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13090000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.680000\n",
      "episodes 6872\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13100000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.680000\n",
      "episodes 6877\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13110000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.680000\n",
      "episodes 6882\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13120000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.680000\n",
      "episodes 6887\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13130000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.680000\n",
      "episodes 6892\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13140000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.680000\n",
      "episodes 6897\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13150000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.680000\n",
      "episodes 6902\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13160000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.680000\n",
      "episodes 6907\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13170000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.680000\n",
      "episodes 6912\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13180000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.680000\n",
      "episodes 6918\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13190000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.680000\n",
      "episodes 6923\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 13200000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.680000\n",
      "episodes 6928\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13210000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.680000\n",
      "episodes 6933\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13220000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.680000\n",
      "episodes 6939\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13230000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.680000\n",
      "episodes 6944\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13240000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.680000\n",
      "episodes 6949\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13250000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.680000\n",
      "episodes 6955\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13260000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.680000\n",
      "episodes 6960\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13270000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.680000\n",
      "episodes 6965\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13280000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.680000\n",
      "episodes 6971\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13290000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.680000\n",
      "episodes 6976\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13300000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.680000\n",
      "episodes 6981\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13310000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.680000\n",
      "episodes 6986\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13320000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.680000\n",
      "episodes 6991\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13330000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.680000\n",
      "episodes 6997\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 14:55:47,008] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video007000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 13340000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.680000\n",
      "episodes 7002\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13350000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 7007\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13360000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.680000\n",
      "episodes 7012\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13370000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 7018\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13380000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 7023\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13390000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 7028\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13400000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 7034\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13410000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 7039\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13420000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.680000\n",
      "episodes 7044\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13430000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 7050\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13440000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.680000\n",
      "episodes 7055\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13450000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.680000\n",
      "episodes 7060\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13460000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 7065\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13470000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 7070\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13480000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 7075\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13490000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.680000\n",
      "episodes 7080\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13500000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 7086\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13510000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 7091\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13520000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 7096\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13530000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.680000\n",
      "episodes 7102\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13540000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.680000\n",
      "episodes 7107\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13550000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.680000\n",
      "episodes 7113\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13560000\n",
      "mean reward (100 episodes) 18.490000\n",
      "best mean reward 18.680000\n",
      "episodes 7118\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13570000\n",
      "mean reward (100 episodes) 18.530000\n",
      "best mean reward 18.680000\n",
      "episodes 7124\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13580000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.680000\n",
      "episodes 7129\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13590000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.680000\n",
      "episodes 7134\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13600000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.680000\n",
      "episodes 7139\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13610000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.680000\n",
      "episodes 7144\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13620000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.680000\n",
      "episodes 7149\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13630000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 7155\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13640000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.680000\n",
      "episodes 7160\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13650000\n",
      "mean reward (100 episodes) 18.450000\n",
      "best mean reward 18.680000\n",
      "episodes 7165\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13660000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.680000\n",
      "episodes 7171\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13670000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 7176\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13680000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.680000\n",
      "episodes 7181\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13690000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.680000\n",
      "episodes 7186\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13700000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.680000\n",
      "episodes 7192\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13710000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.680000\n",
      "episodes 7197\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13720000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.680000\n",
      "episodes 7202\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13730000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 7208\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13740000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 7213\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13750000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 7218\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13760000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 7223\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13770000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.680000\n",
      "episodes 7229\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13780000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 7234\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13790000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 7239\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13800000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 7244\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13810000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.680000\n",
      "episodes 7250\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13820000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.680000\n",
      "episodes 7255\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13830000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 7261\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13840000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 7266\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13850000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 7272\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13860000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 7277\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13870000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.680000\n",
      "episodes 7282\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13880000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.680000\n",
      "episodes 7287\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13890000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.680000\n",
      "episodes 7292\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13900000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.680000\n",
      "episodes 7298\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13910000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.680000\n",
      "episodes 7303\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13920000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.680000\n",
      "episodes 7309\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 13930000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.680000\n",
      "episodes 7314\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13940000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.680000\n",
      "episodes 7319\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13950000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.680000\n",
      "episodes 7324\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13960000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.680000\n",
      "episodes 7329\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13970000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.680000\n",
      "episodes 7334\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13980000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.680000\n",
      "episodes 7340\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 13990000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.680000\n",
      "episodes 7345\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14000000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.680000\n",
      "episodes 7350\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14010000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.680000\n",
      "episodes 7355\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14020000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.680000\n",
      "episodes 7360\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14030000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.680000\n",
      "episodes 7366\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14040000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.680000\n",
      "episodes 7371\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14050000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.680000\n",
      "episodes 7377\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14060000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.680000\n",
      "episodes 7382\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14070000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 7387\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14080000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.680000\n",
      "episodes 7392\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14090000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.680000\n",
      "episodes 7398\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14100000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 7403\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14110000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.680000\n",
      "episodes 7408\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14120000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.680000\n",
      "episodes 7413\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14130000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.680000\n",
      "episodes 7419\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14140000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.680000\n",
      "episodes 7424\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14150000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.680000\n",
      "episodes 7429\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14160000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.680000\n",
      "episodes 7434\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14170000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.680000\n",
      "episodes 7440\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14180000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.680000\n",
      "episodes 7445\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14190000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 7450\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14200000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.680000\n",
      "episodes 7456\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14210000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.680000\n",
      "episodes 7461\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14220000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.680000\n",
      "episodes 7466\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14230000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.680000\n",
      "episodes 7471\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14240000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 7477\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14250000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.680000\n",
      "episodes 7482\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14260000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.680000\n",
      "episodes 7487\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14270000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 7493\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14280000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 7498\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14290000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.680000\n",
      "episodes 7504\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14300000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.680000\n",
      "episodes 7509\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14310000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 7514\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14320000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.680000\n",
      "episodes 7519\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14330000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 7525\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14340000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 7530\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14350000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 7536\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14360000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.680000\n",
      "episodes 7541\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14370000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.680000\n",
      "episodes 7546\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14380000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 7551\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14390000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 7557\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14400000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.680000\n",
      "episodes 7562\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14410000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 7567\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14420000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.680000\n",
      "episodes 7572\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14430000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 7578\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14440000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.680000\n",
      "episodes 7583\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14450000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.680000\n",
      "episodes 7588\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14460000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.680000\n",
      "episodes 7594\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14470000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 7599\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14480000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 7605\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14490000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 7610\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14500000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.680000\n",
      "episodes 7616\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14510000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 7621\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 14520000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 7626\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14530000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 7632\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14540000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.680000\n",
      "episodes 7637\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14550000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.680000\n",
      "episodes 7642\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14560000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.680000\n",
      "episodes 7647\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14570000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.680000\n",
      "episodes 7653\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14580000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.680000\n",
      "episodes 7658\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14590000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.680000\n",
      "episodes 7663\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14600000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 7668\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14610000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 7673\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14620000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 7678\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14630000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 7684\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14640000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 7688\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14650000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 7694\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14660000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 7699\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14670000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.680000\n",
      "episodes 7704\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14680000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.680000\n",
      "episodes 7709\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14690000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.680000\n",
      "episodes 7715\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14700000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 7720\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14710000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 7726\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14720000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.680000\n",
      "episodes 7731\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14730000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.680000\n",
      "episodes 7736\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14740000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.680000\n",
      "episodes 7741\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14750000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.680000\n",
      "episodes 7747\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14760000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 7752\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14770000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.680000\n",
      "episodes 7757\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14780000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 7763\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14790000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.680000\n",
      "episodes 7767\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14800000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.680000\n",
      "episodes 7772\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14810000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.680000\n",
      "episodes 7778\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14820000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 7783\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14830000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.680000\n",
      "episodes 7788\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14840000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.680000\n",
      "episodes 7794\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14850000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 7799\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14860000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 7804\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14870000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.680000\n",
      "episodes 7810\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14880000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.680000\n",
      "episodes 7815\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14890000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 7820\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14900000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.680000\n",
      "episodes 7825\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14910000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 7830\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14920000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 7836\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14930000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 7841\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14940000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 7846\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14950000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.680000\n",
      "episodes 7851\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14960000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.680000\n",
      "episodes 7857\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14970000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.680000\n",
      "episodes 7862\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14980000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.680000\n",
      "episodes 7868\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 14990000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 7873\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15000000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 7878\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15010000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.680000\n",
      "episodes 7883\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15020000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.680000\n",
      "episodes 7889\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15030000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.680000\n",
      "episodes 7894\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15040000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 7899\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15050000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.680000\n",
      "episodes 7904\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15060000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.680000\n",
      "episodes 7910\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15070000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 7915\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15080000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.680000\n",
      "episodes 7920\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15090000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 7926\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15100000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.680000\n",
      "episodes 7931\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 15110000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.680000\n",
      "episodes 7936\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15120000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 7941\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15130000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.680000\n",
      "episodes 7946\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15140000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.680000\n",
      "episodes 7951\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15150000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.680000\n",
      "episodes 7957\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15160000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.680000\n",
      "episodes 7962\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15170000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.680000\n",
      "episodes 7967\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15180000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.680000\n",
      "episodes 7972\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15190000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.680000\n",
      "episodes 7978\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15200000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.680000\n",
      "episodes 7983\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15210000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 7989\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15220000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.680000\n",
      "episodes 7994\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15230000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 7999\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 16:43:06,494] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video008000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 15240000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 8004\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15250000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.680000\n",
      "episodes 8009\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15260000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.680000\n",
      "episodes 8015\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15270000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.680000\n",
      "episodes 8020\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15280000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.680000\n",
      "episodes 8025\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15290000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.680000\n",
      "episodes 8030\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15300000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 8035\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15310000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 8041\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15320000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 8046\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15330000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.680000\n",
      "episodes 8051\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15340000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.680000\n",
      "episodes 8056\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15350000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.680000\n",
      "episodes 8061\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15360000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.680000\n",
      "episodes 8067\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15370000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 8072\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15380000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.680000\n",
      "episodes 8077\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15390000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.680000\n",
      "episodes 8082\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15400000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.680000\n",
      "episodes 8088\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15410000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.680000\n",
      "episodes 8093\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15420000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.680000\n",
      "episodes 8099\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15430000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 8104\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15440000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 8110\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15450000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 8115\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15460000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 8120\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15470000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.680000\n",
      "episodes 8126\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15480000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 8131\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15490000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 8136\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15500000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.680000\n",
      "episodes 8142\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15510000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 8147\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15520000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.680000\n",
      "episodes 8152\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15530000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 8158\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15540000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.680000\n",
      "episodes 8163\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15550000\n",
      "mean reward (100 episodes) 18.530000\n",
      "best mean reward 18.680000\n",
      "episodes 8169\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15560000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 8173\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15570000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.680000\n",
      "episodes 8178\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15580000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.680000\n",
      "episodes 8183\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15590000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 8188\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15600000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 8194\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15610000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 8199\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15620000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.680000\n",
      "episodes 8204\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15630000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.680000\n",
      "episodes 8209\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15640000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.680000\n",
      "episodes 8214\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15650000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.680000\n",
      "episodes 8219\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15660000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.680000\n",
      "episodes 8225\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15670000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.680000\n",
      "episodes 8230\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15680000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.680000\n",
      "episodes 8235\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15690000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.680000\n",
      "episodes 8241\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15700000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.680000\n",
      "episodes 8246\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15710000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.680000\n",
      "episodes 8251\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15720000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.680000\n",
      "episodes 8256\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15730000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.680000\n",
      "episodes 8262\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15740000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.680000\n",
      "episodes 8267\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15750000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.680000\n",
      "episodes 8272\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15760000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.680000\n",
      "episodes 8278\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15770000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.680000\n",
      "episodes 8283\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15780000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.680000\n",
      "episodes 8288\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15790000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.680000\n",
      "episodes 8293\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15800000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.680000\n",
      "episodes 8298\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15810000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.680000\n",
      "episodes 8303\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15820000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.680000\n",
      "episodes 8308\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 15830000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.680000\n",
      "episodes 8313\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15840000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.680000\n",
      "episodes 8319\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15850000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.680000\n",
      "episodes 8324\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15860000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.680000\n",
      "episodes 8329\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15870000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.680000\n",
      "episodes 8335\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15880000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.680000\n",
      "episodes 8340\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15890000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.680000\n",
      "episodes 8345\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15900000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.680000\n",
      "episodes 8350\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15910000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.680000\n",
      "episodes 8355\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15920000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.680000\n",
      "episodes 8360\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15930000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.680000\n",
      "episodes 8366\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15940000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.680000\n",
      "episodes 8371\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15950000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.680000\n",
      "episodes 8377\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15960000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.680000\n",
      "episodes 8382\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15970000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.680000\n",
      "episodes 8387\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15980000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.680000\n",
      "episodes 8393\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 15990000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.680000\n",
      "episodes 8398\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16000000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 8403\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16010000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.680000\n",
      "episodes 8408\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16020000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.680000\n",
      "episodes 8413\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16030000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.680000\n",
      "episodes 8418\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16040000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.680000\n",
      "episodes 8423\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16050000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.680000\n",
      "episodes 8429\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16060000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 8434\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16070000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.680000\n",
      "episodes 8439\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16080000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.680000\n",
      "episodes 8444\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16090000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.680000\n",
      "episodes 8450\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16100000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.680000\n",
      "episodes 8455\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16110000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 8461\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16120000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 8466\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16130000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 8471\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16140000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.680000\n",
      "episodes 8477\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16150000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.680000\n",
      "episodes 8481\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16160000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 8487\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16170000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.680000\n",
      "episodes 8492\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16180000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.680000\n",
      "episodes 8497\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16190000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.680000\n",
      "episodes 8503\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16200000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 8508\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16210000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 8514\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16220000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 8519\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16230000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.680000\n",
      "episodes 8525\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16240000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 8530\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16250000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.680000\n",
      "episodes 8535\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16260000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 8540\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16270000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.680000\n",
      "episodes 8546\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16280000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.680000\n",
      "episodes 8551\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16290000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.680000\n",
      "episodes 8556\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16300000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 8561\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16310000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 8567\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16320000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.680000\n",
      "episodes 8572\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16330000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.680000\n",
      "episodes 8577\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16340000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.680000\n",
      "episodes 8583\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16350000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.680000\n",
      "episodes 8588\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16360000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.680000\n",
      "episodes 8593\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16370000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 8598\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16380000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.680000\n",
      "episodes 8604\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16390000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.680000\n",
      "episodes 8609\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16400000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 8615\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16410000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 8620\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 16420000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 8625\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16430000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.680000\n",
      "episodes 8630\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16440000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.680000\n",
      "episodes 8636\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16450000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 8641\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16460000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 8646\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16470000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.680000\n",
      "episodes 8652\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16480000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 8657\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16490000\n",
      "mean reward (100 episodes) 18.450000\n",
      "best mean reward 18.680000\n",
      "episodes 8662\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16500000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.680000\n",
      "episodes 8668\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16510000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.680000\n",
      "episodes 8673\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16520000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.680000\n",
      "episodes 8678\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16530000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.680000\n",
      "episodes 8684\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16540000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.680000\n",
      "episodes 8689\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16550000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.680000\n",
      "episodes 8694\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16560000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 8700\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16570000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.680000\n",
      "episodes 8705\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16580000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 8710\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16590000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.680000\n",
      "episodes 8715\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16600000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.680000\n",
      "episodes 8721\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16610000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 8726\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16620000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.680000\n",
      "episodes 8731\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16630000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.680000\n",
      "episodes 8737\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16640000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 8742\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16650000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.680000\n",
      "episodes 8747\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16660000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 8753\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16670000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.680000\n",
      "episodes 8758\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16680000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 8763\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16690000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 8769\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16700000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.680000\n",
      "episodes 8774\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16710000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 8779\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16720000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.680000\n",
      "episodes 8785\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16730000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.680000\n",
      "episodes 8790\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16740000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.680000\n",
      "episodes 8796\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16750000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.680000\n",
      "episodes 8801\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16760000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.680000\n",
      "episodes 8806\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16770000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.680000\n",
      "episodes 8812\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16780000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.680000\n",
      "episodes 8817\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16790000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 8822\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16800000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.680000\n",
      "episodes 8827\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16810000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.680000\n",
      "episodes 8833\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16820000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.680000\n",
      "episodes 8838\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16830000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.680000\n",
      "episodes 8843\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16840000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.680000\n",
      "episodes 8848\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16850000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.680000\n",
      "episodes 8853\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16860000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.680000\n",
      "episodes 8859\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16870000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.680000\n",
      "episodes 8864\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16880000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.680000\n",
      "episodes 8869\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16890000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.680000\n",
      "episodes 8875\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16900000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.680000\n",
      "episodes 8880\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16910000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.680000\n",
      "episodes 8885\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16920000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.680000\n",
      "episodes 8890\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16930000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 8896\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16940000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.680000\n",
      "episodes 8901\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16950000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.680000\n",
      "episodes 8907\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16960000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 8912\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16970000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.680000\n",
      "episodes 8917\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16980000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.680000\n",
      "episodes 8923\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 16990000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.680000\n",
      "episodes 8928\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17000000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.680000\n",
      "episodes 8933\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 17010000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.680000\n",
      "episodes 8938\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17020000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.680000\n",
      "episodes 8944\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17030000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.680000\n",
      "episodes 8949\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17040000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.680000\n",
      "episodes 8954\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17050000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.680000\n",
      "episodes 8960\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17060000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.680000\n",
      "episodes 8965\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17070000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.680000\n",
      "episodes 8970\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17080000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 8975\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17090000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.680000\n",
      "episodes 8981\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17100000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.680000\n",
      "episodes 8986\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17110000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.680000\n",
      "episodes 8991\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17120000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.680000\n",
      "episodes 8996\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 18:30:02,066] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video009000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 17130000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.680000\n",
      "episodes 9001\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17140000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 9007\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17150000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.680000\n",
      "episodes 9012\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17160000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.680000\n",
      "episodes 9017\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17170000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.680000\n",
      "episodes 9022\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17180000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.680000\n",
      "episodes 9027\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17190000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.680000\n",
      "episodes 9032\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17200000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.680000\n",
      "episodes 9038\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17210000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.680000\n",
      "episodes 9043\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17220000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.680000\n",
      "episodes 9048\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17230000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.680000\n",
      "episodes 9053\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17240000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.680000\n",
      "episodes 9059\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17250000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 18.680000\n",
      "episodes 9064\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17260000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.680000\n",
      "episodes 9069\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17270000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.680000\n",
      "episodes 9074\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17280000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.680000\n",
      "episodes 9079\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17290000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.680000\n",
      "episodes 9085\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17300000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.680000\n",
      "episodes 9090\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17310000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.680000\n",
      "episodes 9095\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17320000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.680000\n",
      "episodes 9101\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17330000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.680000\n",
      "episodes 9106\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17340000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.680000\n",
      "episodes 9111\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17350000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.680000\n",
      "episodes 9116\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17360000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.680000\n",
      "episodes 9121\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17370000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.680000\n",
      "episodes 9127\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17380000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.680000\n",
      "episodes 9132\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17390000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.680000\n",
      "episodes 9137\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17400000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.680000\n",
      "episodes 9143\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17410000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.680000\n",
      "episodes 9148\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17420000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.680000\n",
      "episodes 9153\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17430000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.680000\n",
      "episodes 9159\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17440000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 9164\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17450000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.680000\n",
      "episodes 9169\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17460000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.680000\n",
      "episodes 9174\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17470000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.680000\n",
      "episodes 9179\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17480000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.680000\n",
      "episodes 9184\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17490000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.680000\n",
      "episodes 9190\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17500000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.680000\n",
      "episodes 9195\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17510000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.680000\n",
      "episodes 9200\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17520000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.680000\n",
      "episodes 9205\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17530000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.680000\n",
      "episodes 9211\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17540000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.680000\n",
      "episodes 9216\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17550000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.680000\n",
      "episodes 9221\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17560000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.680000\n",
      "episodes 9226\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17570000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.680000\n",
      "episodes 9231\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17580000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.680000\n",
      "episodes 9236\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17590000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.680000\n",
      "episodes 9241\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17600000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.680000\n",
      "episodes 9247\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17610000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.680000\n",
      "episodes 9252\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17620000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.680000\n",
      "episodes 9257\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17630000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.680000\n",
      "episodes 9262\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17640000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.680000\n",
      "episodes 9267\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17650000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.680000\n",
      "episodes 9272\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17660000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.680000\n",
      "episodes 9278\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17670000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.680000\n",
      "episodes 9283\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17680000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.680000\n",
      "episodes 9288\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 17690000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.680000\n",
      "episodes 9293\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21580000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.700000\n",
      "episodes 11339\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21590000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.700000\n",
      "episodes 11344\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 21600000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.700000\n",
      "episodes 11349\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21610000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.700000\n",
      "episodes 11355\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21620000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.700000\n",
      "episodes 11360\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21630000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.700000\n",
      "episodes 11365\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21640000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.700000\n",
      "episodes 11370\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21650000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.700000\n",
      "episodes 11375\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21660000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.700000\n",
      "episodes 11381\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21670000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.700000\n",
      "episodes 11386\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21680000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.700000\n",
      "episodes 11392\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21690000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.700000\n",
      "episodes 11397\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21700000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.700000\n",
      "episodes 11402\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21710000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.700000\n",
      "episodes 11407\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21720000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.700000\n",
      "episodes 11413\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21730000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.700000\n",
      "episodes 11418\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21740000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.700000\n",
      "episodes 11423\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21750000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.700000\n",
      "episodes 11429\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21760000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.700000\n",
      "episodes 11435\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21770000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.700000\n",
      "episodes 11440\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21780000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.700000\n",
      "episodes 11445\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21790000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.700000\n",
      "episodes 11450\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21800000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.700000\n",
      "episodes 11455\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21810000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.700000\n",
      "episodes 11461\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21820000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.700000\n",
      "episodes 11466\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21830000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.700000\n",
      "episodes 11471\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21840000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.700000\n",
      "episodes 11476\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21850000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.700000\n",
      "episodes 11481\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21860000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.700000\n",
      "episodes 11486\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21870000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.700000\n",
      "episodes 11492\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21880000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.700000\n",
      "episodes 11497\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21890000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.700000\n",
      "episodes 11502\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21900000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.700000\n",
      "episodes 11507\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21910000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.700000\n",
      "episodes 11513\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21920000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.700000\n",
      "episodes 11518\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21930000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.700000\n",
      "episodes 11523\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21940000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.700000\n",
      "episodes 11529\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21950000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.700000\n",
      "episodes 11534\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21960000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.700000\n",
      "episodes 11539\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21970000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.700000\n",
      "episodes 11544\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21980000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.700000\n",
      "episodes 11549\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 21990000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.700000\n",
      "episodes 11554\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22000000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.700000\n",
      "episodes 11560\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22010000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.700000\n",
      "episodes 11565\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22020000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.700000\n",
      "episodes 11570\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22030000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.700000\n",
      "episodes 11576\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22040000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.700000\n",
      "episodes 11581\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22050000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.700000\n",
      "episodes 11586\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22060000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.700000\n",
      "episodes 11591\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22070000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.700000\n",
      "episodes 11597\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22080000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.700000\n",
      "episodes 11602\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22090000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.700000\n",
      "episodes 11607\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22100000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.700000\n",
      "episodes 11612\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22110000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.700000\n",
      "episodes 11618\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22120000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.700000\n",
      "episodes 11623\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22130000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.700000\n",
      "episodes 11628\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22140000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.700000\n",
      "episodes 11634\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22150000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.700000\n",
      "episodes 11639\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22160000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.700000\n",
      "episodes 11644\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22170000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.700000\n",
      "episodes 11649\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22180000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.700000\n",
      "episodes 11655\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 22190000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.700000\n",
      "episodes 11660\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22200000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.700000\n",
      "episodes 11666\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22210000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.700000\n",
      "episodes 11671\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22220000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.700000\n",
      "episodes 11676\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22230000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.700000\n",
      "episodes 11682\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22240000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.700000\n",
      "episodes 11687\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22250000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.700000\n",
      "episodes 11692\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22260000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.700000\n",
      "episodes 11698\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22270000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.700000\n",
      "episodes 11703\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22280000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.700000\n",
      "episodes 11708\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22290000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.700000\n",
      "episodes 11713\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22300000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.700000\n",
      "episodes 11719\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22310000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.700000\n",
      "episodes 11724\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22320000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.700000\n",
      "episodes 11729\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22330000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.700000\n",
      "episodes 11734\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22340000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.700000\n",
      "episodes 11740\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22350000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.700000\n",
      "episodes 11745\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22360000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.700000\n",
      "episodes 11750\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22370000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.700000\n",
      "episodes 11756\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22380000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.700000\n",
      "episodes 11760\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22390000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.700000\n",
      "episodes 11766\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22400000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.700000\n",
      "episodes 11772\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22410000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.700000\n",
      "episodes 11777\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22420000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.700000\n",
      "episodes 11782\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22430000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.700000\n",
      "episodes 11788\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22440000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.700000\n",
      "episodes 11793\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22450000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.700000\n",
      "episodes 11799\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22460000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.700000\n",
      "episodes 11804\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22470000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.700000\n",
      "episodes 11809\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22480000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.700000\n",
      "episodes 11815\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22490000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.700000\n",
      "episodes 11820\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22500000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.700000\n",
      "episodes 11825\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22510000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.700000\n",
      "episodes 11831\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22520000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.700000\n",
      "episodes 11836\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22530000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.700000\n",
      "episodes 11842\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22540000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.700000\n",
      "episodes 11847\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22550000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.700000\n",
      "episodes 11852\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22560000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.700000\n",
      "episodes 11858\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22570000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.700000\n",
      "episodes 11863\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22580000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.700000\n",
      "episodes 11868\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22590000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.700000\n",
      "episodes 11874\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22600000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.700000\n",
      "episodes 11879\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22610000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.700000\n",
      "episodes 11884\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22620000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.700000\n",
      "episodes 11889\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22630000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.700000\n",
      "episodes 11895\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22640000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.700000\n",
      "episodes 11900\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22650000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.700000\n",
      "episodes 11905\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22660000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.700000\n",
      "episodes 11910\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22670000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.700000\n",
      "episodes 11915\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22680000\n",
      "mean reward (100 episodes) 18.510000\n",
      "best mean reward 18.700000\n",
      "episodes 11921\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22690000\n",
      "mean reward (100 episodes) 18.450000\n",
      "best mean reward 18.700000\n",
      "episodes 11926\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22700000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.700000\n",
      "episodes 11931\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22710000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.700000\n",
      "episodes 11936\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22720000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.700000\n",
      "episodes 11942\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22730000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.700000\n",
      "episodes 11947\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22740000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.700000\n",
      "episodes 11952\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22750000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.700000\n",
      "episodes 11957\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22760000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.700000\n",
      "episodes 11962\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22770000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.700000\n",
      "episodes 11967\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 22780000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.700000\n",
      "episodes 11973\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22790000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.700000\n",
      "episodes 11978\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22800000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.700000\n",
      "episodes 11983\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22810000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.700000\n",
      "episodes 11989\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22820000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.700000\n",
      "episodes 11995\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 23:51:31,763] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video012000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 22830000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.700000\n",
      "episodes 12000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22840000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.700000\n",
      "episodes 12005\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22850000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.700000\n",
      "episodes 12011\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22860000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.700000\n",
      "episodes 12016\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22870000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.700000\n",
      "episodes 12021\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22880000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.700000\n",
      "episodes 12026\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22890000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.700000\n",
      "episodes 12032\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22900000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.700000\n",
      "episodes 12037\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22910000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.700000\n",
      "episodes 12042\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22920000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.700000\n",
      "episodes 12047\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22930000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.700000\n",
      "episodes 12052\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22940000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.700000\n",
      "episodes 12057\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22950000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.700000\n",
      "episodes 12063\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22960000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.700000\n",
      "episodes 12068\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22970000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.700000\n",
      "episodes 12073\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22980000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.700000\n",
      "episodes 12079\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 22990000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.700000\n",
      "episodes 12084\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23000000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.700000\n",
      "episodes 12089\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23010000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.700000\n",
      "episodes 12095\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23020000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.700000\n",
      "episodes 12100\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23030000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.700000\n",
      "episodes 12105\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23040000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.700000\n",
      "episodes 12111\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23050000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.700000\n",
      "episodes 12116\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23060000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.700000\n",
      "episodes 12121\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23070000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.700000\n",
      "episodes 12127\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23080000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.700000\n",
      "episodes 12132\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23090000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.700000\n",
      "episodes 12138\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23100000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.700000\n",
      "episodes 12143\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23110000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.700000\n",
      "episodes 12148\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23120000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.700000\n",
      "episodes 12154\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23130000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.700000\n",
      "episodes 12159\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23140000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.700000\n",
      "episodes 12164\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23150000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.700000\n",
      "episodes 12169\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23160000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.700000\n",
      "episodes 12175\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23170000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.700000\n",
      "episodes 12180\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23180000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.700000\n",
      "episodes 12185\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23190000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.700000\n",
      "episodes 12191\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23200000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.700000\n",
      "episodes 12196\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23210000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.700000\n",
      "episodes 12201\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23220000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.700000\n",
      "episodes 12206\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23230000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.700000\n",
      "episodes 12212\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23240000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.700000\n",
      "episodes 12217\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23250000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.700000\n",
      "episodes 12222\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23260000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.700000\n",
      "episodes 12227\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23270000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.700000\n",
      "episodes 12233\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23280000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.700000\n",
      "episodes 12238\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23290000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.700000\n",
      "episodes 12243\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23300000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.700000\n",
      "episodes 12248\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23310000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.700000\n",
      "episodes 12254\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23320000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.700000\n",
      "episodes 12259\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23330000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.700000\n",
      "episodes 12264\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23340000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.700000\n",
      "episodes 12269\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23350000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.700000\n",
      "episodes 12275\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23360000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.700000\n",
      "episodes 12280\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23370000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.700000\n",
      "episodes 12285\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23380000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.700000\n",
      "episodes 12290\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23390000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.700000\n",
      "episodes 12295\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23400000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.700000\n",
      "episodes 12301\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23410000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.700000\n",
      "episodes 12306\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 23420000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.700000\n",
      "episodes 12311\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23430000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.700000\n",
      "episodes 12317\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23440000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.700000\n",
      "episodes 12322\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23450000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.700000\n",
      "episodes 12327\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23460000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.700000\n",
      "episodes 12333\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23470000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.700000\n",
      "episodes 12338\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23480000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.700000\n",
      "episodes 12343\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23490000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.700000\n",
      "episodes 12348\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23500000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.700000\n",
      "episodes 12353\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23510000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.700000\n",
      "episodes 12359\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23520000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.700000\n",
      "episodes 12364\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23530000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.700000\n",
      "episodes 12369\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23540000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.700000\n",
      "episodes 12375\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23550000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.700000\n",
      "episodes 12380\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23560000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.700000\n",
      "episodes 12385\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23570000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.700000\n",
      "episodes 12391\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23580000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.700000\n",
      "episodes 12396\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23590000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.700000\n",
      "episodes 12401\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23600000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.700000\n",
      "episodes 12406\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23610000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.700000\n",
      "episodes 12412\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23620000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.700000\n",
      "episodes 12417\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23630000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.700000\n",
      "episodes 12422\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23640000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.700000\n",
      "episodes 12427\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23650000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.700000\n",
      "episodes 12432\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23660000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.700000\n",
      "episodes 12437\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23670000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.700000\n",
      "episodes 12443\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23680000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.700000\n",
      "episodes 12448\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23690000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.700000\n",
      "episodes 12453\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23700000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.700000\n",
      "episodes 12458\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23710000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.700000\n",
      "episodes 12464\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23720000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.700000\n",
      "episodes 12469\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23730000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.700000\n",
      "episodes 12474\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23740000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.700000\n",
      "episodes 12480\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23750000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.700000\n",
      "episodes 12485\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23760000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.700000\n",
      "episodes 12490\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23770000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.700000\n",
      "episodes 12496\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23780000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.700000\n",
      "episodes 12501\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23790000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.700000\n",
      "episodes 12506\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23800000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.700000\n",
      "episodes 12512\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23810000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.700000\n",
      "episodes 12517\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23820000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.700000\n",
      "episodes 12522\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23830000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.700000\n",
      "episodes 12527\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23840000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.700000\n",
      "episodes 12533\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23850000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.700000\n",
      "episodes 12538\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23860000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.700000\n",
      "episodes 12543\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23870000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.700000\n",
      "episodes 12549\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23880000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.700000\n",
      "episodes 12554\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23890000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.700000\n",
      "episodes 12559\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23900000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.700000\n",
      "episodes 12564\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23910000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.700000\n",
      "episodes 12569\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23920000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.700000\n",
      "episodes 12575\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23930000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.700000\n",
      "episodes 12580\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23940000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.700000\n",
      "episodes 12585\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23950000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.700000\n",
      "episodes 12590\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23960000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.700000\n",
      "episodes 12595\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23970000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.700000\n",
      "episodes 12601\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23980000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.700000\n",
      "episodes 12606\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 23990000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.700000\n",
      "episodes 12611\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24000000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.700000\n",
      "episodes 12616\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 24010000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.700000\n",
      "episodes 12622\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24020000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.700000\n",
      "episodes 12627\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24030000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.700000\n",
      "episodes 12632\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24040000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.700000\n",
      "episodes 12637\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24050000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.700000\n",
      "episodes 12643\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24060000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.700000\n",
      "episodes 12648\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24070000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.700000\n",
      "episodes 12654\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24080000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.700000\n",
      "episodes 12659\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24090000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.700000\n",
      "episodes 12664\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24100000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.700000\n",
      "episodes 12670\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24110000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.700000\n",
      "episodes 12675\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24120000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.700000\n",
      "episodes 12680\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24130000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.700000\n",
      "episodes 12686\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24140000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.700000\n",
      "episodes 12691\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24150000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.700000\n",
      "episodes 12697\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24160000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.700000\n",
      "episodes 12702\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24170000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.700000\n",
      "episodes 12707\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24180000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.700000\n",
      "episodes 12713\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24190000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.700000\n",
      "episodes 12718\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24200000\n",
      "mean reward (100 episodes) 18.480000\n",
      "best mean reward 18.700000\n",
      "episodes 12723\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24210000\n",
      "mean reward (100 episodes) 18.450000\n",
      "best mean reward 18.700000\n",
      "episodes 12728\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24220000\n",
      "mean reward (100 episodes) 18.490000\n",
      "best mean reward 18.700000\n",
      "episodes 12734\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24230000\n",
      "mean reward (100 episodes) 18.520000\n",
      "best mean reward 18.700000\n",
      "episodes 12739\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24240000\n",
      "mean reward (100 episodes) 18.490000\n",
      "best mean reward 18.700000\n",
      "episodes 12744\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24250000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.700000\n",
      "episodes 12749\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24260000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.700000\n",
      "episodes 12754\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24270000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.700000\n",
      "episodes 12760\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24280000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.700000\n",
      "episodes 12765\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24290000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.700000\n",
      "episodes 12770\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24300000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.700000\n",
      "episodes 12775\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24310000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.700000\n",
      "episodes 12781\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24320000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.700000\n",
      "episodes 12786\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24330000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.700000\n",
      "episodes 12791\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24340000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.700000\n",
      "episodes 12796\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24350000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.700000\n",
      "episodes 12801\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24360000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.700000\n",
      "episodes 12807\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24370000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.700000\n",
      "episodes 12812\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24380000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.700000\n",
      "episodes 12817\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24390000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.700000\n",
      "episodes 12822\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24400000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.700000\n",
      "episodes 12827\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24410000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.700000\n",
      "episodes 12832\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24420000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.700000\n",
      "episodes 12838\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24430000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.700000\n",
      "episodes 12843\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24440000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.700000\n",
      "episodes 12848\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24450000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.700000\n",
      "episodes 12853\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24460000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.700000\n",
      "episodes 12858\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24470000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.700000\n",
      "episodes 12863\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24480000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.700000\n",
      "episodes 12869\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24490000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.700000\n",
      "episodes 12874\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24500000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.700000\n",
      "episodes 12879\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24510000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.700000\n",
      "episodes 12884\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24520000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.700000\n",
      "episodes 12890\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24530000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.700000\n",
      "episodes 12895\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24540000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.700000\n",
      "episodes 12901\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24550000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.700000\n",
      "episodes 12906\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24560000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.700000\n",
      "episodes 12911\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24570000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.700000\n",
      "episodes 12916\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24580000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.700000\n",
      "episodes 12922\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24590000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.700000\n",
      "episodes 12927\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 24600000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.700000\n",
      "episodes 12932\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24610000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.700000\n",
      "episodes 12937\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24620000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.700000\n",
      "episodes 12943\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24630000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.700000\n",
      "episodes 12948\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24640000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.700000\n",
      "episodes 12953\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24650000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.700000\n",
      "episodes 12958\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24660000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.700000\n",
      "episodes 12963\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24670000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.700000\n",
      "episodes 12969\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24680000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.700000\n",
      "episodes 12974\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24690000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.700000\n",
      "episodes 12980\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24700000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.700000\n",
      "episodes 12985\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24710000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.700000\n",
      "episodes 12991\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24720000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.700000\n",
      "episodes 12996\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 01:38:35,856] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video013000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 24730000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.700000\n",
      "episodes 13002\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24740000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.700000\n",
      "episodes 13007\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24750000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.700000\n",
      "episodes 13012\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24760000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.700000\n",
      "episodes 13017\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24770000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.700000\n",
      "episodes 13022\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24780000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.700000\n",
      "episodes 13028\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24790000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.700000\n",
      "episodes 13033\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24800000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.700000\n",
      "episodes 13038\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24810000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.700000\n",
      "episodes 13043\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24820000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.700000\n",
      "episodes 13049\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24830000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.700000\n",
      "episodes 13054\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24840000\n",
      "mean reward (100 episodes) 18.460000\n",
      "best mean reward 18.700000\n",
      "episodes 13060\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24850000\n",
      "mean reward (100 episodes) 18.630000\n",
      "best mean reward 18.700000\n",
      "episodes 13065\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24860000\n",
      "mean reward (100 episodes) 18.620000\n",
      "best mean reward 18.700000\n",
      "episodes 13071\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24870000\n",
      "mean reward (100 episodes) 18.560000\n",
      "best mean reward 18.700000\n",
      "episodes 13076\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24880000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.700000\n",
      "episodes 13081\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24890000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.700000\n",
      "episodes 13087\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24900000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.700000\n",
      "episodes 13092\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24910000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.700000\n",
      "episodes 13097\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24920000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.700000\n",
      "episodes 13102\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24930000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.700000\n",
      "episodes 13108\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24940000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.700000\n",
      "episodes 13113\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24950000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.700000\n",
      "episodes 13118\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24960000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.700000\n",
      "episodes 13123\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24970000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.700000\n",
      "episodes 13129\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24980000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.700000\n",
      "episodes 13134\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 24990000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.700000\n",
      "episodes 13139\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25000000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.700000\n",
      "episodes 13145\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25010000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.700000\n",
      "episodes 13150\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25020000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.700000\n",
      "episodes 13155\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25030000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.700000\n",
      "episodes 13160\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25040000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.700000\n",
      "episodes 13165\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25050000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.700000\n",
      "episodes 13170\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25060000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.700000\n",
      "episodes 13176\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25070000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.700000\n",
      "episodes 13181\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25080000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.700000\n",
      "episodes 13186\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25090000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.700000\n",
      "episodes 13192\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25100000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.700000\n",
      "episodes 13197\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25110000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.700000\n",
      "episodes 13202\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25120000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.700000\n",
      "episodes 13208\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25130000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.700000\n",
      "episodes 13213\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25140000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.700000\n",
      "episodes 13218\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25150000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.700000\n",
      "episodes 13223\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25160000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.700000\n",
      "episodes 13228\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25170000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.700000\n",
      "episodes 13234\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25180000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.700000\n",
      "episodes 13239\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25190000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.700000\n",
      "episodes 13244\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25200000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.700000\n",
      "episodes 13249\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25210000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.700000\n",
      "episodes 13255\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25220000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.700000\n",
      "episodes 13260\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25230000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.700000\n",
      "episodes 13265\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25240000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.700000\n",
      "episodes 13271\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25250000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.700000\n",
      "episodes 13276\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25260000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.700000\n",
      "episodes 13281\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25270000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.700000\n",
      "episodes 13286\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25280000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.700000\n",
      "episodes 13290\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25290000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.700000\n",
      "episodes 13296\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25300000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.700000\n",
      "episodes 13301\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25310000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.700000\n",
      "episodes 13307\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 25320000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.700000\n",
      "episodes 13312\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25330000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.700000\n",
      "episodes 13317\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25340000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.700000\n",
      "episodes 13323\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25350000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.700000\n",
      "episodes 13327\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25360000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.700000\n",
      "episodes 13333\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25370000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.700000\n",
      "episodes 13338\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25380000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.700000\n",
      "episodes 13343\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25390000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.700000\n",
      "episodes 13349\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25400000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.700000\n",
      "episodes 13354\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25410000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.700000\n",
      "episodes 13360\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25420000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.700000\n",
      "episodes 13365\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25430000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.700000\n",
      "episodes 13370\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25440000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.700000\n",
      "episodes 13375\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25450000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.700000\n",
      "episodes 13381\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25460000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.700000\n",
      "episodes 13386\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25470000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.700000\n",
      "episodes 13391\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25480000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.700000\n",
      "episodes 13396\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25490000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.700000\n",
      "episodes 13402\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25500000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.700000\n",
      "episodes 13407\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25510000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.700000\n",
      "episodes 13412\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25520000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.700000\n",
      "episodes 13418\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25530000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.700000\n",
      "episodes 13423\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25540000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.700000\n",
      "episodes 13429\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25550000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.700000\n",
      "episodes 13434\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25560000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.700000\n",
      "episodes 13440\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25570000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.700000\n",
      "episodes 13445\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25580000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.700000\n",
      "episodes 13450\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25590000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.700000\n",
      "episodes 13456\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25600000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.700000\n",
      "episodes 13461\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25610000\n",
      "mean reward (100 episodes) 18.550000\n",
      "best mean reward 18.700000\n",
      "episodes 13467\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25620000\n",
      "mean reward (100 episodes) 18.660000\n",
      "best mean reward 18.700000\n",
      "episodes 13472\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25630000\n",
      "mean reward (100 episodes) 18.600000\n",
      "best mean reward 18.700000\n",
      "episodes 13478\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25640000\n",
      "mean reward (100 episodes) 18.560000\n",
      "best mean reward 18.700000\n",
      "episodes 13483\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25650000\n",
      "mean reward (100 episodes) 18.680000\n",
      "best mean reward 18.700000\n",
      "episodes 13488\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25660000\n",
      "mean reward (100 episodes) 18.700000\n",
      "best mean reward 18.700000\n",
      "episodes 13494\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25670000\n",
      "mean reward (100 episodes) 18.710000\n",
      "best mean reward 18.740000\n",
      "episodes 13499\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25680000\n",
      "mean reward (100 episodes) 18.740000\n",
      "best mean reward 18.740000\n",
      "episodes 13504\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25690000\n",
      "mean reward (100 episodes) 18.770000\n",
      "best mean reward 18.770000\n",
      "episodes 13510\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25700000\n",
      "mean reward (100 episodes) 18.730000\n",
      "best mean reward 18.790000\n",
      "episodes 13515\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25710000\n",
      "mean reward (100 episodes) 18.620000\n",
      "best mean reward 18.790000\n",
      "episodes 13520\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25720000\n",
      "mean reward (100 episodes) 18.520000\n",
      "best mean reward 18.790000\n",
      "episodes 13525\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25730000\n",
      "mean reward (100 episodes) 18.580000\n",
      "best mean reward 18.790000\n",
      "episodes 13531\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25740000\n",
      "mean reward (100 episodes) 18.530000\n",
      "best mean reward 18.790000\n",
      "episodes 13536\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25750000\n",
      "mean reward (100 episodes) 18.560000\n",
      "best mean reward 18.790000\n",
      "episodes 13541\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25760000\n",
      "mean reward (100 episodes) 18.450000\n",
      "best mean reward 18.790000\n",
      "episodes 13546\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25770000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 13552\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25780000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 13557\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25790000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.790000\n",
      "episodes 13562\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25800000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 13568\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25810000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 13573\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25820000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 13578\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25830000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 13584\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25840000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 13590\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25850000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 13595\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25860000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 13600\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25870000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 13606\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25880000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 13611\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25890000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 13616\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25900000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 13621\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 25910000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 13626\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25920000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 13631\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25930000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 13637\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25940000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 13642\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25950000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 13647\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25960000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 13653\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25970000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 13658\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25980000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 13663\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 25990000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 13668\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26000000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 13674\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26010000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 13679\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26020000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 13684\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26030000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 13690\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26040000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 13695\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26050000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 13700\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26060000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 13705\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26070000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 13710\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26080000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 13715\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26090000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 13721\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26100000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 13726\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26110000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 13731\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26120000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 13737\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26130000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 13742\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26140000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 13747\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26150000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 13752\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26160000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 13758\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26170000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 13763\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26180000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 13769\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26190000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 13774\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26200000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 13780\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26210000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 13785\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26220000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 13790\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26230000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 13796\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26240000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 13801\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26250000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 13806\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26260000\n",
      "mean reward (100 episodes) 18.530000\n",
      "best mean reward 18.790000\n",
      "episodes 13812\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26270000\n",
      "mean reward (100 episodes) 18.490000\n",
      "best mean reward 18.790000\n",
      "episodes 13817\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26280000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.790000\n",
      "episodes 13822\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26290000\n",
      "mean reward (100 episodes) 18.450000\n",
      "best mean reward 18.790000\n",
      "episodes 13827\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26300000\n",
      "mean reward (100 episodes) 18.470000\n",
      "best mean reward 18.790000\n",
      "episodes 13833\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26310000\n",
      "mean reward (100 episodes) 18.530000\n",
      "best mean reward 18.790000\n",
      "episodes 13838\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26320000\n",
      "mean reward (100 episodes) 18.460000\n",
      "best mean reward 18.790000\n",
      "episodes 13843\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26330000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 13849\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26340000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 13854\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26350000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 13859\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26360000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 13864\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26370000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 13869\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26380000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 13875\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26390000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 13880\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26400000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 13886\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26410000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 13891\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26420000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 13896\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26430000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 13902\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26440000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 13907\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26450000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 13912\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26460000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 13917\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26470000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 13923\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26480000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 13928\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26490000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 13933\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 26500000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 13938\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26510000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 13944\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26520000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 13949\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26530000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 13954\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26540000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 13960\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26550000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 13965\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26560000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 13970\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26570000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 13975\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26580000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 13980\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26590000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 13986\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26600000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 13991\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26610000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 13997\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 03:25:10,546] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video014000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 26620000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 14003\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26630000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 14008\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26640000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 14013\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26650000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 14019\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26660000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 14024\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26670000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 14029\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26680000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 14034\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26690000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 14040\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26700000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 14045\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26710000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 14050\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26720000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 14055\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26730000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 14060\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26740000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 14066\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26750000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 14071\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26760000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 14076\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26770000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 14081\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26780000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 14087\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26790000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 14092\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26800000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 14097\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26810000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 14103\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26820000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 14108\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26830000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 14113\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26840000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 14119\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26850000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 14124\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26860000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 14129\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26870000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 14134\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26880000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.790000\n",
      "episodes 14140\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26890000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 14145\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26900000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 14151\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26910000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 14156\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26920000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 14161\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26930000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 14166\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26940000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 14172\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26950000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 14177\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26960000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 14182\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26970000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 14187\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26980000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 14193\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 26990000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 14198\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27000000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 14203\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27010000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 14209\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27020000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 14214\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27030000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 14219\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27040000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 14224\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27050000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 14229\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27060000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 14234\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27070000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 14240\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27080000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 14246\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27090000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 14251\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27100000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 14256\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27110000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 14261\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27120000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 14267\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27130000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 14272\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27140000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 14277\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27150000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 14282\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27160000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 14288\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27170000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.790000\n",
      "episodes 14292\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27180000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.790000\n",
      "episodes 14298\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27190000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 14303\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27200000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 14308\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 27210000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 14313\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27220000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 14319\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27230000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 14324\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27240000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 14330\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27250000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 14335\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27260000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.790000\n",
      "episodes 14340\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27270000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.790000\n",
      "episodes 14345\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27280000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.790000\n",
      "episodes 14350\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27290000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.790000\n",
      "episodes 14356\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27300000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.790000\n",
      "episodes 14361\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27310000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.790000\n",
      "episodes 14366\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27320000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 14372\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27330000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 14377\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27340000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 14382\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27350000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.790000\n",
      "episodes 14388\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27360000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 14393\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27370000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 14398\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27380000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.790000\n",
      "episodes 14403\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27390000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 14409\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27400000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 14414\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27410000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 14420\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27420000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 14425\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27430000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 14431\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27440000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 14436\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27450000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 14441\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27460000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 14446\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27470000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 14452\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27480000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 14457\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27490000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 14463\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27500000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 14468\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27510000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 14473\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27520000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 14478\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27530000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 14483\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27540000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 14489\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27550000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 14494\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27560000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 14499\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27570000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 14505\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27580000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 14510\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27590000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 14516\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27600000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 14521\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27610000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 14526\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27620000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 14532\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27630000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 14537\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27640000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 14542\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27650000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 14547\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27660000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 14553\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27670000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 14558\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27680000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 14563\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27690000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 14568\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27700000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 14574\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27710000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 14579\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27720000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 14584\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27730000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 14589\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27740000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 14595\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27750000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 14600\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27760000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 14605\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27770000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 14611\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27780000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 14616\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27790000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 14621\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 27800000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 14627\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27810000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 14632\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27820000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 14637\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27830000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 14642\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27840000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 14647\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27850000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 14653\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27860000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 14658\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27870000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 14663\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27880000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 14669\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27890000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 14674\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27900000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 14680\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27910000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 14685\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27920000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 14690\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27930000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 14696\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27940000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 14701\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27950000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 14707\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27960000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 14712\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27970000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 14717\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27980000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 14722\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 27990000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 14727\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28000000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 14733\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28010000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.790000\n",
      "episodes 14738\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28020000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 14744\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28030000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 14750\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28040000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 14755\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28050000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 14760\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28060000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 14765\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28070000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 14770\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28080000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 14775\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28090000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 14781\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28100000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 14786\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28110000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 14791\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28120000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 14797\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28130000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 14802\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28140000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 14807\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28150000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 14813\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28160000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 14818\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28170000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 14823\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28180000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 14829\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28190000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 14834\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28200000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 14839\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28210000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 14844\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28220000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 14850\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28230000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 14855\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28240000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 14860\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28250000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 14865\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28260000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 14871\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28270000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 14876\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28280000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 14881\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28290000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 14887\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28300000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 14892\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28310000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 14897\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28320000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 14903\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28330000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 14908\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28340000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 14913\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28350000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 14919\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28360000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 14924\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28370000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 14930\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28380000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 14935\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 28390000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 14940\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28400000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 14946\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28410000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 14951\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28420000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 14956\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28430000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 14962\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28440000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 14967\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28450000\n",
      "mean reward (100 episodes) 18.480000\n",
      "best mean reward 18.790000\n",
      "episodes 14972\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28460000\n",
      "mean reward (100 episodes) 18.520000\n",
      "best mean reward 18.790000\n",
      "episodes 14978\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28470000\n",
      "mean reward (100 episodes) 18.470000\n",
      "best mean reward 18.790000\n",
      "episodes 14983\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28480000\n",
      "mean reward (100 episodes) 18.480000\n",
      "best mean reward 18.790000\n",
      "episodes 14989\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28490000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.790000\n",
      "episodes 14994\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28500000\n",
      "mean reward (100 episodes) 18.470000\n",
      "best mean reward 18.790000\n",
      "episodes 14999\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 05:11:18,343] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video015000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 28510000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.790000\n",
      "episodes 15005\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28520000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 15010\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28530000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 15015\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28540000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 15020\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28550000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 15025\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28560000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 15030\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28570000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 15036\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28580000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 15041\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28590000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 15047\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28600000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 15052\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28610000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 15058\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28620000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 15063\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28630000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 15068\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28640000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 15073\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28650000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 15078\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28660000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 15083\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28670000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 15089\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28680000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 15094\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28690000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 15099\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28700000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 15105\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28710000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 15110\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28720000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 15115\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28730000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 15120\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28740000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 15126\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28750000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 15131\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28760000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 15137\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28770000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 15142\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28780000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 15147\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28790000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 15152\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28800000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.790000\n",
      "episodes 15157\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28810000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 15162\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28820000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 15168\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28830000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 15173\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28840000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 15178\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28850000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 15183\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28860000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 15189\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28870000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.790000\n",
      "episodes 15194\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28880000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.790000\n",
      "episodes 15199\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28890000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.790000\n",
      "episodes 15204\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28900000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 18.790000\n",
      "episodes 15209\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28910000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.790000\n",
      "episodes 15215\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28920000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.790000\n",
      "episodes 15220\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28930000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.790000\n",
      "episodes 15225\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28940000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.790000\n",
      "episodes 15231\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28950000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.790000\n",
      "episodes 15236\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28960000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.790000\n",
      "episodes 15241\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28970000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.790000\n",
      "episodes 15246\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28980000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.790000\n",
      "episodes 15252\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 28990000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.790000\n",
      "episodes 15257\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29000000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.790000\n",
      "episodes 15262\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29010000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.790000\n",
      "episodes 15267\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29020000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.790000\n",
      "episodes 15272\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29030000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.790000\n",
      "episodes 15277\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29040000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.790000\n",
      "episodes 15282\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29050000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.790000\n",
      "episodes 15288\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29060000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 15293\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29070000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 15298\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29080000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 15303\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29090000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 15309\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 29100000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 15314\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29110000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.790000\n",
      "episodes 15319\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29120000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 15325\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29130000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 15330\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29140000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 15336\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29150000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 15341\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29160000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 15346\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29170000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 15351\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29180000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 15356\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29190000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 15362\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29200000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 15367\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29210000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 15372\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29220000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 15378\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29230000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 15383\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29240000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 15388\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29250000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 15393\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29260000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 15398\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29270000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 15403\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29280000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 15408\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29290000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 15414\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29300000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 15419\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29310000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 15424\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29320000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 15430\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29330000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 15434\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29340000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 15440\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29350000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.790000\n",
      "episodes 15445\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29360000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.790000\n",
      "episodes 15450\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29370000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.790000\n",
      "episodes 15456\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29380000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.790000\n",
      "episodes 15461\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29390000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.790000\n",
      "episodes 15466\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29400000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.790000\n",
      "episodes 15471\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29410000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.790000\n",
      "episodes 15476\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29420000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.790000\n",
      "episodes 15482\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29430000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.790000\n",
      "episodes 15487\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29440000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.790000\n",
      "episodes 15492\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29450000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 15497\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29460000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.790000\n",
      "episodes 15502\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29470000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.790000\n",
      "episodes 15507\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29480000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.790000\n",
      "episodes 15513\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29490000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.790000\n",
      "episodes 15518\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29500000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.790000\n",
      "episodes 15523\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29510000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.790000\n",
      "episodes 15529\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29520000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.790000\n",
      "episodes 15534\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29530000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.790000\n",
      "episodes 15539\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29540000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.790000\n",
      "episodes 15544\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29550000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.790000\n",
      "episodes 15549\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29560000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.790000\n",
      "episodes 15554\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29570000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.790000\n",
      "episodes 15559\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29580000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.790000\n",
      "episodes 15565\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29590000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.790000\n",
      "episodes 15570\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29600000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.790000\n",
      "episodes 15575\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29610000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.790000\n",
      "episodes 15581\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29620000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.790000\n",
      "episodes 15586\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29630000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.790000\n",
      "episodes 15592\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29640000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.790000\n",
      "episodes 15597\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29650000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 15603\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29660000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 15608\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29670000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 15613\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29680000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 15619\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 29690000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 15623\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29700000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 15628\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29710000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 15634\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29720000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 15639\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29730000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 15644\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29740000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.790000\n",
      "episodes 15648\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29750000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 15654\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29760000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 15659\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29770000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 15664\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29780000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 15670\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29790000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 15675\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29800000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 15680\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29810000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 15686\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29820000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 15691\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29830000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 15697\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29840000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 15702\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29850000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 15708\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29860000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 15713\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29870000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 15718\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29880000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 15723\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29890000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 15729\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29900000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 15734\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29910000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 15739\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29920000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 15745\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29930000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.790000\n",
      "episodes 15750\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29940000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.790000\n",
      "episodes 15755\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29950000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.790000\n",
      "episodes 15761\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29960000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.790000\n",
      "episodes 15766\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29970000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.790000\n",
      "episodes 15771\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29980000\n",
      "mean reward (100 episodes) 18.450000\n",
      "best mean reward 18.790000\n",
      "episodes 15776\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 29990000\n",
      "mean reward (100 episodes) 18.510000\n",
      "best mean reward 18.790000\n",
      "episodes 15782\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30000000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 15787\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30010000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 15792\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30020000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 15797\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30030000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 15803\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30040000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 15808\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30050000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 15814\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30060000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 15819\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30070000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 15824\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30080000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 15830\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30090000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 15835\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30100000\n",
      "mean reward (100 episodes) 18.460000\n",
      "best mean reward 18.790000\n",
      "episodes 15841\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30110000\n",
      "mean reward (100 episodes) 18.460000\n",
      "best mean reward 18.790000\n",
      "episodes 15846\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30120000\n",
      "mean reward (100 episodes) 18.450000\n",
      "best mean reward 18.790000\n",
      "episodes 15851\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30130000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 15857\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30140000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.790000\n",
      "episodes 15862\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30150000\n",
      "mean reward (100 episodes) 18.510000\n",
      "best mean reward 18.790000\n",
      "episodes 15867\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30160000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.790000\n",
      "episodes 15872\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30170000\n",
      "mean reward (100 episodes) 18.510000\n",
      "best mean reward 18.790000\n",
      "episodes 15878\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30180000\n",
      "mean reward (100 episodes) 18.460000\n",
      "best mean reward 18.790000\n",
      "episodes 15883\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30190000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.790000\n",
      "episodes 15888\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30200000\n",
      "mean reward (100 episodes) 18.530000\n",
      "best mean reward 18.790000\n",
      "episodes 15894\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30210000\n",
      "mean reward (100 episodes) 18.520000\n",
      "best mean reward 18.790000\n",
      "episodes 15899\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30220000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.790000\n",
      "episodes 15904\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30230000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.790000\n",
      "episodes 15909\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30240000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 15914\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30250000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 15919\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30260000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 15925\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30270000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.790000\n",
      "episodes 15930\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 30280000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 15936\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30290000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 15941\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30300000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 15946\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30310000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 15952\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30320000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.790000\n",
      "episodes 15957\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30330000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 15962\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30340000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 15968\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30350000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.790000\n",
      "episodes 15973\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30360000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 15978\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30370000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 15983\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30380000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 15988\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30390000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 15994\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30400000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 15999\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 06:58:44,406] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video016000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 30410000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 16004\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30420000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.790000\n",
      "episodes 16010\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30430000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 16015\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30440000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.790000\n",
      "episodes 16020\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30450000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 16026\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30460000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 16031\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30470000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 16037\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30480000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 16042\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30490000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 16047\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30500000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 16053\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30510000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 16058\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30520000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 16064\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30530000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 16069\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30540000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 16074\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30550000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 16080\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30560000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 16085\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30570000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 16091\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30580000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 16096\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30590000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 16102\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30600000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.790000\n",
      "episodes 16107\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30610000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 16112\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30620000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.790000\n",
      "episodes 16118\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30630000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.790000\n",
      "episodes 16123\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30640000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.790000\n",
      "episodes 16128\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30650000\n",
      "mean reward (100 episodes) 18.490000\n",
      "best mean reward 18.790000\n",
      "episodes 16133\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30660000\n",
      "mean reward (100 episodes) 18.480000\n",
      "best mean reward 18.790000\n",
      "episodes 16139\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30670000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.790000\n",
      "episodes 16144\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30680000\n",
      "mean reward (100 episodes) 18.510000\n",
      "best mean reward 18.790000\n",
      "episodes 16149\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30690000\n",
      "mean reward (100 episodes) 18.520000\n",
      "best mean reward 18.790000\n",
      "episodes 16155\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30700000\n",
      "mean reward (100 episodes) 18.570000\n",
      "best mean reward 18.790000\n",
      "episodes 16159\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30710000\n",
      "mean reward (100 episodes) 18.580000\n",
      "best mean reward 18.790000\n",
      "episodes 16165\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30720000\n",
      "mean reward (100 episodes) 18.590000\n",
      "best mean reward 18.790000\n",
      "episodes 16170\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30730000\n",
      "mean reward (100 episodes) 18.550000\n",
      "best mean reward 18.790000\n",
      "episodes 16176\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30740000\n",
      "mean reward (100 episodes) 18.590000\n",
      "best mean reward 18.790000\n",
      "episodes 16181\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30750000\n",
      "mean reward (100 episodes) 18.570000\n",
      "best mean reward 18.790000\n",
      "episodes 16187\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30760000\n",
      "mean reward (100 episodes) 18.580000\n",
      "best mean reward 18.790000\n",
      "episodes 16192\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30770000\n",
      "mean reward (100 episodes) 18.590000\n",
      "best mean reward 18.790000\n",
      "episodes 16198\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30780000\n",
      "mean reward (100 episodes) 18.470000\n",
      "best mean reward 18.790000\n",
      "episodes 16203\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30790000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.790000\n",
      "episodes 16208\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30800000\n",
      "mean reward (100 episodes) 18.490000\n",
      "best mean reward 18.790000\n",
      "episodes 16213\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30810000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.790000\n",
      "episodes 16218\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30820000\n",
      "mean reward (100 episodes) 18.520000\n",
      "best mean reward 18.790000\n",
      "episodes 16224\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30830000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.790000\n",
      "episodes 16229\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30840000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.790000\n",
      "episodes 16234\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30850000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.790000\n",
      "episodes 16239\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30860000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 16245\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30870000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 16250\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30880000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.790000\n",
      "episodes 16255\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30890000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.790000\n",
      "episodes 16260\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30900000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 16266\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30910000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 16271\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30920000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 16277\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30930000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 16282\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30940000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 16287\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30950000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 16292\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30960000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 16298\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30970000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 16303\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30980000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 16308\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 30990000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 16314\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 31000000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 16319\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31010000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 16324\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31020000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 16329\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31030000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 16334\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31040000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 16339\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31050000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 16344\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31060000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 16350\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31070000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 16355\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31080000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 16359\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31090000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.790000\n",
      "episodes 16365\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31100000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.790000\n",
      "episodes 16370\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31110000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 16376\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31120000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 16381\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31130000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 16386\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31140000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 16391\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31150000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.790000\n",
      "episodes 16397\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31160000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 16402\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31170000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 16407\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31180000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 16412\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31190000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 16418\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31200000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.790000\n",
      "episodes 16423\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31210000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.790000\n",
      "episodes 16428\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31220000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 16434\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31230000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 16439\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31240000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 16445\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31250000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 16450\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31260000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 16455\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31270000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 16461\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31280000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 16466\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31290000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 16471\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31300000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 16477\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31310000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 16482\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31320000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 16487\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31330000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 16492\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31340000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 16498\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31350000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 16503\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31360000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 16508\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31370000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 16514\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31380000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 16519\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31390000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 16524\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31400000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 16530\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31410000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 16535\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31420000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 16540\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31430000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 16546\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31440000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 16551\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31450000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 16556\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31460000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 16561\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31470000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.790000\n",
      "episodes 16566\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31480000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 16572\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31490000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 16577\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31500000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 16583\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31510000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 16588\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31520000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 16593\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31530000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 16598\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31540000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 16604\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31550000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 16610\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31560000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 16615\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31570000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 16620\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31580000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 16625\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 31590000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 16631\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31600000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 16636\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31610000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 16642\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31620000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 16647\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31630000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 16652\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31640000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 16657\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31650000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 16663\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31660000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 16668\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31670000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 16673\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31680000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 16678\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31690000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 16684\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31700000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 16689\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31710000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 16694\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31720000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 16700\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31730000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 16705\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31740000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 16711\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31750000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 16716\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31760000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 16721\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31770000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 16727\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31780000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 16732\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31790000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 16737\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31800000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 16743\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31810000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 16748\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31820000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 16753\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31830000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 16758\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31840000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 16763\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31850000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 16768\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31860000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 16774\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31870000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 16779\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31880000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 16784\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31890000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 16790\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31900000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 16795\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31910000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 16800\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31920000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 16805\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31930000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 16811\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31940000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 16816\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31950000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 16821\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31960000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 16827\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31970000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 16832\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31980000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 16837\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 31990000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 16842\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32000000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 16848\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32010000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 16853\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32020000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 16859\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32030000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 16864\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32040000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 16869\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32050000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 16875\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32060000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 16880\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32070000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 16885\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32080000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 16890\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32090000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 16896\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32100000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 16901\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32110000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 16907\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32120000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 16912\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32130000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 16917\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32140000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 16922\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32150000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 16927\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32160000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 16933\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32170000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 16938\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 32180000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 16943\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32190000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 16949\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32200000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 16954\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32210000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 16959\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32220000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 16964\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32230000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 16970\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32240000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 16975\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32250000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 16980\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32260000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 16985\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32270000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 16990\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32280000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 16995\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 08:44:55,541] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video017000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 32290000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 17001\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32300000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 17006\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32310000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 17011\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32320000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 17017\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32330000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 17022\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32340000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 17027\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32350000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 17033\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32360000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 17038\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32370000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 17044\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32380000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 17049\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32390000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 17054\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32400000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 17060\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32410000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 17065\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32420000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 17070\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32430000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 17075\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32440000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 17081\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32450000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 17086\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32460000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 17092\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32470000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 17097\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32480000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 17102\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32490000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 17107\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32500000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 17113\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32510000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 17118\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32520000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 17123\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32530000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 17129\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32540000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 17134\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32550000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 17139\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32560000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 17145\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32570000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 17150\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32580000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 17156\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32590000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 17161\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32600000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 17166\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32610000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 17172\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32620000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 17177\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32630000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 17182\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32640000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 17188\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32650000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 17193\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32660000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 17198\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32670000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 17203\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32680000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 17208\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32690000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 17213\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32700000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 17219\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32710000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 17224\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32720000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 17230\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32730000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 17235\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32740000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 17241\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32750000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 17246\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32760000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 17251\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32770000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 17256\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32780000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 17261\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32790000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 17267\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32800000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 17271\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32810000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 17277\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32820000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 17282\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32830000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 17287\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32840000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 17293\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32850000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 17298\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32860000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 17303\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32870000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 17309\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 32880000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 17314\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32890000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 17319\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32900000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 17325\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32910000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 17330\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32920000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 17335\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32930000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 17341\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32940000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 17346\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32950000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 17352\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32960000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 17357\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32970000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 17362\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32980000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 17367\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 32990000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 17373\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33000000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 17378\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33010000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 17383\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33020000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 17388\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33030000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 17394\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33040000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 17399\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33050000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 17404\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33060000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 17409\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33070000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 17415\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33080000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 17420\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33090000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 17425\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33100000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 17431\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33110000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 17436\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33120000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 17441\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33130000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 17447\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33140000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 17452\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33150000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 17458\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33160000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 17463\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33170000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 17468\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33180000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 17473\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33190000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 17479\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33200000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 17484\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33210000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 17489\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33220000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 17494\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33230000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 17499\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33240000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 17505\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33250000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 17510\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33260000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 17516\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33270000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 17521\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33280000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 17526\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33290000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 17531\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33300000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 17537\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33310000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 17542\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33320000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 17547\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33330000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 17553\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33340000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.790000\n",
      "episodes 17558\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33350000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.790000\n",
      "episodes 17563\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33360000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.790000\n",
      "episodes 17568\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33370000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.790000\n",
      "episodes 17573\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33380000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.790000\n",
      "episodes 17579\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33390000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.790000\n",
      "episodes 17584\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33400000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.790000\n",
      "episodes 17589\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33410000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 17595\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33420000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 17600\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33430000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.790000\n",
      "episodes 17605\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33440000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.790000\n",
      "episodes 17611\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33450000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.790000\n",
      "episodes 17616\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33460000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 17621\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 33470000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 17627\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33480000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.790000\n",
      "episodes 17632\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33490000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 17637\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33500000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 17642\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33510000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 17648\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33520000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 17653\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33530000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 17659\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33540000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 17664\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33550000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.790000\n",
      "episodes 17669\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33560000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 17674\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33570000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 17680\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33580000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.790000\n",
      "episodes 17685\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33590000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 17690\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33600000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 17696\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33610000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 17701\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33620000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.790000\n",
      "episodes 17706\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33630000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 17711\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33640000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 17716\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33650000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 17721\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33660000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 17727\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33670000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 17732\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33680000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 17737\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33690000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 17743\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33700000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 17748\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33710000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 17753\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33720000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 17759\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33730000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 17764\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33740000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 17770\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33750000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 17775\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33760000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 17780\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33770000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 17786\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33780000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 17791\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33790000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 17796\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33800000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 17801\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33810000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 17806\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33820000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 17812\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33830000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 17817\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33840000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 17823\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33850000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 17828\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33860000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 17834\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33870000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 17839\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33880000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 17844\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33890000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 17849\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33900000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 17855\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33910000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 17860\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33920000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 17865\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33930000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 17871\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33940000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 17876\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33950000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 17881\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33960000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 17886\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33970000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 17892\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33980000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 17897\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 33990000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 17903\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34000000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 17908\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34010000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 17913\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34020000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 17918\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34030000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 17923\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34040000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.790000\n",
      "episodes 17928\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34050000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 17934\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 34060000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.790000\n",
      "episodes 17939\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34070000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.790000\n",
      "episodes 17944\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34080000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.790000\n",
      "episodes 17949\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34090000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 17955\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34100000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.790000\n",
      "episodes 17960\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34110000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 17965\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34120000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.790000\n",
      "episodes 17970\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34130000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.790000\n",
      "episodes 17975\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34140000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.790000\n",
      "episodes 17981\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34150000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 17986\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34160000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.790000\n",
      "episodes 17991\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34170000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.790000\n",
      "episodes 17996\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 10:31:20,669] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video018000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 34180000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.790000\n",
      "episodes 18001\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34190000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.790000\n",
      "episodes 18006\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34200000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.790000\n",
      "episodes 18011\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34210000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 18017\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34220000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 18022\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34230000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 18027\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34240000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 18033\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34250000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 18038\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34260000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 18043\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34270000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.790000\n",
      "episodes 18048\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34280000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.790000\n",
      "episodes 18053\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34290000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 18059\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34300000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 18065\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34310000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 18070\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34320000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 18075\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34330000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 18080\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34340000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 18086\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34350000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 18091\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34360000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 18096\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34370000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 18102\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34380000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 18107\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34390000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 18112\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34400000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 18117\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34410000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 18123\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34420000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 18128\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34430000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 18133\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34440000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 18138\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34450000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 18144\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34460000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 18149\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34470000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 18154\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34480000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 18160\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34490000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 18165\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34500000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 18170\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34510000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 18176\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34520000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 18181\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34530000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 18186\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34540000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 18191\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34550000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 18197\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34560000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 18202\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34570000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 18207\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34580000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 18213\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34590000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 18218\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34600000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 18223\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34610000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 18229\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34620000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 18234\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34630000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 18239\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34640000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 18244\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34650000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 18249\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34660000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 18254\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34670000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 18260\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34680000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 18265\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34690000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 18270\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34700000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 18276\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34710000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 18281\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34720000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 18286\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34730000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 18291\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34740000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 18297\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34750000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 18302\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34760000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 18307\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 34770000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 18312\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34780000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 18318\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34790000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 18323\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34800000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 18328\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34810000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 18334\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34820000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 18339\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34830000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 18344\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34840000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 18350\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34850000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 18355\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34860000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 18360\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34870000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 18366\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34880000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 18371\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34890000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 18376\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34900000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 18382\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34910000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 18387\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34920000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 18393\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34930000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 18398\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34940000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 18403\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34950000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 18409\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34960000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 18414\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34970000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 18419\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34980000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 18425\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 34990000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 18430\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35000000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 18436\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35010000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.790000\n",
      "episodes 18441\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35020000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 18446\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35030000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 18451\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35040000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 18457\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35050000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 18462\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35060000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 18467\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35070000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 18472\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35080000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 18477\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35090000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 18483\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35100000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 18488\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35110000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 18493\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35120000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 18499\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35130000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 18504\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35140000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 18509\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35150000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 18515\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35160000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 18520\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35170000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 18525\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35180000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 18531\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35190000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 18536\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35200000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 18542\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35210000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 18547\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35220000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 18552\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35230000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 18558\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35240000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 18563\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35250000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 18568\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35260000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 18573\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35270000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 18578\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35280000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 18584\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35290000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 18589\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35300000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 18595\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35310000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 18600\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35320000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 18605\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35330000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 18611\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35340000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 18617\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35350000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 18622\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 35360000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 18627\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35370000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 18633\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35380000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 18638\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35390000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 18643\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35400000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 18649\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35410000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 18654\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35420000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 18659\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35430000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 18664\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35440000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 18670\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35450000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 18675\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35460000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 18681\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35470000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 18686\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35480000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 18691\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35490000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 18696\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35500000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 18702\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35510000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 18707\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35520000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 18713\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35530000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 18718\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35540000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 18723\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35550000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 18728\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35560000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 18734\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35570000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 18739\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35580000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 18744\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35590000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 18750\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35600000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 18755\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35610000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 18760\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35620000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 18765\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35630000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 18771\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35640000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 18776\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35650000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 18782\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35660000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 18787\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35670000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 18792\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35680000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 18798\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35690000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 18803\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35700000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 18808\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35710000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 18813\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35720000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 18819\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35730000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 18824\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35740000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 18829\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35750000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 18835\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35760000\n",
      "mean reward (100 episodes) 18.470000\n",
      "best mean reward 18.790000\n",
      "episodes 18840\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35770000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.790000\n",
      "episodes 18846\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35780000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 18851\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35790000\n",
      "mean reward (100 episodes) 18.520000\n",
      "best mean reward 18.790000\n",
      "episodes 18856\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35800000\n",
      "mean reward (100 episodes) 18.460000\n",
      "best mean reward 18.790000\n",
      "episodes 18861\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35810000\n",
      "mean reward (100 episodes) 18.450000\n",
      "best mean reward 18.790000\n",
      "episodes 18867\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35820000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 18872\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35830000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.790000\n",
      "episodes 18877\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35840000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 18882\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35850000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.790000\n",
      "episodes 18888\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35860000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.790000\n",
      "episodes 18893\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35870000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 18899\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35880000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 18904\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35890000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 18909\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35900000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 18914\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35910000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 18919\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35920000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 18924\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35930000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 18929\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35940000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 18934\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 35950000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 18939\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35960000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 18945\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35970000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 18950\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35980000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 18956\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 35990000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 18961\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36000000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 18966\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36010000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 18971\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36020000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 18977\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36030000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 18982\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36040000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 18987\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36050000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 18992\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36060000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 18997\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 12:18:03,397] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video019000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 36070000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 19002\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36080000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 19008\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36090000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 19013\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36100000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 19019\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36110000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 19024\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36120000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 19029\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36130000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 19034\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36140000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 19040\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36150000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 19045\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36160000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 19051\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36170000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 19056\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36180000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 19061\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36190000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 19066\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36200000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 19072\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36210000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 19077\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36220000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 19082\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36230000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 19087\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36240000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 19093\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36250000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 19098\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36260000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 19103\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36270000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 19108\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36280000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 19114\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36290000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 19119\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36300000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 19124\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36310000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 19129\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36320000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 19135\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36330000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 19140\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36340000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 19145\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36350000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 19151\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36360000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 19156\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36370000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 19162\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36380000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 19167\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36390000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 19173\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36400000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 19178\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36410000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 19183\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36420000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 19189\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36430000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 19194\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36440000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 19199\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36450000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 19205\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36460000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 19210\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36470000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 19215\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36480000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.790000\n",
      "episodes 19220\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36490000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.790000\n",
      "episodes 19226\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36500000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.790000\n",
      "episodes 19231\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36510000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 19236\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36520000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 19241\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36530000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 19246\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36540000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 19251\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36550000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 19257\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36560000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 19262\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36570000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 19267\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36580000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 19273\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36590000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 19278\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36600000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 19283\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36610000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 19288\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36620000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 19294\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36630000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 19299\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36640000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 19304\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36650000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 19309\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 36660000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 19315\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36670000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 19320\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36680000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 19325\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36690000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 19330\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36700000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 19336\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36710000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 19341\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36720000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 19347\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36730000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 19352\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36740000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 19357\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36750000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 19363\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36760000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 19369\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36770000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 19374\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36780000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 19379\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36790000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 19385\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36800000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 19390\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36810000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 19395\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36820000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 19401\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36830000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 19406\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36840000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 19411\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36850000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 19416\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36860000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 19421\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36870000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 19427\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36880000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 19432\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36890000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 19437\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36900000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 19443\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36910000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 19448\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36920000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 19453\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36930000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 19459\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36940000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 19464\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36950000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 19469\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36960000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 19475\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36970000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 19480\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36980000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 19486\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 36990000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 19491\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37000000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 19496\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37010000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 19502\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37020000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 19507\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37030000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.790000\n",
      "episodes 19512\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37040000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 19517\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37050000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 19522\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37060000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 19527\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37070000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 19533\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37080000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 19538\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37090000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 19543\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37100000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 19549\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37110000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 19554\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37120000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 19559\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37130000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 19564\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37140000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 19570\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37150000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 19575\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37160000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 19580\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37170000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 19585\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37180000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 19591\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37190000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 19596\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37200000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 19601\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37210000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 19606\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37220000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 19612\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37230000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 19617\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37240000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 19623\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 37250000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 19628\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37260000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 19633\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37270000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 19638\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37280000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 19644\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37290000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 19649\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37300000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 19654\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37310000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.790000\n",
      "episodes 19659\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37320000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 19664\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37330000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 19669\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37340000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.790000\n",
      "episodes 19675\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37350000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 19680\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37360000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 19686\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37370000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 19691\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37380000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 19696\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37390000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 19701\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37400000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 19707\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37410000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 19712\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37420000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 19717\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37430000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 19723\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37440000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 19728\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37450000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 19734\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37460000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 19739\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37470000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 19744\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37480000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 19750\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37490000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.790000\n",
      "episodes 19755\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37500000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 19760\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37510000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.790000\n",
      "episodes 19766\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37520000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 19771\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37530000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.790000\n",
      "episodes 19776\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37540000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.790000\n",
      "episodes 19782\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37550000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 19787\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37560000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 19793\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37570000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 19798\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37580000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.790000\n",
      "episodes 19803\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37590000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.790000\n",
      "episodes 19809\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37600000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 19814\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37610000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 19819\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37620000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 19825\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37630000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 19830\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37640000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 19835\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37650000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 19840\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37660000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 19846\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37670000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 19851\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37680000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 19856\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37690000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 19862\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37700000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 19867\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37710000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 19873\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37720000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 19878\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37730000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 19883\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37740000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 19888\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37750000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 19894\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37760000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 19899\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37770000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 19904\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37780000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 19909\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37790000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 19915\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37800000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 19920\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37810000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 19925\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37820000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 19930\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37830000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 19935\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 37840000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 19940\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37850000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 19946\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37860000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 19951\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37870000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 19956\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37880000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 19961\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37890000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.790000\n",
      "episodes 19967\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37900000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.790000\n",
      "episodes 19971\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37910000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.790000\n",
      "episodes 19977\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37920000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.790000\n",
      "episodes 19982\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37930000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.790000\n",
      "episodes 19988\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37940000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.790000\n",
      "episodes 19993\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37950000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 19998\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 14:05:03,122] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video020000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 37960000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 20004\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37970000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 20009\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37980000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.790000\n",
      "episodes 20014\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 37990000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 20020\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38000000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 20025\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38010000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 20030\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38020000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 20036\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38030000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 20041\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38040000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 20047\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38050000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 20051\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38060000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 20056\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38070000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 20062\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38080000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 20067\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38090000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 20072\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38100000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 20078\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38110000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 20083\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38120000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 20089\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38130000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 20094\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38140000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 20100\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38150000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 20105\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38160000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.790000\n",
      "episodes 20110\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38170000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 20115\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38180000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 20120\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38190000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 20126\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38200000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 20131\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38210000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 20136\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38220000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 20142\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38230000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 20147\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38240000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 20152\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38250000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 20158\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38260000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 20163\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38270000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 20169\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38280000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 20174\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38290000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.790000\n",
      "episodes 20180\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38300000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 20186\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38310000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 20191\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38320000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 20196\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38330000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 20202\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38340000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 20207\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38350000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 20212\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38360000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 20218\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38370000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 20223\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38380000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.790000\n",
      "episodes 20229\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38390000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 20234\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38400000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 20239\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38410000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 20244\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38420000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 20250\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38430000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 20255\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38440000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 20260\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38450000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 20266\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38460000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.790000\n",
      "episodes 20271\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38470000\n",
      "mean reward (100 episodes) 18.420000\n",
      "best mean reward 18.790000\n",
      "episodes 20276\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38480000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 20282\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38490000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 20287\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38500000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 20292\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38510000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 20298\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38520000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 20303\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38530000\n",
      "mean reward (100 episodes) 18.460000\n",
      "best mean reward 18.790000\n",
      "episodes 20309\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38540000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 20314\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 38550000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 20319\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38560000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 20324\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38570000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 20329\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38580000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 20335\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38590000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 20340\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38600000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 20345\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38610000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 20350\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38620000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 20356\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38630000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 20361\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38640000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 20367\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38650000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 20372\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38660000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 20378\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38670000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 20384\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38680000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 20389\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38690000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.790000\n",
      "episodes 20394\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38700000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 20400\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38710000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 20405\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38720000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.790000\n",
      "episodes 20411\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38730000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.790000\n",
      "episodes 20416\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38740000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.790000\n",
      "episodes 20421\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38750000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 20427\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38760000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 20432\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38770000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 20437\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38780000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 20442\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38790000\n",
      "mean reward (100 episodes) 18.310000\n",
      "best mean reward 18.790000\n",
      "episodes 20447\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38800000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 20452\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38810000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 20457\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38820000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 20462\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38830000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 20468\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38840000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 20473\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38850000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 20478\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38860000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 20483\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38870000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 20489\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38880000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 20494\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38890000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 20499\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38900000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.790000\n",
      "episodes 20504\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38910000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 20509\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38920000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.790000\n",
      "episodes 20514\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38930000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 20520\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38940000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 20525\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38950000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 20530\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38960000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 20535\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38970000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 20541\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38980000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 20546\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 38990000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 20551\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39000000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 20556\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39010000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 20561\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39020000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 20567\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39030000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 20572\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39040000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 20577\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39050000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 20583\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39060000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 20588\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39070000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 20593\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39080000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.790000\n",
      "episodes 20598\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39090000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 20604\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39100000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 20609\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39110000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 20614\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39120000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 20620\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39130000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 20625\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 39140000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 20630\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39150000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 20635\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39160000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.790000\n",
      "episodes 20641\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39170000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 20646\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39180000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 20651\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39190000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 20657\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39200000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.790000\n",
      "episodes 20662\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39210000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 20667\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39220000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 20672\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39230000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 20678\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39240000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 20683\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39250000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 20688\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39260000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 20693\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39270000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 20699\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39280000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 20704\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39290000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 20709\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39300000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 20714\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39310000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 20720\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39320000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 20725\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39330000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 20730\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39340000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 20735\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39350000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 20740\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39360000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 20745\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39370000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 20751\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39380000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 20756\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39390000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 20762\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39400000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 20767\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39410000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 20772\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39420000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 20778\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39430000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 20783\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39440000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 20788\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39450000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.790000\n",
      "episodes 20793\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39460000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 20799\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39470000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 20804\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39480000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 20809\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39490000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 20815\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39500000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 20820\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39510000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 20825\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39520000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 20830\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39530000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 20835\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39540000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 20841\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39550000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 20846\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39560000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 20851\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39570000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 20856\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39580000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 20861\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39590000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 20867\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39600000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.790000\n",
      "episodes 20872\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39610000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.790000\n",
      "episodes 20877\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39620000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 20882\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39630000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 20888\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39640000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 20893\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39650000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 20899\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39660000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 20904\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39670000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 20909\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39680000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 20915\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39690000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 20920\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39700000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 20925\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39710000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 20930\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39720000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 20936\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 39730000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 20941\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39740000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 20946\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39750000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 20952\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39760000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 20957\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39770000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 20962\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39780000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 20968\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39790000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 20973\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39800000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 20978\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39810000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 20984\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39820000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 20989\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39830000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 20994\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39840000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 20999\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 15:51:19,289] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video021000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 39850000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 21004\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39860000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 21010\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39870000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 21015\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39880000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 21020\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39890000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 21026\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39900000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 21031\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39910000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 21036\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39920000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 21042\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39930000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 21047\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39940000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 21052\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39950000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 21058\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39960000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 21063\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39970000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 21068\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39980000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 21073\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 39990000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 21079\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40000000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 21084\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40010000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 21090\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40020000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 21095\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40030000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 21100\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40040000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 21106\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40050000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 21111\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40060000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 21116\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40070000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 21121\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40080000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 21126\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40090000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 21132\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40100000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 21137\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40110000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 21142\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40120000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 21148\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40130000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 21153\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40140000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 21158\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40150000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 21164\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40160000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 21169\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40170000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 21175\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40180000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 21180\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40190000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 21185\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40200000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 21191\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40210000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 21196\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40220000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 21202\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40230000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 21207\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40240000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 21212\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40250000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 21217\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40260000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 21222\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40270000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 21228\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40280000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.790000\n",
      "episodes 21233\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40290000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 21239\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40300000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 21244\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40310000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 21249\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40320000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 21254\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40330000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 21260\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40340000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 21265\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40350000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 21270\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40360000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 21276\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40370000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 21281\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40380000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 21286\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40390000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 21292\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40400000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 21297\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40410000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 21303\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40420000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 21308\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40430000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 21313\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 40440000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.790000\n",
      "episodes 21319\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40450000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 21324\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40460000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.790000\n",
      "episodes 21330\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40470000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 21335\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40480000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 21340\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40490000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 21345\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40500000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 21350\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40510000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 21355\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40520000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 21360\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40530000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 21365\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40540000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 21371\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40550000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 21376\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40560000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 21381\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40570000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 21386\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40580000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 21391\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40590000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 21397\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40600000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.790000\n",
      "episodes 21402\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40610000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.790000\n",
      "episodes 21407\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40620000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 21412\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40630000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 21418\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40640000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 21423\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40650000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.790000\n",
      "episodes 21428\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40660000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 21433\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40670000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 21438\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40680000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 21444\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40690000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.790000\n",
      "episodes 21449\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40700000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 21454\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40710000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 21459\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40720000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.790000\n",
      "episodes 21464\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40730000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 21470\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40740000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.790000\n",
      "episodes 21475\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40750000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.790000\n",
      "episodes 21480\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40760000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.790000\n",
      "episodes 21485\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40770000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.790000\n",
      "episodes 21490\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40780000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 21496\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40790000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.790000\n",
      "episodes 21501\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40800000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.790000\n",
      "episodes 21506\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40810000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.790000\n",
      "episodes 21512\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40820000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.790000\n",
      "episodes 21517\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40830000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.790000\n",
      "episodes 21522\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40840000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.790000\n",
      "episodes 21528\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40850000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.790000\n",
      "episodes 21533\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40860000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.790000\n",
      "episodes 21539\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40870000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.790000\n",
      "episodes 21544\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40880000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.790000\n",
      "episodes 21549\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40890000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 21555\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40900000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 21560\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40910000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 21566\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40920000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 21571\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40930000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 21577\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40940000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 21582\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40950000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 21587\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40960000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 21592\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40970000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 21597\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40980000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 21603\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 40990000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 21608\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41000000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.790000\n",
      "episodes 21613\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41010000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 21619\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41020000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 21624\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 41030000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 21629\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41040000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 21634\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41050000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 21639\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41060000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 21645\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41070000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 21650\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41080000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 21655\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41090000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 21661\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41100000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 21665\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41110000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.790000\n",
      "episodes 21670\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41120000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.790000\n",
      "episodes 21676\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41130000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 21681\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41140000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.790000\n",
      "episodes 21687\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41150000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 21692\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41160000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.790000\n",
      "episodes 21697\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41170000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 21702\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41180000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 21708\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41190000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 21713\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41200000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 21719\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41210000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 21724\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41220000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 21729\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41230000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 21734\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41240000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 21740\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41250000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 21745\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41260000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 21750\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41270000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 21755\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41280000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 21761\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41290000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 21766\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41300000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.790000\n",
      "episodes 21772\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41310000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 21777\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41320000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 21782\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41330000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 21787\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41340000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 21792\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41350000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.790000\n",
      "episodes 21798\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41360000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 21803\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41370000\n",
      "mean reward (100 episodes) 17.980000\n",
      "best mean reward 18.790000\n",
      "episodes 21808\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41380000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 21813\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41390000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 21819\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41400000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 21824\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41410000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.790000\n",
      "episodes 21829\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41420000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 21834\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41430000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 21840\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41440000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 21845\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41450000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 21851\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41460000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 21856\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41470000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.790000\n",
      "episodes 21862\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41480000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 21867\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41490000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.790000\n",
      "episodes 21872\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41500000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 21878\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41510000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 21883\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41520000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 21888\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41530000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 21894\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41540000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 21899\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41550000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 21904\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41560000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 21910\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41570000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 21915\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41580000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 21920\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41590000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 21926\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41600000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 21931\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41610000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.790000\n",
      "episodes 21936\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 41620000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.790000\n",
      "episodes 21941\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41630000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 21947\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41640000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.790000\n",
      "episodes 21952\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41650000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 21957\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41660000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.790000\n",
      "episodes 21962\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41670000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.790000\n",
      "episodes 21967\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41680000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.790000\n",
      "episodes 21973\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41690000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 21979\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41700000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.790000\n",
      "episodes 21984\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41710000\n",
      "mean reward (100 episodes) 17.870000\n",
      "best mean reward 18.790000\n",
      "episodes 21989\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41720000\n",
      "mean reward (100 episodes) 17.930000\n",
      "best mean reward 18.790000\n",
      "episodes 21994\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41730000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 21999\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 17:37:48,647] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video022000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 41740000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 22004\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41750000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.790000\n",
      "episodes 22010\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41760000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.790000\n",
      "episodes 22015\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41770000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.790000\n",
      "episodes 22020\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41780000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 22025\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41790000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.790000\n",
      "episodes 22031\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41800000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.790000\n",
      "episodes 22036\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41810000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.790000\n",
      "episodes 22041\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41820000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.790000\n",
      "episodes 22047\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41830000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 22052\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41840000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.790000\n",
      "episodes 22057\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41850000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.790000\n",
      "episodes 22063\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41860000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.790000\n",
      "episodes 22068\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41870000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.790000\n",
      "episodes 22074\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41880000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.790000\n",
      "episodes 22079\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41890000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.790000\n",
      "episodes 22084\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41900000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.790000\n",
      "episodes 22090\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41910000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.790000\n",
      "episodes 22095\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41920000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.790000\n",
      "episodes 22100\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41930000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.790000\n",
      "episodes 22105\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41940000\n",
      "mean reward (100 episodes) 18.170000\n",
      "best mean reward 18.790000\n",
      "episodes 22111\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41950000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.790000\n",
      "episodes 22116\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41960000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 22122\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41970000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 22127\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41980000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 22133\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 41990000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.790000\n",
      "episodes 22138\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42000000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 22143\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42010000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.790000\n",
      "episodes 22148\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42020000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 22154\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42030000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.790000\n",
      "episodes 22159\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42040000\n",
      "mean reward (100 episodes) 18.190000\n",
      "best mean reward 18.790000\n",
      "episodes 22164\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42050000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 22170\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42060000\n",
      "mean reward (100 episodes) 18.260000\n",
      "best mean reward 18.790000\n",
      "episodes 22175\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42070000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.790000\n",
      "episodes 22181\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42080000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.790000\n",
      "episodes 22187\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42090000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.790000\n",
      "episodes 22192\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42100000\n",
      "mean reward (100 episodes) 18.510000\n",
      "best mean reward 18.790000\n",
      "episodes 22197\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42110000\n",
      "mean reward (100 episodes) 18.520000\n",
      "best mean reward 18.790000\n",
      "episodes 22203\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42120000\n",
      "mean reward (100 episodes) 18.560000\n",
      "best mean reward 18.790000\n",
      "episodes 22208\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42130000\n",
      "mean reward (100 episodes) 18.640000\n",
      "best mean reward 18.790000\n",
      "episodes 22214\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42140000\n",
      "mean reward (100 episodes) 18.580000\n",
      "best mean reward 18.790000\n",
      "episodes 22219\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42150000\n",
      "mean reward (100 episodes) 18.530000\n",
      "best mean reward 18.790000\n",
      "episodes 22224\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42160000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.790000\n",
      "episodes 22230\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42170000\n",
      "mean reward (100 episodes) 18.570000\n",
      "best mean reward 18.790000\n",
      "episodes 22235\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42180000\n",
      "mean reward (100 episodes) 18.600000\n",
      "best mean reward 18.790000\n",
      "episodes 22241\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42190000\n",
      "mean reward (100 episodes) 18.620000\n",
      "best mean reward 18.790000\n",
      "episodes 22246\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42200000\n",
      "mean reward (100 episodes) 18.680000\n",
      "best mean reward 18.790000\n",
      "episodes 22252\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42210000\n",
      "mean reward (100 episodes) 18.700000\n",
      "best mean reward 18.790000\n",
      "episodes 22257\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42220000\n",
      "mean reward (100 episodes) 18.580000\n",
      "best mean reward 18.790000\n",
      "episodes 22262\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42230000\n",
      "mean reward (100 episodes) 18.510000\n",
      "best mean reward 18.790000\n",
      "episodes 22267\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42240000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.790000\n",
      "episodes 22272\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42250000\n",
      "mean reward (100 episodes) 18.370000\n",
      "best mean reward 18.790000\n",
      "episodes 22277\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42260000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.790000\n",
      "episodes 22283\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42270000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 22288\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42280000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.790000\n",
      "episodes 22294\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42290000\n",
      "mean reward (100 episodes) 18.250000\n",
      "best mean reward 18.790000\n",
      "episodes 22299\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42300000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.790000\n",
      "episodes 22304\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42310000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.790000\n",
      "episodes 22310\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42320000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.790000\n",
      "episodes 22315\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 42330000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.790000\n",
      "episodes 22321\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42340000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.790000\n",
      "episodes 22327\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42350000\n",
      "mean reward (100 episodes) 18.490000\n",
      "best mean reward 18.790000\n",
      "episodes 22332\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42360000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.790000\n",
      "episodes 22338\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42370000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.790000\n",
      "episodes 22343\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42380000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.790000\n",
      "episodes 22348\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42390000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.790000\n",
      "episodes 22354\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42400000\n",
      "mean reward (100 episodes) 18.570000\n",
      "best mean reward 18.790000\n",
      "episodes 22359\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42410000\n",
      "mean reward (100 episodes) 18.620000\n",
      "best mean reward 18.790000\n",
      "episodes 22364\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42420000\n",
      "mean reward (100 episodes) 18.770000\n",
      "best mean reward 18.790000\n",
      "episodes 22370\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42430000\n",
      "mean reward (100 episodes) 18.780000\n",
      "best mean reward 18.790000\n",
      "episodes 22375\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42440000\n",
      "mean reward (100 episodes) 18.780000\n",
      "best mean reward 18.810000\n",
      "episodes 22381\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42450000\n",
      "mean reward (100 episodes) 18.750000\n",
      "best mean reward 18.810000\n",
      "episodes 22386\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42460000\n",
      "mean reward (100 episodes) 18.710000\n",
      "best mean reward 18.810000\n",
      "episodes 22391\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42470000\n",
      "mean reward (100 episodes) 18.690000\n",
      "best mean reward 18.810000\n",
      "episodes 22396\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42480000\n",
      "mean reward (100 episodes) 18.730000\n",
      "best mean reward 18.810000\n",
      "episodes 22402\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42490000\n",
      "mean reward (100 episodes) 18.700000\n",
      "best mean reward 18.810000\n",
      "episodes 22407\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42500000\n",
      "mean reward (100 episodes) 18.630000\n",
      "best mean reward 18.810000\n",
      "episodes 22412\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42510000\n",
      "mean reward (100 episodes) 18.640000\n",
      "best mean reward 18.810000\n",
      "episodes 22418\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42520000\n",
      "mean reward (100 episodes) 18.630000\n",
      "best mean reward 18.810000\n",
      "episodes 22423\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42530000\n",
      "mean reward (100 episodes) 18.610000\n",
      "best mean reward 18.810000\n",
      "episodes 22429\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42540000\n",
      "mean reward (100 episodes) 18.610000\n",
      "best mean reward 18.810000\n",
      "episodes 22434\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42550000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.810000\n",
      "episodes 22439\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42560000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.810000\n",
      "episodes 22444\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42570000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.810000\n",
      "episodes 22449\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42580000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.810000\n",
      "episodes 22455\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42590000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.810000\n",
      "episodes 22460\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42600000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.810000\n",
      "episodes 22465\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42610000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.810000\n",
      "episodes 22471\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42620000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.810000\n",
      "episodes 22476\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42630000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.810000\n",
      "episodes 22481\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42640000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.810000\n",
      "episodes 22487\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42650000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.810000\n",
      "episodes 22492\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42660000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.810000\n",
      "episodes 22497\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42670000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.810000\n",
      "episodes 22502\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42680000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.810000\n",
      "episodes 22508\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42690000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.810000\n",
      "episodes 22513\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42700000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.810000\n",
      "episodes 22518\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42710000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.810000\n",
      "episodes 22523\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42720000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.810000\n",
      "episodes 22528\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42730000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.810000\n",
      "episodes 22534\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42740000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.810000\n",
      "episodes 22539\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42750000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.810000\n",
      "episodes 22544\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42760000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.810000\n",
      "episodes 22550\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42770000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.810000\n",
      "episodes 22555\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42780000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.810000\n",
      "episodes 22561\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42790000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.810000\n",
      "episodes 22566\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42800000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.810000\n",
      "episodes 22572\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42810000\n",
      "mean reward (100 episodes) 18.390000\n",
      "best mean reward 18.810000\n",
      "episodes 22577\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42820000\n",
      "mean reward (100 episodes) 18.410000\n",
      "best mean reward 18.810000\n",
      "episodes 22582\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42830000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.810000\n",
      "episodes 22587\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42840000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.810000\n",
      "episodes 22593\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42850000\n",
      "mean reward (100 episodes) 18.480000\n",
      "best mean reward 18.810000\n",
      "episodes 22598\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42860000\n",
      "mean reward (100 episodes) 18.500000\n",
      "best mean reward 18.810000\n",
      "episodes 22604\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42870000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.810000\n",
      "episodes 22609\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42880000\n",
      "mean reward (100 episodes) 18.520000\n",
      "best mean reward 18.810000\n",
      "episodes 22614\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42890000\n",
      "mean reward (100 episodes) 18.490000\n",
      "best mean reward 18.810000\n",
      "episodes 22619\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42900000\n",
      "mean reward (100 episodes) 18.520000\n",
      "best mean reward 18.810000\n",
      "episodes 22625\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42910000\n",
      "mean reward (100 episodes) 18.630000\n",
      "best mean reward 18.810000\n",
      "episodes 22630\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 42920000\n",
      "mean reward (100 episodes) 18.580000\n",
      "best mean reward 18.810000\n",
      "episodes 22636\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42930000\n",
      "mean reward (100 episodes) 18.430000\n",
      "best mean reward 18.810000\n",
      "episodes 22641\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42940000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.810000\n",
      "episodes 22646\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42950000\n",
      "mean reward (100 episodes) 18.590000\n",
      "best mean reward 18.810000\n",
      "episodes 22652\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42960000\n",
      "mean reward (100 episodes) 18.580000\n",
      "best mean reward 18.810000\n",
      "episodes 22657\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42970000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.810000\n",
      "episodes 22663\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42980000\n",
      "mean reward (100 episodes) 18.460000\n",
      "best mean reward 18.810000\n",
      "episodes 22668\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 42990000\n",
      "mean reward (100 episodes) 18.530000\n",
      "best mean reward 18.810000\n",
      "episodes 22673\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43000000\n",
      "mean reward (100 episodes) 18.530000\n",
      "best mean reward 18.810000\n",
      "episodes 22679\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43010000\n",
      "mean reward (100 episodes) 18.560000\n",
      "best mean reward 18.810000\n",
      "episodes 22685\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43020000\n",
      "mean reward (100 episodes) 18.580000\n",
      "best mean reward 18.810000\n",
      "episodes 22690\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43030000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.810000\n",
      "episodes 22695\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43040000\n",
      "mean reward (100 episodes) 18.620000\n",
      "best mean reward 18.810000\n",
      "episodes 22701\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43050000\n",
      "mean reward (100 episodes) 18.540000\n",
      "best mean reward 18.810000\n",
      "episodes 22706\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43060000\n",
      "mean reward (100 episodes) 18.580000\n",
      "best mean reward 18.810000\n",
      "episodes 22711\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43070000\n",
      "mean reward (100 episodes) 18.630000\n",
      "best mean reward 18.810000\n",
      "episodes 22717\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43080000\n",
      "mean reward (100 episodes) 18.570000\n",
      "best mean reward 18.810000\n",
      "episodes 22722\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43090000\n",
      "mean reward (100 episodes) 18.590000\n",
      "best mean reward 18.810000\n",
      "episodes 22727\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43100000\n",
      "mean reward (100 episodes) 18.470000\n",
      "best mean reward 18.810000\n",
      "episodes 22732\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43110000\n",
      "mean reward (100 episodes) 18.440000\n",
      "best mean reward 18.810000\n",
      "episodes 22737\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43120000\n",
      "mean reward (100 episodes) 18.400000\n",
      "best mean reward 18.810000\n",
      "episodes 22742\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43130000\n",
      "mean reward (100 episodes) 18.360000\n",
      "best mean reward 18.810000\n",
      "episodes 22748\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43140000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.810000\n",
      "episodes 22753\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43150000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.810000\n",
      "episodes 22758\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43160000\n",
      "mean reward (100 episodes) 18.080000\n",
      "best mean reward 18.810000\n",
      "episodes 22763\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43170000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.810000\n",
      "episodes 22768\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43180000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.810000\n",
      "episodes 22773\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43190000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.810000\n",
      "episodes 22778\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43200000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.810000\n",
      "episodes 22784\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43210000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.810000\n",
      "episodes 22789\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43220000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.810000\n",
      "episodes 22794\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43230000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.810000\n",
      "episodes 22799\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43240000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.810000\n",
      "episodes 22804\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43250000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 22810\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43260000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 22815\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43270000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 22820\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43280000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 18.810000\n",
      "episodes 22825\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43290000\n",
      "mean reward (100 episodes) 17.410000\n",
      "best mean reward 18.810000\n",
      "episodes 22830\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43300000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 22835\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43310000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 22841\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43320000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 22846\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43330000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.810000\n",
      "episodes 22852\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43340000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 22857\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43350000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 22862\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43360000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 22867\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43370000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 22872\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43380000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 22877\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43390000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 22882\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43400000\n",
      "mean reward (100 episodes) 17.400000\n",
      "best mean reward 18.810000\n",
      "episodes 22887\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43410000\n",
      "mean reward (100 episodes) 17.370000\n",
      "best mean reward 18.810000\n",
      "episodes 22892\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43420000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 22897\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43430000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 22903\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43440000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 22908\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43450000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 22913\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43460000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 22918\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43470000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 22924\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43480000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 22929\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43490000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 22934\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43500000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 22940\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 43510000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 22944\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43520000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.810000\n",
      "episodes 22950\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43530000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 22954\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43540000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 22960\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43550000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 22966\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43560000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.810000\n",
      "episodes 22971\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43570000\n",
      "mean reward (100 episodes) 17.820000\n",
      "best mean reward 18.810000\n",
      "episodes 22976\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43580000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.810000\n",
      "episodes 22982\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43590000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.810000\n",
      "episodes 22987\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43600000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.810000\n",
      "episodes 22992\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43610000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 22997\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 19:24:01,403] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video023000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 43620000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.810000\n",
      "episodes 23003\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43630000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 23008\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43640000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 23013\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43650000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 23018\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43660000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 23024\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43670000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 23029\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43680000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 23034\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43690000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 23039\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43700000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 18.810000\n",
      "episodes 23045\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43710000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 18.810000\n",
      "episodes 23050\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43720000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 23055\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43730000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 23061\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43740000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.810000\n",
      "episodes 23066\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43750000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.810000\n",
      "episodes 23071\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43760000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 18.810000\n",
      "episodes 23076\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43770000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 23081\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43780000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 18.810000\n",
      "episodes 23086\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43790000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 23092\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43800000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 23097\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43810000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 23102\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43820000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.810000\n",
      "episodes 23107\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43830000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 23112\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43840000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 23117\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43850000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 23123\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43860000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 23128\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43870000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.810000\n",
      "episodes 23133\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43880000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 23138\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43890000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.810000\n",
      "episodes 23144\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43900000\n",
      "mean reward (100 episodes) 17.960000\n",
      "best mean reward 18.810000\n",
      "episodes 23149\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43910000\n",
      "mean reward (100 episodes) 17.950000\n",
      "best mean reward 18.810000\n",
      "episodes 23154\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43920000\n",
      "mean reward (100 episodes) 17.850000\n",
      "best mean reward 18.810000\n",
      "episodes 23160\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43930000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.810000\n",
      "episodes 23164\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43940000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.810000\n",
      "episodes 23169\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43950000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 23175\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43960000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 23180\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43970000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 23185\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43980000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 23190\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 43990000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 23195\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44000000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.810000\n",
      "episodes 23201\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44010000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 23206\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44020000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 23211\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44030000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 23217\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44040000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 23222\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44050000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 18.810000\n",
      "episodes 23227\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44060000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 23232\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44070000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 23237\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44080000\n",
      "mean reward (100 episodes) 17.410000\n",
      "best mean reward 18.810000\n",
      "episodes 23243\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44090000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 23248\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44100000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 23253\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44110000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 23259\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44120000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 18.810000\n",
      "episodes 23264\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44130000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 23269\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44140000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.810000\n",
      "episodes 23274\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44150000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 23279\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44160000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.810000\n",
      "episodes 23285\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44170000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 23290\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44180000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 23296\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44190000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 23301\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44200000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 23307\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 44210000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 23312\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44220000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 23317\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44230000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 23322\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44240000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.810000\n",
      "episodes 23328\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44250000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.810000\n",
      "episodes 23333\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44260000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.810000\n",
      "episodes 23338\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44270000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 23343\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44280000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 23349\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44290000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 23354\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44300000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 23359\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44310000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 23365\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44320000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.810000\n",
      "episodes 23370\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44330000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.810000\n",
      "episodes 23376\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44340000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.810000\n",
      "episodes 23381\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44350000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.810000\n",
      "episodes 23386\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44360000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 23391\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44370000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.810000\n",
      "episodes 23396\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44380000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.810000\n",
      "episodes 23402\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44390000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 23407\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44400000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.810000\n",
      "episodes 23412\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44410000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.810000\n",
      "episodes 23417\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44420000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.810000\n",
      "episodes 23423\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44430000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 18.810000\n",
      "episodes 23428\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44440000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 23433\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44450000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 23438\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44460000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 23443\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44470000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 23449\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44480000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 18.810000\n",
      "episodes 23454\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44490000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.810000\n",
      "episodes 23459\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44500000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 23464\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44510000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 23470\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44520000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 23475\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44530000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 23480\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44540000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 23486\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44550000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.810000\n",
      "episodes 23491\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44560000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.810000\n",
      "episodes 23496\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44570000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.810000\n",
      "episodes 23502\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44580000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 23507\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44590000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 23512\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44600000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 23517\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44610000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.810000\n",
      "episodes 23522\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44620000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 23527\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44630000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.810000\n",
      "episodes 23532\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44640000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.810000\n",
      "episodes 23537\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44650000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 23543\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44660000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 23548\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44670000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 23553\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44680000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 23559\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44690000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 23564\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44700000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 23569\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44710000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 23574\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44720000\n",
      "mean reward (100 episodes) 17.310000\n",
      "best mean reward 18.810000\n",
      "episodes 23579\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44730000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 18.810000\n",
      "episodes 23584\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44740000\n",
      "mean reward (100 episodes) 17.350000\n",
      "best mean reward 18.810000\n",
      "episodes 23589\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44750000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 18.810000\n",
      "episodes 23595\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44760000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 18.810000\n",
      "episodes 23600\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44770000\n",
      "mean reward (100 episodes) 17.340000\n",
      "best mean reward 18.810000\n",
      "episodes 23605\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44780000\n",
      "mean reward (100 episodes) 17.380000\n",
      "best mean reward 18.810000\n",
      "episodes 23611\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44790000\n",
      "mean reward (100 episodes) 17.350000\n",
      "best mean reward 18.810000\n",
      "episodes 23616\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 44800000\n",
      "mean reward (100 episodes) 17.330000\n",
      "best mean reward 18.810000\n",
      "episodes 23621\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44810000\n",
      "mean reward (100 episodes) 17.310000\n",
      "best mean reward 18.810000\n",
      "episodes 23626\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44820000\n",
      "mean reward (100 episodes) 17.280000\n",
      "best mean reward 18.810000\n",
      "episodes 23631\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44830000\n",
      "mean reward (100 episodes) 17.290000\n",
      "best mean reward 18.810000\n",
      "episodes 23637\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44840000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 18.810000\n",
      "episodes 23643\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44850000\n",
      "mean reward (100 episodes) 17.290000\n",
      "best mean reward 18.810000\n",
      "episodes 23647\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44860000\n",
      "mean reward (100 episodes) 17.230000\n",
      "best mean reward 18.810000\n",
      "episodes 23653\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44870000\n",
      "mean reward (100 episodes) 17.120000\n",
      "best mean reward 18.810000\n",
      "episodes 23658\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44880000\n",
      "mean reward (100 episodes) 17.130000\n",
      "best mean reward 18.810000\n",
      "episodes 23663\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44890000\n",
      "mean reward (100 episodes) 17.160000\n",
      "best mean reward 18.810000\n",
      "episodes 23668\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44900000\n",
      "mean reward (100 episodes) 17.290000\n",
      "best mean reward 18.810000\n",
      "episodes 23673\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44910000\n",
      "mean reward (100 episodes) 17.370000\n",
      "best mean reward 18.810000\n",
      "episodes 23679\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44920000\n",
      "mean reward (100 episodes) 17.380000\n",
      "best mean reward 18.810000\n",
      "episodes 23684\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44930000\n",
      "mean reward (100 episodes) 17.380000\n",
      "best mean reward 18.810000\n",
      "episodes 23689\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44940000\n",
      "mean reward (100 episodes) 17.320000\n",
      "best mean reward 18.810000\n",
      "episodes 23694\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44950000\n",
      "mean reward (100 episodes) 17.310000\n",
      "best mean reward 18.810000\n",
      "episodes 23700\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44960000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 23705\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44970000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 23711\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44980000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 23716\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 44990000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 18.810000\n",
      "episodes 23721\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45000000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 23726\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45010000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 23732\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45020000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 23737\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45030000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 23742\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45040000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.810000\n",
      "episodes 23748\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45050000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.810000\n",
      "episodes 23753\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45060000\n",
      "mean reward (100 episodes) 17.860000\n",
      "best mean reward 18.810000\n",
      "episodes 23758\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45070000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.810000\n",
      "episodes 23764\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45080000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 18.810000\n",
      "episodes 23769\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45090000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.810000\n",
      "episodes 23775\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45100000\n",
      "mean reward (100 episodes) 17.990000\n",
      "best mean reward 18.810000\n",
      "episodes 23780\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45110000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.810000\n",
      "episodes 23786\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45120000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.810000\n",
      "episodes 23791\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45130000\n",
      "mean reward (100 episodes) 18.140000\n",
      "best mean reward 18.810000\n",
      "episodes 23797\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45140000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.810000\n",
      "episodes 23802\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45150000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.810000\n",
      "episodes 23808\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45160000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.810000\n",
      "episodes 23813\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45170000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.810000\n",
      "episodes 23818\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45180000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.810000\n",
      "episodes 23823\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45190000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.810000\n",
      "episodes 23828\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45200000\n",
      "mean reward (100 episodes) 18.030000\n",
      "best mean reward 18.810000\n",
      "episodes 23834\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45210000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.810000\n",
      "episodes 23839\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45220000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.810000\n",
      "episodes 23844\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45230000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.810000\n",
      "episodes 23849\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45240000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 18.810000\n",
      "episodes 23854\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45250000\n",
      "mean reward (100 episodes) 17.890000\n",
      "best mean reward 18.810000\n",
      "episodes 23860\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45260000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.810000\n",
      "episodes 23865\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45270000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.810000\n",
      "episodes 23870\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45280000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.810000\n",
      "episodes 23875\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45290000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 23881\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45300000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.810000\n",
      "episodes 23886\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45310000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 23891\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45320000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 23897\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45330000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 23902\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45340000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 18.810000\n",
      "episodes 23907\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45350000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 23913\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45360000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 23918\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45370000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 23923\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45380000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 23928\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 45390000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 23934\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45400000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 23939\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45410000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 23944\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45420000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 23949\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45430000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 18.810000\n",
      "episodes 23954\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45440000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.810000\n",
      "episodes 23960\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45450000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.810000\n",
      "episodes 23965\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45460000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 23970\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45470000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 18.810000\n",
      "episodes 23976\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45480000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 23981\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45490000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.810000\n",
      "episodes 23986\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45500000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.810000\n",
      "episodes 23991\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45510000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 23997\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 21:10:48,934] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video024000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 45520000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.810000\n",
      "episodes 24002\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45530000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 24007\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45540000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 24013\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45550000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 24018\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45560000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 24023\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45570000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 24029\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45580000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 24034\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45590000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 24039\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45600000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 24044\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45610000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.810000\n",
      "episodes 24049\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45620000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.810000\n",
      "episodes 24055\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45630000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 24060\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45640000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 24065\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45650000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 24070\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45660000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.810000\n",
      "episodes 24076\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45670000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.810000\n",
      "episodes 24081\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45680000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 24086\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45690000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.810000\n",
      "episodes 24092\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45700000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 24097\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45710000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 24102\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45720000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.810000\n",
      "episodes 24108\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45730000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 24113\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45740000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 24118\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45750000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 24124\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45760000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 24129\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45770000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 24134\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45780000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 18.810000\n",
      "episodes 24139\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45790000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 18.810000\n",
      "episodes 24145\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45800000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 24150\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45810000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 24155\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45820000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 24160\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45830000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 24166\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45840000\n",
      "mean reward (100 episodes) 17.380000\n",
      "best mean reward 18.810000\n",
      "episodes 24171\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45850000\n",
      "mean reward (100 episodes) 17.330000\n",
      "best mean reward 18.810000\n",
      "episodes 24176\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45860000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 18.810000\n",
      "episodes 24181\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45870000\n",
      "mean reward (100 episodes) 17.220000\n",
      "best mean reward 18.810000\n",
      "episodes 24187\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45880000\n",
      "mean reward (100 episodes) 17.280000\n",
      "best mean reward 18.810000\n",
      "episodes 24192\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45890000\n",
      "mean reward (100 episodes) 17.270000\n",
      "best mean reward 18.810000\n",
      "episodes 24197\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45900000\n",
      "mean reward (100 episodes) 17.250000\n",
      "best mean reward 18.810000\n",
      "episodes 24203\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45910000\n",
      "mean reward (100 episodes) 17.230000\n",
      "best mean reward 18.810000\n",
      "episodes 24208\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45920000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 18.810000\n",
      "episodes 24213\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45930000\n",
      "mean reward (100 episodes) 17.240000\n",
      "best mean reward 18.810000\n",
      "episodes 24219\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45940000\n",
      "mean reward (100 episodes) 17.110000\n",
      "best mean reward 18.810000\n",
      "episodes 24224\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45950000\n",
      "mean reward (100 episodes) 17.120000\n",
      "best mean reward 18.810000\n",
      "episodes 24229\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45960000\n",
      "mean reward (100 episodes) 17.230000\n",
      "best mean reward 18.810000\n",
      "episodes 24234\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45970000\n",
      "mean reward (100 episodes) 17.300000\n",
      "best mean reward 18.810000\n",
      "episodes 24239\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45980000\n",
      "mean reward (100 episodes) 17.160000\n",
      "best mean reward 18.810000\n",
      "episodes 24244\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 45990000\n",
      "mean reward (100 episodes) 17.090000\n",
      "best mean reward 18.810000\n",
      "episodes 24250\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46000000\n",
      "mean reward (100 episodes) 17.060000\n",
      "best mean reward 18.810000\n",
      "episodes 24255\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46010000\n",
      "mean reward (100 episodes) 17.090000\n",
      "best mean reward 18.810000\n",
      "episodes 24260\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46020000\n",
      "mean reward (100 episodes) 17.140000\n",
      "best mean reward 18.810000\n",
      "episodes 24266\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46030000\n",
      "mean reward (100 episodes) 17.140000\n",
      "best mean reward 18.810000\n",
      "episodes 24271\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46040000\n",
      "mean reward (100 episodes) 17.140000\n",
      "best mean reward 18.810000\n",
      "episodes 24276\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46050000\n",
      "mean reward (100 episodes) 17.230000\n",
      "best mean reward 18.810000\n",
      "episodes 24281\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46060000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 18.810000\n",
      "episodes 24286\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46070000\n",
      "mean reward (100 episodes) 17.190000\n",
      "best mean reward 18.810000\n",
      "episodes 24292\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46080000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 18.810000\n",
      "episodes 24297\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46090000\n",
      "mean reward (100 episodes) 17.250000\n",
      "best mean reward 18.810000\n",
      "episodes 24302\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46100000\n",
      "mean reward (100 episodes) 17.160000\n",
      "best mean reward 18.810000\n",
      "episodes 24307\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 46110000\n",
      "mean reward (100 episodes) 17.130000\n",
      "best mean reward 18.810000\n",
      "episodes 24312\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46120000\n",
      "mean reward (100 episodes) 17.090000\n",
      "best mean reward 18.810000\n",
      "episodes 24318\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46130000\n",
      "mean reward (100 episodes) 17.190000\n",
      "best mean reward 18.810000\n",
      "episodes 24323\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46140000\n",
      "mean reward (100 episodes) 17.130000\n",
      "best mean reward 18.810000\n",
      "episodes 24328\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46150000\n",
      "mean reward (100 episodes) 17.200000\n",
      "best mean reward 18.810000\n",
      "episodes 24333\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46160000\n",
      "mean reward (100 episodes) 17.200000\n",
      "best mean reward 18.810000\n",
      "episodes 24339\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46170000\n",
      "mean reward (100 episodes) 17.270000\n",
      "best mean reward 18.810000\n",
      "episodes 24344\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46180000\n",
      "mean reward (100 episodes) 17.330000\n",
      "best mean reward 18.810000\n",
      "episodes 24349\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46190000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 18.810000\n",
      "episodes 24355\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46200000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 24360\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46210000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 24366\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46220000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 24371\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46230000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 24376\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46240000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 24382\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46250000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 24387\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46260000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 24392\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46270000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 24397\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46280000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 24403\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46290000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 24408\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46300000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 24413\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46310000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 24418\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46320000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 24424\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46330000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.810000\n",
      "episodes 24429\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46340000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.810000\n",
      "episodes 24435\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46350000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 24440\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46360000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 24445\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46370000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 24450\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46380000\n",
      "mean reward (100 episodes) 17.360000\n",
      "best mean reward 18.810000\n",
      "episodes 24455\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46390000\n",
      "mean reward (100 episodes) 17.300000\n",
      "best mean reward 18.810000\n",
      "episodes 24460\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46400000\n",
      "mean reward (100 episodes) 17.210000\n",
      "best mean reward 18.810000\n",
      "episodes 24466\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46410000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 18.810000\n",
      "episodes 24471\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46420000\n",
      "mean reward (100 episodes) 17.190000\n",
      "best mean reward 18.810000\n",
      "episodes 24477\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46430000\n",
      "mean reward (100 episodes) 17.220000\n",
      "best mean reward 18.810000\n",
      "episodes 24482\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46440000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 18.810000\n",
      "episodes 24487\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46450000\n",
      "mean reward (100 episodes) 17.410000\n",
      "best mean reward 18.810000\n",
      "episodes 24493\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46460000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 24498\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46470000\n",
      "mean reward (100 episodes) 17.400000\n",
      "best mean reward 18.810000\n",
      "episodes 24503\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46480000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 24509\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46490000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 18.810000\n",
      "episodes 24514\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46500000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 18.810000\n",
      "episodes 24520\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46510000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 24525\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46520000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 18.810000\n",
      "episodes 24530\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46530000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 18.810000\n",
      "episodes 24535\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46540000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 24541\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46550000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 24546\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46560000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 24551\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46570000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.810000\n",
      "episodes 24557\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46580000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.810000\n",
      "episodes 24562\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46590000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 24567\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46600000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 24572\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46610000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 24578\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46620000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 24583\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46630000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 24589\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46640000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 24594\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46650000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.810000\n",
      "episodes 24599\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46660000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 24605\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46670000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 18.810000\n",
      "episodes 24610\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46680000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 24615\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46690000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 24621\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 46700000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 24626\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46710000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 24631\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46720000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 24637\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46730000\n",
      "mean reward (100 episodes) 17.400000\n",
      "best mean reward 18.810000\n",
      "episodes 24642\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46740000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 24647\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46750000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 24652\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46760000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 18.810000\n",
      "episodes 24657\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46770000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 18.810000\n",
      "episodes 24663\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46780000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.810000\n",
      "episodes 24668\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46790000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 24673\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46800000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 24678\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46810000\n",
      "mean reward (100 episodes) 17.400000\n",
      "best mean reward 18.810000\n",
      "episodes 24683\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46820000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 18.810000\n",
      "episodes 24689\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46830000\n",
      "mean reward (100 episodes) 17.280000\n",
      "best mean reward 18.810000\n",
      "episodes 24694\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46840000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 18.810000\n",
      "episodes 24699\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46850000\n",
      "mean reward (100 episodes) 17.370000\n",
      "best mean reward 18.810000\n",
      "episodes 24705\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46860000\n",
      "mean reward (100 episodes) 17.290000\n",
      "best mean reward 18.810000\n",
      "episodes 24710\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46870000\n",
      "mean reward (100 episodes) 17.280000\n",
      "best mean reward 18.810000\n",
      "episodes 24714\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46880000\n",
      "mean reward (100 episodes) 17.280000\n",
      "best mean reward 18.810000\n",
      "episodes 24720\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46890000\n",
      "mean reward (100 episodes) 17.300000\n",
      "best mean reward 18.810000\n",
      "episodes 24725\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46900000\n",
      "mean reward (100 episodes) 17.270000\n",
      "best mean reward 18.810000\n",
      "episodes 24730\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46910000\n",
      "mean reward (100 episodes) 17.210000\n",
      "best mean reward 18.810000\n",
      "episodes 24735\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46920000\n",
      "mean reward (100 episodes) 17.250000\n",
      "best mean reward 18.810000\n",
      "episodes 24740\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46930000\n",
      "mean reward (100 episodes) 17.270000\n",
      "best mean reward 18.810000\n",
      "episodes 24745\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46940000\n",
      "mean reward (100 episodes) 17.180000\n",
      "best mean reward 18.810000\n",
      "episodes 24751\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46950000\n",
      "mean reward (100 episodes) 17.310000\n",
      "best mean reward 18.810000\n",
      "episodes 24756\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46960000\n",
      "mean reward (100 episodes) 17.340000\n",
      "best mean reward 18.810000\n",
      "episodes 24761\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46970000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 24767\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46980000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 24772\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 46990000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 24777\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47000000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 18.810000\n",
      "episodes 24782\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47010000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 24787\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47020000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 24793\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47030000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 24798\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47040000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 24803\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47050000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 24808\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47060000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 24814\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47070000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 24819\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47080000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 24824\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47090000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.810000\n",
      "episodes 24829\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47100000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 24834\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47110000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 18.810000\n",
      "episodes 24839\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47120000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.810000\n",
      "episodes 24844\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47130000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 24849\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47140000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 24854\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47150000\n",
      "mean reward (100 episodes) 17.410000\n",
      "best mean reward 18.810000\n",
      "episodes 24860\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47160000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 18.810000\n",
      "episodes 24865\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47170000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 18.810000\n",
      "episodes 24870\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47180000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 24876\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47190000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 24881\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47200000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 24886\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47210000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 24891\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47220000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 24896\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47230000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 24902\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47240000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 24907\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47250000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 18.810000\n",
      "episodes 24912\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47260000\n",
      "mean reward (100 episodes) 17.380000\n",
      "best mean reward 18.810000\n",
      "episodes 24917\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47270000\n",
      "mean reward (100 episodes) 17.290000\n",
      "best mean reward 18.810000\n",
      "episodes 24922\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47280000\n",
      "mean reward (100 episodes) 17.370000\n",
      "best mean reward 18.810000\n",
      "episodes 24927\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 47290000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 24933\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47300000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.810000\n",
      "episodes 24938\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47310000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 24943\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47320000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 24948\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47330000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 24953\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47340000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 24959\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47350000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 24964\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47360000\n",
      "mean reward (100 episodes) 17.410000\n",
      "best mean reward 18.810000\n",
      "episodes 24969\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47370000\n",
      "mean reward (100 episodes) 17.400000\n",
      "best mean reward 18.810000\n",
      "episodes 24974\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47380000\n",
      "mean reward (100 episodes) 17.340000\n",
      "best mean reward 18.810000\n",
      "episodes 24979\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47390000\n",
      "mean reward (100 episodes) 17.240000\n",
      "best mean reward 18.810000\n",
      "episodes 24984\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47400000\n",
      "mean reward (100 episodes) 17.290000\n",
      "best mean reward 18.810000\n",
      "episodes 24989\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47410000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 18.810000\n",
      "episodes 24995\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-22 22:57:59,727] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video025000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 47420000\n",
      "mean reward (100 episodes) 17.360000\n",
      "best mean reward 18.810000\n",
      "episodes 25000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47430000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 25005\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47440000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 25011\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47450000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.810000\n",
      "episodes 25016\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47460000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 25022\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47470000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 25027\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47480000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 25032\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47490000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.810000\n",
      "episodes 25037\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47500000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 25042\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47510000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.810000\n",
      "episodes 25047\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47520000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.810000\n",
      "episodes 25052\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47530000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.810000\n",
      "episodes 25057\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47540000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 25062\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47550000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 25068\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47560000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 25073\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47570000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.810000\n",
      "episodes 25078\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47580000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 25083\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47590000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 25088\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47600000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 25093\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47610000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 25098\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47620000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 25104\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47630000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 25109\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47640000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 25114\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47650000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 25119\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47660000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 25125\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47670000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 25130\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47680000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 18.810000\n",
      "episodes 25135\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47690000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 25141\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47700000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 25146\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47710000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 25151\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47720000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.810000\n",
      "episodes 25156\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47730000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 25161\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47740000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 25166\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47750000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 25172\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47760000\n",
      "mean reward (100 episodes) 17.340000\n",
      "best mean reward 18.810000\n",
      "episodes 25177\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47770000\n",
      "mean reward (100 episodes) 17.370000\n",
      "best mean reward 18.810000\n",
      "episodes 25183\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47780000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 25188\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47790000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 25193\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47800000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 18.810000\n",
      "episodes 25198\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47810000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 25204\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47820000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 25209\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47830000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 25214\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47840000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 18.810000\n",
      "episodes 25220\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47850000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 18.810000\n",
      "episodes 25225\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47860000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 25230\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47870000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 18.810000\n",
      "episodes 25235\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47880000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.810000\n",
      "episodes 25241\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47890000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 25246\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47900000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 25251\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47910000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 25256\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47920000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.810000\n",
      "episodes 25262\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47930000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 25267\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47940000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 25272\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47950000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.810000\n",
      "episodes 25277\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47960000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.810000\n",
      "episodes 25282\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47970000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 25287\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47980000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 25292\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 47990000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 25298\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48000000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 25303\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 48010000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.810000\n",
      "episodes 25308\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48020000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 18.810000\n",
      "episodes 25313\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48030000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.810000\n",
      "episodes 25318\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48040000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 25323\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48050000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 25328\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48060000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 25334\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48070000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 25339\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48080000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 25345\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48090000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.810000\n",
      "episodes 25350\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48100000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 25355\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48110000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 25361\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48120000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 25367\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48130000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 25372\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48140000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 25377\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48150000\n",
      "mean reward (100 episodes) 17.530000\n",
      "best mean reward 18.810000\n",
      "episodes 25382\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48160000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.810000\n",
      "episodes 25387\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48170000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 25392\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48180000\n",
      "mean reward (100 episodes) 17.520000\n",
      "best mean reward 18.810000\n",
      "episodes 25397\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48190000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 25403\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48200000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 25408\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48210000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 25413\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48220000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 25418\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48230000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 25423\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48240000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.810000\n",
      "episodes 25429\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48250000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 25434\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48260000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.810000\n",
      "episodes 25440\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48270000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.810000\n",
      "episodes 25445\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48280000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 25451\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48290000\n",
      "mean reward (100 episodes) 17.510000\n",
      "best mean reward 18.810000\n",
      "episodes 25456\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48300000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 25461\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48310000\n",
      "mean reward (100 episodes) 17.400000\n",
      "best mean reward 18.810000\n",
      "episodes 25466\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48320000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 18.810000\n",
      "episodes 25471\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48330000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 18.810000\n",
      "episodes 25477\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48340000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 18.810000\n",
      "episodes 25482\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48350000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 25487\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48360000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.810000\n",
      "episodes 25492\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48370000\n",
      "mean reward (100 episodes) 17.460000\n",
      "best mean reward 18.810000\n",
      "episodes 25497\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48380000\n",
      "mean reward (100 episodes) 17.410000\n",
      "best mean reward 18.810000\n",
      "episodes 25502\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48390000\n",
      "mean reward (100 episodes) 17.360000\n",
      "best mean reward 18.810000\n",
      "episodes 25508\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48400000\n",
      "mean reward (100 episodes) 17.340000\n",
      "best mean reward 18.810000\n",
      "episodes 25513\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48410000\n",
      "mean reward (100 episodes) 17.340000\n",
      "best mean reward 18.810000\n",
      "episodes 25518\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48420000\n",
      "mean reward (100 episodes) 17.330000\n",
      "best mean reward 18.810000\n",
      "episodes 25523\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48430000\n",
      "mean reward (100 episodes) 17.310000\n",
      "best mean reward 18.810000\n",
      "episodes 25529\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48440000\n",
      "mean reward (100 episodes) 17.250000\n",
      "best mean reward 18.810000\n",
      "episodes 25534\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48450000\n",
      "mean reward (100 episodes) 17.280000\n",
      "best mean reward 18.810000\n",
      "episodes 25539\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48460000\n",
      "mean reward (100 episodes) 17.160000\n",
      "best mean reward 18.810000\n",
      "episodes 25544\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48470000\n",
      "mean reward (100 episodes) 17.250000\n",
      "best mean reward 18.810000\n",
      "episodes 25549\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48480000\n",
      "mean reward (100 episodes) 17.320000\n",
      "best mean reward 18.810000\n",
      "episodes 25554\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48490000\n",
      "mean reward (100 episodes) 17.300000\n",
      "best mean reward 18.810000\n",
      "episodes 25559\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48500000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 18.810000\n",
      "episodes 25565\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48510000\n",
      "mean reward (100 episodes) 17.330000\n",
      "best mean reward 18.810000\n",
      "episodes 25570\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48520000\n",
      "mean reward (100 episodes) 17.320000\n",
      "best mean reward 18.810000\n",
      "episodes 25575\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48530000\n",
      "mean reward (100 episodes) 17.190000\n",
      "best mean reward 18.810000\n",
      "episodes 25580\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48540000\n",
      "mean reward (100 episodes) 17.170000\n",
      "best mean reward 18.810000\n",
      "episodes 25585\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48550000\n",
      "mean reward (100 episodes) 17.110000\n",
      "best mean reward 18.810000\n",
      "episodes 25590\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48560000\n",
      "mean reward (100 episodes) 17.170000\n",
      "best mean reward 18.810000\n",
      "episodes 25595\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48570000\n",
      "mean reward (100 episodes) 17.200000\n",
      "best mean reward 18.810000\n",
      "episodes 25601\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48580000\n",
      "mean reward (100 episodes) 17.160000\n",
      "best mean reward 18.810000\n",
      "episodes 25606\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48590000\n",
      "mean reward (100 episodes) 17.220000\n",
      "best mean reward 18.810000\n",
      "episodes 25611\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 48600000\n",
      "mean reward (100 episodes) 17.190000\n",
      "best mean reward 18.810000\n",
      "episodes 25616\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48610000\n",
      "mean reward (100 episodes) 17.180000\n",
      "best mean reward 18.810000\n",
      "episodes 25622\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48620000\n",
      "mean reward (100 episodes) 17.130000\n",
      "best mean reward 18.810000\n",
      "episodes 25627\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48630000\n",
      "mean reward (100 episodes) 17.190000\n",
      "best mean reward 18.810000\n",
      "episodes 25632\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48640000\n",
      "mean reward (100 episodes) 17.230000\n",
      "best mean reward 18.810000\n",
      "episodes 25637\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48650000\n",
      "mean reward (100 episodes) 17.290000\n",
      "best mean reward 18.810000\n",
      "episodes 25643\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48660000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 18.810000\n",
      "episodes 25648\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48670000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 18.810000\n",
      "episodes 25653\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48680000\n",
      "mean reward (100 episodes) 17.330000\n",
      "best mean reward 18.810000\n",
      "episodes 25658\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48690000\n",
      "mean reward (100 episodes) 17.220000\n",
      "best mean reward 18.810000\n",
      "episodes 25663\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48700000\n",
      "mean reward (100 episodes) 17.140000\n",
      "best mean reward 18.810000\n",
      "episodes 25668\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48710000\n",
      "mean reward (100 episodes) 17.170000\n",
      "best mean reward 18.810000\n",
      "episodes 25673\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48720000\n",
      "mean reward (100 episodes) 17.230000\n",
      "best mean reward 18.810000\n",
      "episodes 25678\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48730000\n",
      "mean reward (100 episodes) 17.240000\n",
      "best mean reward 18.810000\n",
      "episodes 25683\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48740000\n",
      "mean reward (100 episodes) 17.210000\n",
      "best mean reward 18.810000\n",
      "episodes 25688\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48750000\n",
      "mean reward (100 episodes) 17.140000\n",
      "best mean reward 18.810000\n",
      "episodes 25694\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48760000\n",
      "mean reward (100 episodes) 17.170000\n",
      "best mean reward 18.810000\n",
      "episodes 25699\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48770000\n",
      "mean reward (100 episodes) 17.250000\n",
      "best mean reward 18.810000\n",
      "episodes 25704\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48780000\n",
      "mean reward (100 episodes) 17.340000\n",
      "best mean reward 18.810000\n",
      "episodes 25709\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48790000\n",
      "mean reward (100 episodes) 17.300000\n",
      "best mean reward 18.810000\n",
      "episodes 25714\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48800000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 25720\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48810000\n",
      "mean reward (100 episodes) 17.380000\n",
      "best mean reward 18.810000\n",
      "episodes 25725\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48820000\n",
      "mean reward (100 episodes) 17.290000\n",
      "best mean reward 18.810000\n",
      "episodes 25730\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48830000\n",
      "mean reward (100 episodes) 17.250000\n",
      "best mean reward 18.810000\n",
      "episodes 25735\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48840000\n",
      "mean reward (100 episodes) 17.280000\n",
      "best mean reward 18.810000\n",
      "episodes 25740\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48850000\n",
      "mean reward (100 episodes) 17.230000\n",
      "best mean reward 18.810000\n",
      "episodes 25745\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48860000\n",
      "mean reward (100 episodes) 17.210000\n",
      "best mean reward 18.810000\n",
      "episodes 25750\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48870000\n",
      "mean reward (100 episodes) 17.260000\n",
      "best mean reward 18.810000\n",
      "episodes 25755\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48880000\n",
      "mean reward (100 episodes) 17.270000\n",
      "best mean reward 18.810000\n",
      "episodes 25761\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48890000\n",
      "mean reward (100 episodes) 17.380000\n",
      "best mean reward 18.810000\n",
      "episodes 25766\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48900000\n",
      "mean reward (100 episodes) 17.390000\n",
      "best mean reward 18.810000\n",
      "episodes 25771\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48910000\n",
      "mean reward (100 episodes) 17.340000\n",
      "best mean reward 18.810000\n",
      "episodes 25776\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48920000\n",
      "mean reward (100 episodes) 17.330000\n",
      "best mean reward 18.810000\n",
      "episodes 25781\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48930000\n",
      "mean reward (100 episodes) 17.440000\n",
      "best mean reward 18.810000\n",
      "episodes 25786\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48940000\n",
      "mean reward (100 episodes) 17.480000\n",
      "best mean reward 18.810000\n",
      "episodes 25791\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48950000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 25796\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48960000\n",
      "mean reward (100 episodes) 17.450000\n",
      "best mean reward 18.810000\n",
      "episodes 25802\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48970000\n",
      "mean reward (100 episodes) 17.360000\n",
      "best mean reward 18.810000\n",
      "episodes 25807\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48980000\n",
      "mean reward (100 episodes) 17.340000\n",
      "best mean reward 18.810000\n",
      "episodes 25812\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 48990000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 25817\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49000000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 25823\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49010000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 25828\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49020000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 25833\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49030000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 25838\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49040000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.810000\n",
      "episodes 25843\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49050000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 25848\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49060000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 25854\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49070000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 25859\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49080000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 25864\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49090000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 25869\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49100000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 25874\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49110000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.810000\n",
      "episodes 25880\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49120000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.810000\n",
      "episodes 25885\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49130000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.810000\n",
      "episodes 25890\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49140000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.810000\n",
      "episodes 25895\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49150000\n",
      "mean reward (100 episodes) 17.880000\n",
      "best mean reward 18.810000\n",
      "episodes 25900\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49160000\n",
      "mean reward (100 episodes) 17.910000\n",
      "best mean reward 18.810000\n",
      "episodes 25905\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49170000\n",
      "mean reward (100 episodes) 17.900000\n",
      "best mean reward 18.810000\n",
      "episodes 25911\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49180000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.810000\n",
      "episodes 25915\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 49190000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 25920\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49200000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 25926\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49210000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 25931\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49220000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 25936\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49230000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 25942\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49240000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 25947\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49250000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.810000\n",
      "episodes 25952\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49260000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 25957\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49270000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.810000\n",
      "episodes 25963\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49280000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.810000\n",
      "episodes 25968\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49290000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.810000\n",
      "episodes 25973\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49300000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 25978\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49310000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.810000\n",
      "episodes 25983\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49320000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.810000\n",
      "episodes 25988\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49330000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 25993\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49340000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 18.810000\n",
      "episodes 25999\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-23 00:46:07,845] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.8907.video026000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 49350000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 26004\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49360000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 26009\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49370000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.810000\n",
      "episodes 26015\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49380000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.810000\n",
      "episodes 26020\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49390000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.810000\n",
      "episodes 26025\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49400000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.810000\n",
      "episodes 26030\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49410000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 18.810000\n",
      "episodes 26035\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49420000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 26040\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49430000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 26046\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49440000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 18.810000\n",
      "episodes 26051\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49450000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.810000\n",
      "episodes 26056\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49460000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 26061\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49470000\n",
      "mean reward (100 episodes) 17.490000\n",
      "best mean reward 18.810000\n",
      "episodes 26067\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49480000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 26072\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49490000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 18.810000\n",
      "episodes 26078\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49500000\n",
      "mean reward (100 episodes) 17.430000\n",
      "best mean reward 18.810000\n",
      "episodes 26083\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49510000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 18.810000\n",
      "episodes 26088\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49520000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 26094\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49530000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 26099\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49540000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 18.810000\n",
      "episodes 26104\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49550000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 18.810000\n",
      "episodes 26110\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49560000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.810000\n",
      "episodes 26115\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49570000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 26120\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49580000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 26125\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49590000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.810000\n",
      "episodes 26130\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49600000\n",
      "mean reward (100 episodes) 17.760000\n",
      "best mean reward 18.810000\n",
      "episodes 26136\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49610000\n",
      "mean reward (100 episodes) 17.770000\n",
      "best mean reward 18.810000\n",
      "episodes 26141\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49620000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.810000\n",
      "episodes 26146\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49630000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.810000\n",
      "episodes 26151\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49640000\n",
      "mean reward (100 episodes) 17.780000\n",
      "best mean reward 18.810000\n",
      "episodes 26156\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49650000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.810000\n",
      "episodes 26161\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49660000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.810000\n",
      "episodes 26166\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49670000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.810000\n",
      "episodes 26172\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49680000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.810000\n",
      "episodes 26177\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49690000\n",
      "mean reward (100 episodes) 17.800000\n",
      "best mean reward 18.810000\n",
      "episodes 26182\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49700000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 26188\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49710000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 26193\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49720000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 26198\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49730000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 18.810000\n",
      "episodes 26203\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49740000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 26208\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49750000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 26214\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49760000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.810000\n",
      "episodes 26219\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49770000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.810000\n",
      "episodes 26224\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49780000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 26230\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49790000\n",
      "mean reward (100 episodes) 17.690000\n",
      "best mean reward 18.810000\n",
      "episodes 26235\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49800000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 26241\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49810000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 18.810000\n",
      "episodes 26246\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49820000\n",
      "mean reward (100 episodes) 17.700000\n",
      "best mean reward 18.810000\n",
      "episodes 26251\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49830000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 18.810000\n",
      "episodes 26257\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49840000\n",
      "mean reward (100 episodes) 17.830000\n",
      "best mean reward 18.810000\n",
      "episodes 26262\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49850000\n",
      "mean reward (100 episodes) 17.810000\n",
      "best mean reward 18.810000\n",
      "episodes 26267\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49860000\n",
      "mean reward (100 episodes) 17.750000\n",
      "best mean reward 18.810000\n",
      "episodes 26273\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49870000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 26278\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49880000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 26283\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49890000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 26288\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49900000\n",
      "mean reward (100 episodes) 17.660000\n",
      "best mean reward 18.810000\n",
      "episodes 26294\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49910000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 26299\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49920000\n",
      "mean reward (100 episodes) 17.540000\n",
      "best mean reward 18.810000\n",
      "episodes 26304\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49930000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.810000\n",
      "episodes 26310\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 49940000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 26315\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49950000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.810000\n",
      "episodes 26320\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49960000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 18.810000\n",
      "episodes 26326\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49970000\n",
      "mean reward (100 episodes) 17.790000\n",
      "best mean reward 18.810000\n",
      "episodes 26331\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49980000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 26336\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 49990000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 26342\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 50000000\n",
      "mean reward (100 episodes) 17.680000\n",
      "best mean reward 18.810000\n",
      "episodes 26347\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 50010000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 26352\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 50020000\n",
      "mean reward (100 episodes) 17.670000\n",
      "best mean reward 18.810000\n",
      "episodes 26357\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 50030000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 26363\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 50040000\n",
      "mean reward (100 episodes) 17.620000\n",
      "best mean reward 18.810000\n",
      "episodes 26368\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 50050000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 26373\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 50060000\n",
      "mean reward (100 episodes) 17.580000\n",
      "best mean reward 18.810000\n",
      "episodes 26378\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 50070000\n",
      "mean reward (100 episodes) 17.610000\n",
      "best mean reward 18.810000\n",
      "episodes 26384\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 50080000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 26389\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 50090000\n",
      "mean reward (100 episodes) 17.650000\n",
      "best mean reward 18.810000\n",
      "episodes 26395\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n",
      "Timestep 50100000\n",
      "mean reward (100 episodes) 17.630000\n",
      "best mean reward 18.810000\n",
      "episodes 26399\n",
      "exploration 0.100000\n",
      "learning_rate 0.000050\n"
     ]
    }
   ],
   "source": [
    "for t in itertools.count():\n",
    "    ### 1. Check stopping criterion\n",
    "    if stopping_criterion is not None and stopping_criterion(env, t):\n",
    "        print(\"stopping\")\n",
    "        break\n",
    "\n",
    "    ### 2. Step the env and store the transition\n",
    "    # At this point, \"last_obs\" contains the latest observation that was\n",
    "    # recorded from the simulator. Here, your code needs to store this\n",
    "    # observation and its outcome (reward, next observation, etc.) into\n",
    "    # the replay buffer while stepping the simulator forward one step.\n",
    "    # At the end of this block of code, the simulator should have been\n",
    "    # advanced one step, and the replay buffer should contain one more\n",
    "    # transition.\n",
    "    # Specifically, last_obs must point to the new latest observation.\n",
    "    # Useful functions you'll need to call:\n",
    "    # obs, reward, done, info = env.step(action)\n",
    "    # this steps the environment forward one step\n",
    "    # obs = env.reset()\n",
    "    # this resets the environment if you reached an episode boundary.\n",
    "    # Don't forget to call env.reset() to get a new observation if done\n",
    "    # is true!!\n",
    "    # Note that you cannot use \"last_obs\" directly as input\n",
    "    # into your network, since it needs to be processed to include context\n",
    "    # from previous frames. You should check out the replay buffer\n",
    "    # implementation in dqn_utils.py to see what functionality the replay\n",
    "    # buffer exposes. The replay buffer has a function called\n",
    "    # encode_recent_observation that will take the latest observation\n",
    "    # that you pushed into the buffer and compute the corresponding\n",
    "    # input that should be given to a Q network by appending some\n",
    "    # previous frames.\n",
    "    # Don't forget to include epsilon greedy exploration!\n",
    "    # And remember that the first time you enter this loop, the model\n",
    "    # may not yet have been initialized (but of course, the first step\n",
    "    # might as well be random, since you haven't trained your net...)\n",
    "\n",
    "    #####\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    idx = replay_buffer.store_frame(last_obs)\n",
    "    \n",
    "    if not model_initialized or random.random() < exploration.value(t):\n",
    "        action = random.randint(0, num_actions - 1)\n",
    "    else:\n",
    "        obs = replay_buffer.encode_recent_observation()\n",
    "        action = session.run(greedy_action, {obs_t_ph: [obs]})\n",
    "    \n",
    "    next_obs, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        last_obs = env.reset()\n",
    "\n",
    "    replay_buffer.store_effect(idx, action, reward, done)\n",
    "    last_obs = next_obs\n",
    "\n",
    "    replay_buffer.store_effect(working_idx, action, reward, done)\n",
    "\n",
    "    #####\n",
    "\n",
    "    # at this point, the environment should have been advanced one step (and\n",
    "    # reset if done was true), and last_obs should point to the new latest\n",
    "    # observation\n",
    "\n",
    "    ### 3. Perform experience replay and train the network.\n",
    "    # note that this is only done if the replay buffer contains enough samples\n",
    "    # for us to learn something useful -- until then, the model will not be\n",
    "    # initialized and random actions should be taken\n",
    "    if (t > learning_starts and\n",
    "            t % learning_freq == 0 and\n",
    "            replay_buffer.can_sample(batch_size)):\n",
    "        # Here, you should perform training. Training consists of four steps:\n",
    "        # 3.a: use the replay buffer to sample a batch of transitions (see the\n",
    "        # replay buffer code for function definition, each batch that you sample\n",
    "        # should consist of current observations, current actions, rewards,\n",
    "        # next observations, and done indicator).\n",
    "        # 3.b: initialize the model if it has not been initialized yet; to do\n",
    "        # that, call\n",
    "        #    initialize_interdependent_variables(session, tf.global_variables(), {\n",
    "        #        obs_t_ph: obs_t_batch,\n",
    "        #        obs_tp1_ph: obs_tp1_batch,\n",
    "        #    })\n",
    "        # where obs_t_batch and obs_tp1_batch are the batches of observations at\n",
    "        # the current and next time step. The boolean variable model_initialized\n",
    "        # indicates whether or not the model has been initialized.\n",
    "        # Remember that you have to update the target network too (see 3.d)!\n",
    "        # 3.c: train the model. To do this, you'll need to use the train_fn and\n",
    "        # total_error ops that were created earlier: total_error is what you\n",
    "        # created to compute the total Bellman error in a batch, and train_fn\n",
    "        # will actually perform a gradient step and update the network parameters\n",
    "        # to reduce total_error. When calling session.run on these you'll need to\n",
    "        # populate the following placeholders:\n",
    "        # obs_t_ph\n",
    "        # act_t_ph\n",
    "        # rew_t_ph\n",
    "        # obs_tp1_ph\n",
    "        # done_mask_ph\n",
    "        # (this is needed for computing total_error)\n",
    "        # learning_rate -- you can get this from optimizer_spec.lr_schedule.value(t)\n",
    "        # (this is needed by the optimizer to choose the learning rate)\n",
    "        # 3.d: periodically update the target network by calling\n",
    "        # session.run(update_target_fn)\n",
    "        # you should update every target_update_freq steps, and you may find the\n",
    "        # variable num_param_updates useful for this (it was initialized to 0)\n",
    "        #####\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        obs_t_batch, act_batch, rew_batch, obs_tp1_batch, done_mask = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        session.run(train_fn, feed_dict={obs_t_ph: obs_t_batch, act_t_ph: act_batch, \\\n",
    "                    obs_tp1_ph: obs_tp1_batch, rew_t_ph: rew_batch, \\\n",
    "                    done_mask_ph: done_mask, learning_rate: optimizer_spec.lr_schedule.value(t)})\n",
    "        \n",
    "        if t and not  t%target_update_freq:\n",
    "            session.run(update_target_fn)\n",
    "            num_param_updates += 1\n",
    "        #####\n",
    "        \n",
    "\n",
    "    ### 4. Log progress\n",
    "    episode_rewards = get_wrapper_by_name(env, \"Monitor\").get_episode_rewards()\n",
    "    if len(episode_rewards) > 0:\n",
    "        mean_episode_reward = np.mean(episode_rewards[-100:])\n",
    "    if len(episode_rewards) > 100:\n",
    "        best_mean_episode_reward = max(best_mean_episode_reward, mean_episode_reward)\n",
    "    if t % LOG_EVERY_N_STEPS == 0 and model_initialized:\n",
    "        print(\"Timestep %d\" % (t,))\n",
    "        print(\"mean reward (100 episodes) %f\" % mean_episode_reward)\n",
    "        print(\"best mean reward %f\" % best_mean_episode_reward)\n",
    "        print(\"episodes %d\" % len(episode_rewards))\n",
    "        print(\"exploration %f\" % exploration.value(t))\n",
    "        print(\"learning_rate %f\" % optimizer_spec.lr_schedule.value(t))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(env, gym.Wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
